Directory structure:
└── nervosnetwork-rfcs/
    ├── README.md
    ├── CHANGELOG.md
    ├── LICENSE
    ├── 0000-template/
    │   └── 0000-template.md
    ├── rfcs/
    │   ├── 0001-positioning/
    │   │   └── 0001-positioning.md
    │   ├── 0002-ckb/
    │   │   ├── 0002-ckb.md
    │   │   └── images/
    │   │       └── .keep
    │   ├── 0003-ckb-vm/
    │   │   ├── 0003-ckb-vm.md
    │   │   └── 0003-ckb-vm.zh.md
    │   ├── 0004-ckb-block-sync/
    │   │   ├── 0004-ckb-block-sync.md
    │   │   ├── 0004-ckb-block-sync.zh.md
    │   │   └── images/
    │   ├── 0005-priviledged-mode/
    │   │   ├── 0005-priviledged-mode.md
    │   │   └── 0005-priviledged-mode.zh.md
    │   ├── 0006-merkle-tree/
    │   │   ├── 0006-merkle-tree.md
    │   │   └── 0006-merkle-tree.zh.md
    │   ├── 0007-scoring-system-and-network-security/
    │   │   ├── 0007-scoring-system-and-network-security.md
    │   │   └── 0007-scoring-system-and-network-security.zh.md
    │   ├── 0008-serialization/
    │   │   └── 0008-serialization.md
    │   ├── 0009-vm-syscalls/
    │   │   └── 0009-vm-syscalls.md
    │   ├── 0010-eaglesong/
    │   │   ├── 0010-eaglesong.md
    │   │   ├── CompactFIPS202.py
    │   │   ├── constants.py
    │   │   ├── eaglesong.c
    │   │   ├── eaglesong.py
    │   │   ├── hash.c
    │   │   └── hash.py
    │   ├── 0011-transaction-filter-protocol/
    │   │   └── 0011-transaction-filter-protocol.md
    │   ├── 0012-node-discovery/
    │   │   ├── 0012-node-discovery.md
    │   │   ├── 0012-node-discovery.zh.md
    │   │   └── images/
    │   ├── 0013-get-block-template/
    │   │   └── 0013-get-block-template.md
    │   ├── 0014-vm-cycle-limits/
    │   │   └── 0014-vm-cycle-limits.md
    │   ├── 0015-ckb-cryptoeconomics/
    │   │   ├── 0015-ckb-cryptoeconomics.md
    │   │   └── images/
    │   ├── 0017-tx-valid-since/
    │   │   └── 0017-tx-valid-since.md
    │   ├── 0019-data-structures/
    │   │   └── 0019-data-structures.md
    │   ├── 0020-ckb-consensus-protocol/
    │   │   ├── 0020-ckb-consensus-protocol.md
    │   │   └── images/
    │   ├── 0021-ckb-address-format/
    │   │   ├── 0021-ckb-address-format.md
    │   │   └── images/
    │   ├── 0022-transaction-structure/
    │   │   └── 0022-transaction-structure.md
    │   ├── 0023-dao-deposit-withdraw/
    │   │   └── 0023-dao-deposit-withdraw.md
    │   ├── 0024-ckb-genesis-script-list/
    │   │   └── 0024-ckb-genesis-script-list.md
    │   ├── 0025-simple-udt/
    │   │   └── 0025-simple-udt.md
    │   ├── 0026-anyone-can-pay/
    │   │   └── 0026-anyone-can-pay.md
    │   ├── 0027-block-structure/
    │   │   └── 0027-block-structure.md
    │   ├── 0028-change-since-relative-timestamp/
    │   │   └── 0028-change-since-relative-timestamp.md
    │   ├── 0029-allow-script-multiple-matches-on-identical-code/
    │   │   └── 0029-allow-script-multiple-matches-on-identical-code.md
    │   ├── 0030-ensure-index-less-than-length-in-since/
    │   │   └── 0030-ensure-index-less-than-length-in-since.md
    │   ├── 0031-variable-length-header-field/
    │   │   ├── 0031-variable-length-header-field.md
    │   │   ├── 1-appending-the-field-at-the-end.md
    │   │   ├── 2-using-molecule-table-in-new-block-headers.md
    │   │   └── 3-appending-a-hash-at-the-end.md
    │   ├── 0032-ckb-vm-version-selection/
    │   │   └── 0032-ckb-vm-version-selection.md
    │   ├── 0033-ckb-vm-version-1/
    │   │   └── 0033-ckb-vm-version-1.md
    │   ├── 0034-vm-syscalls-2/
    │   │   └── 0034-vm-syscalls-2.md
    │   ├── 0035-ckb2021-p2p-protocol-upgrade/
    │   │   ├── 0035-ckb2021-p2p-protocol-upgrade.md
    │   │   └── 0035-ckb2021-p2p-protocol-upgrade.zh-CN.md
    │   ├── 0036-remove-header-deps-immature-rule/
    │   │   └── 0036-remove-header-deps-immature-rule.md
    │   ├── 0037-ckb2021/
    │   │   └── 0037-ckb2021.md
    │   ├── 0039-cheque/
    │   │   └── 0039-cheque.md
    │   ├── 0042-omnilock/
    │   │   └── 0042-omnilock.md
    │   ├── 0043-ckb-softfork-activation/
    │   │   ├── 0043-ckb-softfork-activation.md
    │   │   ├── deployments.md
    │   │   └── images/
    │   ├── 0044-ckb-light-client/
    │   │   └── 0044-ckb-light-client.md
    │   ├── 0045-client-block-filter/
    │   │   └── 0045-client-block-filter.md
    │   ├── 0046-syscalls-summary/
    │   │   └── 0046-syscalls-summary.md
    │   ├── 0048-remove-block-header-version-reservation-rule/
    │   │   └── 0048-remove-block-header-version-reservation-rule.md
    │   ├── 0049-ckb-vm-version-2/
    │   │   └── 0049-ckb-vm-version-2.md
    │   ├── 0050-vm-syscalls-3/
    │   │   └── 0050-vm-syscalls-3.md
    │   ├── 0051-ckb2023/
    │   │   └── 0051-ckb2023.md
    │   └── 0052-extensible-udt/
    │       └── 0052-extensible-udt.md
    └── .github/
        ├── CODEOWNERS
        ├── scripts/
        │   └── local-link-checker.sh
        └── workflows/
            └── local-link-checker.yaml

================================================
File: README.md
================================================
# Nervos Network RFCs

This repository contains proposals, standards and documentations related to Nervos Network.

The RFC (Request for Comments) process is intended to provide an open and community driven path for new protocols, improvements and best practices, so that all stakeholders can be confident about the direction of Nervos network is evolving in.

RFCs publication here does not make it formally accepted standard until its status becomes Standard.

## Categories

Not all RFCs are standards, there are 2 categories:

* Standards Track - RFC that is intended to be standard followed by protocols, clients and applications in Nervos network.
* Informational - Anything related to Nervos network.

## Process

The RFC process attempts to be as simple as possible at beginning and evolves with the network.

### 1. Discuss Your Idea with Community

Before submitting a RFC pull request, you should send the draft to community to solicit initial feedbacks. The [#rfc-chat discord channel](https://discord.gg/8cWtA9uJR5) or [Nervos Talk](https://talk.nervos.org/) are both good places to go.

### 2. Create A Pull Request

After discussion, please create a pull request to propose your RFC:

> Copy `0000-template` as `rfcs/0000-feature-name`, where `feature-name` is the descriptive name of the RFC. Don't assign a number yet. Reserve a RFC number in [this issue](https://github.com/nervosnetwork/rfcs/issues/246).

Nervos RFCs should be written in English, but translated versions can be provided to help understanding. English version is the canonical version, check english version when there's ambiguity.

Nervos RFCs should follow the keyword conventions defined in [RFC 2119](https://tools.ietf.org/html/rfc2119), [RFC 6919](https://tools.ietf.org/html/rfc6919).

A RFC should be put in either `Informational Track` or `Standards Track`. A RFC on `Standards Track` is a technical specification for software developers to facilitate an interoperable ecosystem. A RFC on `Informational Track` is a descriptive document providing necessary and/or helpful information to users and builders.

A RFC on `Informational Track` has 3 statuses:

1. `Draft` (initial status)
2. `Withdrawn`
3. `Final`

A RFC on `Standards Track` has 5 statuses:

1. `Proposal` (initial status)
2. `Active`
3. `Withdrawn`
4. `Rejected`
5. `Obsolete`

### 3. Review / Accept

The maintainers of RFCs and the community will review the PR, and you should update the RFC according to feedbacks. When a RFC is ready and get enough supports, it will be accepted and merged into this repository. The acceptance of a RFC is based on [rough consensus](https://en.wikipedia.org/wiki/Rough_consensus) at this early stage, we'll keep improving it as the network and ecosystem develops, until we reached the decentralized governance stage.

## RFCs

| Number | Title | Author | Category | Status |
|--------|-------|--------|----------|--------|
| [1](rfcs/0001-positioning) | [The Nervos Network Positioning Paper](rfcs/0001-positioning/0001-positioning.md) | The Nervos Team | Informational | Final |
| [2](rfcs/0002-ckb) | [Nervos CKB: A Common Knowledge Base for Crypto-Economy](rfcs/0002-ckb/0002-ckb.md) | Jan Xie | Informational | Final |
| [3](rfcs/0003-ckb-vm) | [CKB-VM](rfcs/0003-ckb-vm/0003-ckb-vm.md) | Xuejie Xiao | Informational | Final |
| [4](rfcs/0004-ckb-block-sync) | [CKB Block Synchronization Protocol](rfcs/0004-ckb-block-sync/0004-ckb-block-sync.md) | Ian Yang | Informational | Final |
| [5](rfcs/0005-priviledged-mode) | [Privileged architecture support for CKB VM](rfcs/0005-priviledged-mode/0005-priviledged-mode.md) | Xuejie Xiao | Informational | Withdrawn |
| [6](rfcs/0006-merkle-tree) | [Merkle Tree for Static Data](rfcs/0006-merkle-tree/0006-merkle-tree.md) | Ke Wang | Standards Track | Active |
| [7](rfcs/0007-scoring-system-and-network-security) | [P2P Scoring System And Network Security](rfcs/0007-scoring-system-and-network-security/0007-scoring-system-and-network-security.md) | Jinyang Jiang | Standards Track | Withdrawn |
| [8](rfcs/0008-serialization) | [Serialization](rfcs/0008-serialization/0008-serialization.md) | Boyu Yang | Standards Track | Active |
| [9](rfcs/0009-vm-syscalls) | [VM Syscalls](rfcs/0009-vm-syscalls/0009-vm-syscalls.md) | Xuejie Xiao | Standards Track | Active |
| [10](rfcs/0010-eaglesong) | [Eaglesong (Proof-of-Work Function for Nervos CKB)](rfcs/0010-eaglesong/0010-eaglesong.md) | Alan Szepieniec | Standards Track | Active |
| [11](rfcs/0011-transaction-filter-protocol) | [Transaction Filter](rfcs/0011-transaction-filter-protocol/0011-transaction-filter-protocol.md) | Quake Wang | Standards Track | Withdrawn |
| [12](rfcs/0012-node-discovery) | [Node Discovery](rfcs/0012-node-discovery/0012-node-discovery.md) | Linfeng Qian, Jinyang Jiang | Standards Track | Active |
| [13](rfcs/0013-get-block-template) | [Block Template](rfcs/0013-get-block-template/0013-get-block-template.md) | Dingwei Zhang | Standards Track | Active |
| [14](rfcs/0014-vm-cycle-limits) | [VM Cycle Limits](rfcs/0014-vm-cycle-limits/0014-vm-cycle-limits.md) | Xuejie Xiao | Standards Track | Active |
| [15](rfcs/0015-ckb-cryptoeconomics) | [Crypto-Economics of the Nervos Common Knowledge Base](rfcs/0015-ckb-cryptoeconomics/0015-ckb-cryptoeconomics.md) | Kevin Wang, Jan Xie, Jiasun Li, David Zou | Informational | Final |
| [17](rfcs/0017-tx-valid-since) | [Transaction Since Precondition](rfcs/0017-tx-valid-since/0017-tx-valid-since.md) | Jinyang Jiang, Ian Yang, Jordan Mack | Standards Track | Proposal
| [19](rfcs/0019-data-structures) | [Data Structures](rfcs/0019-data-structures/0019-data-structures.md) | Xuejie Xiao | Informational | Withdrawn
| [20](rfcs/0020-ckb-consensus-protocol) | [CKB Consensus Protocol](rfcs/0020-ckb-consensus-protocol/0020-ckb-consensus-protocol.md) | Ren Zhang | Informational | Draft
| [21](rfcs/0021-ckb-address-format) | [CKB Address Format](rfcs/0021-ckb-address-format/0021-ckb-address-format.md) | Cipher Wang, Axel Wan | Standards Track | Active
| [22](rfcs/0022-transaction-structure) | [CKB Transaction Structure](rfcs/0022-transaction-structure/0022-transaction-structure.md) | Ian Yang | Informational | Draft
| [23](rfcs/0023-dao-deposit-withdraw) | [Deposit and Withdraw in Nervos DAO](rfcs/0023-dao-deposit-withdraw/0023-dao-deposit-withdraw.md) | Jan Xie, Xuejie Xiao, Ian Yang | Standards Track | Active
| [24](rfcs/0024-ckb-genesis-script-list) | [CKB Genesis Script List](rfcs/0024-ckb-genesis-script-list/0024-ckb-genesis-script-list.md) | Dylan Duan | Informational | Final
| [25](rfcs/0025-simple-udt) | [Simple UDT](rfcs/0025-simple-udt/0025-simple-udt.md) | Xuejie Xiao | Standards Track | Proposal
| [26](rfcs/0026-anyone-can-pay) | [Anyone-Can-Pay Lock](rfcs/0026-anyone-can-pay/0026-anyone-can-pay.md) | Xuejie Xiao | Standards Track | Proposal
| [27](rfcs/0027-block-structure) | [CKB Block Structure](rfcs/0027-block-structure/0027-block-structure.md) | Ian Yang | Informational | Draft
| [37](rfcs/0037-ckb2021) | [CKB Consensus Change (Edition CKB2021)](rfcs/0037-ckb2021/0037-ckb2021.md) | Ian Yang | Informational | Draft
| [39](rfcs/0039-cheque) | [Cheque Lock](rfcs/0039-cheque/0039-cheque.md) | Dylan Duan | Standards Track | Proposal |
| [42](rfcs/0042-omnilock) | [Omnilock](rfcs/0042-omnilock/0042-omnilock.md) | Xu Jiandong | Standards Track | Proposal
| [43](rfcs/0043-ckb-softfork-activation) | [CKB Softfork Activation](rfcs/0043-ckb-softfork-activation/0043-ckb-softfork-activation.md) | Dingwei Zhang | Standards Track | Proposal
| [44](rfcs/0044-ckb-light-client) | [CKB Light Client Protocol](rfcs/0044-ckb-light-client/0044-ckb-light-client.md) | Boyu Yang | Standards Track | Proposal
| [45](rfcs/0045-client-block-filter) | [CKB Client Side Block Filter Protocol](rfcs/0045-client-block-filter/0045-client-block-filter.md) | Quake Wang | Standards Track | Proposal
| [46](rfcs/0046-syscalls-summary) | [CKB VM Syscalls Summary](rfcs/0046-syscalls-summary/0046-syscalls-summary.md) | Shan | Informational | Draft
## License

This repository is being licensed under terms of [MIT license](LICENSE).


================================================
File: CHANGELOG.md
================================================
## v2019.02.12


### Bug Fixes

* **0002:** typo ([#69](https://github.com/nervosnetwork/rfcs/issues/69)) ([db4661c](https://github.com/nervosnetwork/rfcs/commit/db4661c))
* **0003:** Remove atomic operation support in CKB VM ([#68](https://github.com/nervosnetwork/rfcs/issues/68)) ([af51e3a](https://github.com/nervosnetwork/rfcs/commit/af51e3a))
* **0014:** url in readme ([#61](https://github.com/nervosnetwork/rfcs/issues/61)) ([558f2ba](https://github.com/nervosnetwork/rfcs/commit/558f2ba))



## v2018.01.28

### Updates

* [RFC0002]: This is a major update to CKB whitepaper, one year after its publication. Jan added the latest results come from discussions and developments and removed obsolete contents. ([#64](https://github.com/nervosnetwork/rfcs/pull/64))
* [RFC0003]: Previously, we keep atomic support in CKB VM hoping for maximum compatibility, but since now rv64imc without atomic support is starting to get popular, we don't need to keep atomic instruction support in our design. ([#68](https://github.com/nervosnetwork/rfcs/issues/68))

## v2018.01.14

### New RFC

* [RFC0013]: block template RFC describes the decentralized CKB mining protocol.
* [RFC0014]: cycle limit RFC describes cycle limits used to regulate VM scripts. CKB VM is a flexible VM that is free to implement many control flow constructs, such as loops or branches. As a result, we will need to enforce certain rules in CKB VM to prevent malicious scripts, such as a script with infinite loops.

### Updates

* [RFC0003]: update CKB VM examples based on latest development ([#63](https://github.com/nervosnetwork/rfcs/issues/63))
* [RFC0006]: use more reasonable proof structure ([#62](https://github.com/nervosnetwork/rfcs/issues/62))


## v2018.12.28

The RFC (Request for Comments) process is intended to provide an open and community driven path for new protocols, improvements and best practices. One month later after open source, we have 11 RFCs in draft or proposal status. We haven't finalized them yet, discussions and comments are welcome.


* [RFC0002] provides an overview of the Nervos Common Knowledge Base (CKB), the core component of the Nervos Network, a decentralized application platform with a layered architecture. The CKB is the layer 1 of Nervos, and serves as a general purpose common knowledge base that provides data, asset, and identity services.
* [RFC0003] introduces the VM for scripting on CKB the layer 1 chain. VM layer in CKB is used to perform a series of validation rules to determine if transaction is valid given transaction's inputs and outputs. CKB uses [RISC-V](https://riscv.org/) ISA to implement VM layer. CKB relies on dynamic linking and syscalls to provide additional capabilities required by the blockchain, such as reading external cells or other crypto computations. Any compilers with RV64I support, such as [riscv-gcc](https://github.com/riscv/riscv-gcc), [riscv-llvm](https://github.com/lowRISC/riscv-llvm) or [Rust](https://github.com/rust-embedded/wg/issues/218) can be used to generate CKB compatible scripts.
* [RFC0004] is the protocol how CKB nodes synchronize blocks via the P2P network. Block synchronization **must** be performed in stages with Bitcoin Headers First style. Block is downloaded in parts in each stage and is validated using the obtained parts.
* [RFC0006] proposes Complete Binary Merkle Tree(CBMT) to generate *Merkle Root*  and *Merkle Proof* for a static list of items in CKB. Currently, CBMT is used to calculate *Transactions Root*. Basically, CBMT is a ***complete binary tree***, in which every level, except possibly the last, is completely filled, and all nodes are as far left as possible. And it is also a ***full binary tree***, in which every node other than the leaves has two children. Compare with other Merkle trees, the hash computation of CBMT is minimal, as well as the proof size.
* [RFC0007] describes the scoring system of CKB P2P Networking layer and several networking security strategies based on it.
* [RFC0009] describes syscalls specification, and all the RISC-V VM syscalls implemented in CKB so far.
* [RFC0010] defines the consensus rule “cellbase maturity period”. For each input, if the referenced output transaction is cellbase, it must have at least `CELLBASE_MATURITY` confirmations; else reject this transaction.
* [RFC0011], transaction filter protocol, allows peers to reduce the amount of transaction data they send. Peer which wants to retrieve transactions of interest, has the option of setting filters on each connection. A filter is defined as a [Bloom filter](http://en.wikipedia.org/wiki/Bloom_filter) on data derived from transactions.
* [RFC0012] proposes a P2P node discovery protocol. CKB Node Discovery Protocol mainly refers to [Satoshi Client Node Discovery](https://en.bitcoin.it/wiki/Satoshi_Client_Node_Discovery), with some modifications to meet our requirements.

[RFC0002]: https://github.com/nervosnetwork/rfcs/blob/master/rfcs/0002-ckb/0002-ckb.md
[RFC0003]: https://github.com/nervosnetwork/rfcs/blob/master/rfcs/0003-ckb-vm/0003-ckb-vm.md
[RFC0004]: https://github.com/nervosnetwork/rfcs/blob/master/rfcs/0004-ckb-block-sync/0004-ckb-block-sync.md
[RFC0006]: https://github.com/nervosnetwork/rfcs/blob/master/rfcs/0006-merkle-tree/0006-merkle-tree.md
[RFC0007]: https://github.com/nervosnetwork/rfcs/blob/master/rfcs/0007-scoring-system-and-network-security/0007-scoring-system-and-network-security.md
[RFC0009]: https://github.com/nervosnetwork/rfcs/blob/master/rfcs/0009-vm-syscalls/0009-vm-syscalls.md
[RFC0010]: https://github.com/nervosnetwork/rfcs/blob/master/rfcs/0010-cellbase-maturity-period/0010-cellbase-maturity-period.md
[RFC0011]: https://github.com/nervosnetwork/rfcs/blob/master/rfcs/0011-transaction-filter-protocol/0011-transaction-filter-protocol.md
[RFC0012]: https://github.com/nervosnetwork/rfcs/blob/master/rfcs/0012-node-discovery/0012-node-discovery.md
[RFC0013]: https://github.com/nervosnetwork/rfcs/blob/master/rfcs/0013-get-block-template/0013-get-block-template.md
[RFC0014]: https://github.com/nervosnetwork/rfcs/blob/master/rfcs/0014-vm-cycle-limits/0014-vm-cycle-limits.md


================================================
File: LICENSE
================================================
The MIT License (MIT)

Copyright 2018 Nervos Foundation

Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.


================================================
File: 0000-template/0000-template.md
================================================
---
Number: "0000"
Category: Informational | Standards
Status: Draft (for Informational) | Proposal (for Standards)
Author: Your Name <your@email.com or @twitter_handle or @github_handle>
Created: YYYY-MM-DD
---

# Put RFC Title Here


================================================
File: rfcs/0001-positioning/0001-positioning.md
================================================
---
Number: "0001"
Category: Informational
Status: Final
Author: The Nervos Team
Created: 2019-09-12
---

# The Nervos Network Positioning Paper

## 1. Purpose of This Paper

The Nervos Network is made up of a number of protocols and innovations. It's important to have clear documentation and technical specifications on key protocol design and implementations - for which we utilize an [RFC](https://github.com/nervosnetwork/rfcs) (request for comment) process. However, we feel it's equally important that we help our communities to understand what we try to accomplish, the trade-offs we have made, and how we have arrived at our current design decisions.

We start this document with a detailed examination of the problems that public permissionless blockchains face today and the existing solutions attempting to solve them. We hope this provides the necessary context for our readers to understand our own rationale on how best to approach these challenges, and our underlying design decisions. We then provide a high-level walkthrough of all parts of the Nervos Network, with a focus on how they work together to support the overall vision of the network.

## 2. Background
 
Scalability, sustainability and interoperability are among the largest challenges public permissionless blockchains face today. While many projects claim to have solutions to these problems, it's important to understand where these problems come from and put solutions in the context of possible trade-offs.

### 2.1 Scalability

Bitcoin[1] was the first public permissionless blockchain, designed to be used as peer-to-peer electronic cash. Ethereum[2] made more use cases possible and created a general purpose decentralized computing platform. However, both of these platforms impose limitations on their transaction capabilities - Bitcoin caps its block size and Ethereum caps its block gas limit. These are necessary steps to ensure long-term decentralization, however they also limit the capabilities of both platforms. 

The blockchain community has proposed many scalability solutions in recent years. In general, we can divide these solutions into two categories: on-chain scaling and off-chain scaling.

On-chain scaling solutions aim to expand the throughput of the consensus process and create blockchains with native throughput that rivals centralized systems. Off-chain scaling solutions only use the blockchain as a secure asset and settlement platform, while moving nearly all transactions to upper layers.

#### 2.1.1 On-chain Scaling with a Single Blockchain

The most straightforward way to increase the throughput of a blockchain is to increase its supply of block space. With additional block space, more transactions can flow through the network and be processed. Increasing the supply of block space in response to increased transaction demand also allows for transaction fees to remain low.

Bitcoin Cash (BCH) adopts this approach to scale its peer-to-peer payment network. The Bitcoin Cash protocol began with a maximum block size of 8 MB, which was later increased to 32 MB, and which will continue to be increased indefinitely as transaction demand increases. For reference, following Bitcoin's (BTC) implementation of Segregated Witness in August 2017, the Bitcoin protocol now allows for an average block size of around 2 MB.

In the scope of a datacenter, the math works out. If 7.5 billion people each create 2 on-chain transactions per day, the network will require production of 26 GB blocks every 10 minutes, leading to a blockchain growth rate of 3.75 TB per day or 1.37 PB per year[3]. These storage and bandwidth requirements are reasonable for any cloud service today.

However, constraining node operation to a datacenter environment leads to a single viable network topology and forces compromises in security (the fork rate of the blockchain will increase as data transmission requirements across the network increase), as well as decentralization (the full node count will be reduced as the cost of consensus participation increases).

From an economic standpoint, an ever-increasing block size does alleviate fee pressure felt by users. Analysis of the Bitcoin network has shown that fees remain flat until a block is about 80% full, and then rise exponentially[4].

Though placing the burden of a growing network's costs on its operators may seem to be a reasonable decision, it could be short-sighted for two reasons: 

- Suppression of transaction fees forces miners to rely predominantly on compensation from new coin issuance (block rewards). Unless inflation is a permanent part of the protocol, new coin issuance will eventually stop (when the total coin hard-cap is reached), and miners will receive neither block rewards nor significant transaction fees. The economic impact of this will severely compromise the security model of the network.
- The cost of running a full node becomes prohibitively expensive. This removes the ability of regular users to independently verify a blockchain's history and transactions, forcing reliance on service providers such as exchanges and payment processors to ensure the integrity of the blockchain. This trust requirement negates the core value proposition of public permissionless blockchains as peer-to-peer, trustless distributed systems. 

Transaction cost optimized platforms such as Bitcoin Cash face significant competition from other blockchains (permissioned and permissionless), as well as traditional payment systems. Design decisions that improve security or censorship resistance will incur associated costs and in turn increase the cost of using the platform. Taking into account a competitive landscape, as well as the network's stated objectives, it is likely that lower costs will be the overarching goal of the network, at the expense of any other considerations.

This goal is consistent with our observations of transactional network usage. Users of these systems are indifferent to significant long-run trade-offs because they will only utilize the network for a short time. Once their goods or services have been received and their payment has been settled, these users no longer have any concern for the network's effective operation. The acceptance of these trade-offs is apparent in the widespread use of centralized crypto-asset exchanges, as well as more centralized blockchains. These systems are popular primarily for their convenience and transactional efficiency.

Some smart contract platforms have taken similar approaches to scaling blockchain throughput, allowing only a limited set of "super computer" validators to participate in the consensus process and independently validate the blockchain.

Though compromises in regard to decentralization and network security allow for cheaper transactions and may be convenient for a set of users, the compromised long-term security model, cost barrier to independently verify transactions, and the likely concentration and entrenchment of node operators lead us to believe that this is not a proper approach for scaling public blockchains.

#### 2.1.2 On-chain Scaling through Multiple Chains

On-chain scaling through multiple chains can be accomplished through sharding, as seen in Ethereum 2.0, or application chains, as seen in Polkadot. These designs effectively partition the global state and transactions of the network into multiple chains, allowing each chain to quickly reach local consensus, and later the entirety of the network to reach global consensus through the consensus of the "Beacon Chain" or the "Relay Chain".

These designs allow the multiple chains to utilize a shared security model, while allowing high throughput and fast transactions inside shards (Ethereum) or para-chains (Polkadot). Though each of these systems is a network of interconnected blockchains, they differ in regard to the protocols running on each chain. In Ethereum 2.0, every shard runs the same protocol, while in Polkadot, each para-chain can run a customized protocol, created through the Substrate framework.

In these multi-chain architectures, each dApp (or instance of a dApp) only resides on a single chain. Though developers today are accustomed to the ability to build dApps that seamlessly interact with any other dApp on the blockchain, design patterns will need to adapt to new multi-chain architectures. If a dApp is split across different shards, mechanisms will be required to keep state synced across different instances of the dApp (residing on different shards). Additionally, though layer 2 mechanisms can be deployed for fast cross-shard communication, cross-shard transactions will require global consensus and introduce confirmation latency. 

With these asynchronous transactions, the infamous "train-and-hotel" problem arises. When two transactions must be atomic (for example booking a train ticket and a hotel room on two different shards), new solutions are required. Ethereum introduces contract "yanking", in which a dependent contract is deleted on one shard, created on a second shard (that contains the other dependent contract), and both transactions are then executed on the second shard. However, the yanked contract would then be unavailable on the original shard, introducing usability issues, and again requiring new design patterns.

Sharding has its own advantages and challenges. If shards can be truly independent and cross-shard needs are minimal, a blockchain can linearly scale its throughput by increasing the number of shards. This is best suited for self-contained applications that don't require outside state or collaboration with other applications. 

A sharded architecture can be problematic for applications that are developed by composing together "building block" applications (this is known as the "composability problem"). Composability is especially relevant in the decentralized finance (DeFi) space, where more advanced products tend to be built on top of other building block products. 

On a more technical note, sharding typically requires a "1 + N" topology, in which N chains connect to one meta-chain, introducing an upper bound on the number of shards a meta-chain can support without itself running into scalability issues.

We observe significant value in a unified global state, allowing an ecosystem of interdependent applications to emerge and developers to innovate at the edges, similar to web developers' use of libraries for lower-level concerns and open APIs for service integration. A much simpler development experience is enabled when developers don't have to consider synchronicity (in cross-shard asset transfer or messaging passing), as well as a superior user experience, resulting from consistency in the architectural concerns of blockchain interactions.

We recognize that sharding is a promising scalability solution (in particular for less interdependent applications), however we believe it is beneficial to have a design that concentrates the most valuable state on a single blockchain, allowing composability. With this design, off-chain scaling approaches are utilized to allow for higher throughput.  

#### 2.1.3 Off-chain Scaling through Layer 2

In layer 2 protocols, the base layer blockchain acts as a settlement (or commitment) layer, while a second layer network routes cryptographic proofs that allow participants to "take delivery of" the cryptocurrency. All activities of the second layer are cryptographically secured by the underlying blockchain and the base layer is only used to settle amounts entering/exiting the second layer network, and for dispute resolution. These designs operate without delegation of custody (or risk of loss) of funds and enable instant, nearly free transactions. 

These technologies demonstrate how a store of value network such as Bitcoin could be used for everyday payments. The most typical example of a layer 2 solution in practice is a payment channel between a customer and a coffee shop. Let's assume Alice visits the Bitcoin Coffee Shop every morning. At the beginning of the month, she deposits funds into a Lightning payment channel she has opened with the coffee shop. As she visits each day, she cryptographically signs the coffee shop's right to take some of the funds, in exchange for her coffee. These transactions happen instantly and are completely peer-to-peer, "off-chain", allowing for a smooth customer experience. The Lightning channel is trustless, Alice or the coffee shop can close the channel at any time, taking the funds they are owed at that time. 

Payment channel technologies such as Lightning are only one example of an off-chain scaling technique; there are many maturing technologies that can safely scale blockchain throughput in this way. While payment channels include off-chain agreements to channel balances between two parties, state channels include off-chain agreements to arbitrary state between channel participants. This generalization can be the basis of scalable, trustless, decentralized applications. A single state channel can even be utilized by multiple applications, allowing for even greater efficiency. When one party is ready to exit the channel, they can submit the agreed upon cryptographic proof to the blockchain, which will then execute the agreed state transitions.

A side-chain is another construction that allows for increased throughput, though via trusted third party blockchain operators. With a two-way peg to a blockchain with reliable, trustless consensus, funds can be moved back and forth between the main-chain and side-chain. This allows for a high volume of trusted transactions on the side-chain, with later net settlement on the main-chain. Side-chain transactions have minimal fees, fast confirmation and high throughput. Though side-chains offer a superior experience in some regard, they do compromise on security. There is however, a great deal of research into trustless side-chains, which can provide the same performance improvements without compromising security.

An example of a trustless side-chain technology is Plasma (covered in 5.4), a side-chain architecture that leverages a trust root on a blockchain with broad global consensus. Plasma chains offer the same performance improvements as centralized side-chains, however do so while offering security guarantees. In the event a Plasma chain operator is malicious or malfunctioning, users are provided a mechanism that allows them to safely withdraw their side-chain assets to the main-chain. This is done without the cooperation of the Plasma chain operator, offering users the convenience of side-chain transactions, as well as the security of a layer 1 blockchain.

Off-chain scaling allows for decentralization, security and scalability. By moving everything except settlement transactions and disputes off-chain, a public blockchain's limited global consensus is efficiently utilized. Diverse layer 2 protocols can be implemented based on application requirements, affording flexibility to developers and users. As more participants are added to the network, performance is not impacted and all parties can share the security guarantees offered by layer 1 consensus.

### 2.2 Sustainability

Sustaining the long-term operation of an autonomous, ownerless public blockchain presents quite the challenge. Incentives must be balanced among diverse stakeholders and the system must be designed in a way that allows for widespread full node operation and public verifiability. Hardware requirements must remain reasonable, while supporting an open, global network.

Additionally, once a public blockchain is in operation, it is very difficult to change the underlying rules governing the protocol. From the start, the system must be designed to be sustainable. In this interest, we have conducted a thorough inventory of the challenges in building sustainable, permissionless blockchains.

#### 2.2.1 Decentralization

One of the largest long-term threats public blockchains face is an ever-increasing barrier of independent participation and transaction verification, reflected in the cost of full node operation. Full nodes allow blockchain participants to independently verify the on-chain state/history, and hold miners or validators of the network accountable by refusing to route invalid blocks. As the cost of full nodes increases and their numbers decline, participants in the network are increasingly forced to rely on professional service operators to provide both history and current state, eroding the fundamental trust model of open and permissionless blockchains. 

For a full node to keep up with the progression of the blockchain, it must have adequate computational throughput to validate transactions, bandwidth throughput to receive transactions, and storage capacity to store the entire global state. To control a full node's operating cost, the protocol has to take measures to bound the throughput or capacity growth of all three of these resources. Most blockchain protocols bound their computational or bandwidth throughput, but very few bound the growth of the global state. As these chains grow in size and length of operation, full node operation costs will irreversibly increase.

#### 2.2.2 Economic Models

While there has been a lot of research into consensus protocols in recent years, we believe crypto-economics is an understudied field. Broadly speaking, current crypto-economic models for layer 1 protocols are primarily focused on incentives and punishments to ensure network consensus, and native tokens are mostly used to pay transaction fees or to satisfy staking requirements that provide Sybil resistance.

We believe that a well-designed economic model should go beyond the consensus process and ensure the long-term sustainability of the protocol as well. In particular, the economic model should be designed with the following goals:

- the network should have a sustainable way to compensate service providers (typically miners or validators), ensuring that the network remains sustainably secure
- the network should have a sustainable way to maintain a low barrier to participation, ensuring that the network remains decentralized over time
- the resources of the public network should be efficiently and fairly allocated
- the blockchain's native token must have intrinsic value

#### 2.2.3 Analysis of Bitcoin's Economic Model

The Bitcoin protocol caps the size of blocks and enforces a fixed block time. This makes the network's bandwidth throughput a scarce resource that users must bid on through transaction fees. Bitcoin Script doesn't allow loops, making the length of the script a good approximation of its computational complexity. In general, greater demand for block space translates into higher transaction fees for users. Additionally, the more inputs, outputs or computational steps that are involved in a transaction, the more a user will also pay in transaction fees.

The intrinsic value of Bitcoin comes almost entirely from its monetary premium (society's willingness to treat it as money) and in particular, the willingness to hold it as a store of value. Because miner income is denominated in BTC, this perception has to hold for Bitcoin's economic model to be sustainable. In other words, Bitcoin's security model is circular - it depends on the collective belief that the network is sustainably secure and can therefore be used as a monetary store of value.

Bitcoin's block size cap effectively sets the barrier for network participation - the lower the block size cap is, the easier it is for non-professionals to run full nodes. The Bitcoin global state is its UTXO set, with its growth rate also effectively capped by the block size limit. Users are incentivized to create and utilize UTXOs efficiently; creating more UTXO's translates into higher transaction fees. However, no incentives are provided to encourage combining of UTXOs and reduction of the size of the global state; once a UTXO is created, it will occupy the global state for free until it is spent.

Bitcoin's transaction fee-based economic model is a fair model to allocate its bandwidth throughput, the scarce resource imposed by the protocol. It's a suitable economic model for a peer-to-peer payment system, but is a poor choice for a true store of value platform. Bitcoin users that utilize the blockchain to store value pay transaction fees only once, but can then occupy state forever, enjoying ongoing security provided by miners, who are required to make continuous resource investments.

Bitcoin has a total supply hard-cap and its new issuance via block rewards will eventually drop to zero. This could cause two problems:

First, if Bitcoin continues to succeed as a store of value, the unit value of BTC will continue to increase, and the total value the network secures will also increase (as more monetary value moves on to the network). A store of value platform has to be able to raise its security budget as the value it protects increases over time, otherwise, it invites attackers to double spend and steal the assets of the network. 

When the cost to break protocol security is less than the profit they can earn acting honestly, attackers will always attack. This is analogous to a city that has to raise its military spending as the wealth inside the city increases. Without this investment, sooner or later the city will be attacked and looted. 

With the existence of block rewards, Bitcoin is able to scale security to the aggregate value it stores - if Bitcoin's price doubles, the income that miners receive from block rewards will also double, therefore they can afford to produce twice the hash rate, making the network twice as expensive to attack. 

This however changes when the predictable block rewards drop to zero. Miners will have to rely entirely on transaction fees; their income will no longer scale to the value of the Bitcoin asset, but will be determined by the transaction demand of the network. If transaction demand is not high enough to fill the available block space, total transaction fees will be minuscule. Since transaction fees are strictly a function of block space demand and independent from the price of a Bitcoin, this will have a profound impact on Bitcoin's security model. For Bitcoin to remain secure, we'd have to assume consistent, over-capacity transaction demand, that also scales to the price of Bitcoin. These are very strong assumptions. 

Second, when the predictable block rewards stop, variance in per block income for miners increases, and provides incentives for miners to fork, instead of advancing the blockchain. In the extreme case, when a miner's mempool is empty and they receive a block loaded with fees, their incentive is to fork the chain and steal the fees, as opposed to advancing the chain and producing a block with potentially no income[5]. This is known as the "fee sniping" challenge in the Bitcoin community, to which a satisfying solution has not yet been found, without removing Bitcoin's hard-cap.

#### 2.2.4 Analysis of the Economic Model of Smart Contract Platforms

The typical economic model of smart contract platforms faces even more challenges. Let's use Ethereum as an example. Ethereum's scripting allows loops, therefore the length of a script doesn't reflect the script's computational complexity. This is the reason Ethereum doesn't cap block size or bandwidth throughput, but computational throughput (expressed in the block gas limit).

To get their transactions recorded on the Ethereum blockchain, users bid on the per computation cost they're willing to pay in transaction fees. Ethereum uses the concept of "gas" as measurement of computational cost priced in ETH, and the "gas price" rate control ensures that the cost per step of computation is independent of price movements of the native token. The intrinsic value of the ETH token comes from its position as the payment token of the decentralized computation platform; it is the only currency that can be used to pay for computation on Ethereum.

Ethereum's global state is represented with the EVM's state trie, the data structure that contains the balances and internal state of all accounts. When new accounts or contract values are created, the size of the global state expands. Ethereum charges fixed amounts of gas for insertion of new values into its state storage and offers a fixed "gas stipend" that offsets a transaction's gas costs when values are removed. 

A "pay once, occupy forever" storage model doesn't match the ongoing cost structure of miners and full nodes, and the model provides no incentive for users to voluntarily remove state or remove state sooner. As a result, Ethereum has experienced rapid growth of its state size. A larger state size slows down transaction processing and raises the operating cost of full nodes. Without strong incentives to clear state, this is a trend that's bound to continue.

Similar to Bitcoin, Ethereum's demand-driven gas pricing is a fair model to allocate its computational throughput, the platform's scarce resource. The model also serves Ethereum's purpose as a decentralized computation system. However, its state storage fee model doesn't match its potential proposition as a decentralized state or asset storage platform. Without a cost for long-term state storage, it will always be in users' interests to occupy state forever for free. Without scarcity of state storage capacity, neither a market, nor supply and demand dynamics can be established. 

Unlike Bitcoin, which specifies the block size limit in its core protocol, Ethereum allows miners to dynamically adjust the block gas limit when they produce blocks. Miners with advanced hardware and significant bandwidth are able to produce more blocks, effectively dominating this voting process. Their interest is to adjust the block gas limit upward, raise the bar of participation and force smaller miners out of the competition. This is another factor that contributes to the quickly rising cost of full node operation.

Smart contract platforms like Ethereum are multi-asset platforms. They support issuance and transactions of all types of crypto-assets, typically represented as "tokens". They also provide security to not only their own native tokens, but the value of all crypto-assets on the platform. "Store of value" in a multi-asset context therefore refers to the value preservation property that benefits both the platform's native tokens and the crypto-assets stored on the platform.

With its block rewards, Bitcoin has an excellent "store of value" economic model. Miners are paid a fixed block reward denominated in BTC, and thus their income rises along with the price of BTC. Therefore, the platform has the ability to raise revenue for miners to increase security (measured by the cost of attack) while maintaining a sustainable economic model.

For multi-asset platforms, it becomes much more challenging to fulfill this requirement, because "value" can be expressed with crypto-assets beyond the native token. If the value of crypto-assets secured by the platform increases, but network security doesn't, it becomes more profitable to attack the platform's consensus process to double spend crypto-assets stored on the platform.

For a multi-asset smart contract platform to function as a store of value, proper incentives must be put in place to align in the growth in value of a network's assets with its underlying security. Or put another way, the platform's native token must be a good value capture of the platform's aggregate asset value. If the intrinsic value of a platform's native token is limited to transaction fee payment, its value would be determined solely by transaction demand, instead of the demand of asset storage.

Smart contract platforms that are not designed to function as a store of value have to rely on the native token's monetary premium (the willingness of people to hold the tokens beyond their intrinsic value) to support its ongoing security. This is only feasible if one platform dominates with unique features that can't be found elsewhere, or out-competes others by delivering the lowest possible cost of transactions.

Ethereum currently enjoys such dominance and can therefore maintain its monetary premium. However, with the rise of competing platforms, many designed for higher TPS and providing similar functionality, it's an open question as to whether reliance on a monetary premium alone can sustain a blockchain platform's security, especially if the native tokens are explicitly not designed or believed to be money. Furthermore, even if a platform can provide unique features, its monetary premium can be abstracted away by the user interface through efficient swaps (very likely when mass adoption of blockchain finally comes). Users would hold assets they're most familiar with, such as Bitcoin or stable coins, and acquire platform tokens just in time to pay for transaction fees. In either case, the foundation of a platform's crypto-economics would collapse.

Layer 1 multi-asset platforms have to provide sustainable security for all of the crypto-assets they secure. In other words, they have to have an economic model designed for a store of value.

#### 2.2.5 Funding of Core Protocol Development

Public permissionless blockchains are public infrastructure. Initial development of these systems requires a great deal of funding, and once they are in operation require ongoing maintenance and upgrades. Without dedicated people maintaining these systems, they run the risk of catastrophic bugs and sub-optimal operation. The Bitcoin and Ethereum protocols do not provide a native mechanism to ensure funding of ongoing development, thus rely on the continued engagement of businesses with aligned interests and altruistic open source communities. 

Dash was the first project to utilize a treasury to ensure ongoing development was funded in-protocol. While sustainably supporting the protocol's development, this design makes a compromise in regard to the sustainability of the value of the cryptocurrency. Like most blockchain treasuries, this model relies on inflation-based funding, which erodes the value of long-term holdings. 

The Nervos Network uses a treasury model that provides sustainable funding for core development. Treasury funds come from targeted inflation of short-term token holders, while the effects of this inflation are mitigated for long-term holders. More information about this mechanism is described in (4.6).

### 2.3 Interoperability

Interoperability across blockchains is an often-discussed topic, and many projects have been proposed specifically to address this challenge. With reliable transactions across blockchains, true network effects can be realized in the decentralized economy. 

The first example of blockchain interoperability was atomic swaps between Bitcoin and Litecoin. The trustless exchange of Bitcoin for Litecoin and vice-versa is made possible not through in-protocol mechanisms, but through a shared cryptographic standard (specifically usage of the SHA2-256 hash function).

Similarly, the design of Ethereum 2.0 allows for interconnection of many shard chains, all running the same protocol and utilizing the same cryptographic primitives. This uniformity will be valuable when customizing the protocol for inter-shard communication, however Ethereum 2.0 will not be interoperable with other blockchains that do not utilize the same cryptographic primitives.

Networks of blockchains such as Polkadot or Cosmos go one-step further, allowing blockchains built with the same framework (Cosmos SDK for Cosmos and Substrate for Polkadot) to communicate and interact with one another. These frameworks provide developers some flexibility in building their own protocols, and ensure the availability of identical cryptographic primitives, allowing each chain to parse one another's blocks and cross-validate transactions. However, both protocols rely on bridges or "pegging zones" to connect to blockchains that are not constructed with their own frameworks, introducing an additional layer of trust. To demonstrate: though Cosmos and Polkadot enable "networks of blockchains", the Cosmos and Polkadot networks are not designed to be interoperable with each other. 

The crypto-economics of cross-chain networks may need further study as well. For both Cosmos and Polkadot, native tokens are used for staking, governance and transaction fees. Putting aside the crypto-economic dynamics introduced by staking, which can't alone give a native token intrinsic value (discussed in 4.2.4), reliance on cross-chain transactions to capture ecosystem value can be a weak model. In particular, cross-chain transactions are a weakness, not a strength of multi-chain networks, just as cross-shard transactions are a weakness of sharded databases. They introduce latency, as well as the loss of atomicity and composability. There is a natural tendency for applications that need to interact with each other to eventually move to reside on the same blockchain to reduce cross-chain overhead, reducing the demand for cross-chain transactions and therefore demand for the native token.

Cross-chain networks benefit from network effects - the more interconnected chains there are in a network, the more valuable the network is, and the more attractive it is to potential new participants in the network. Ideally, such value would be captured by the native token and used to further encourage the growth of the network. However, in a pooled security network such as Polkadot, higher cost of network participation becomes a deterrent for the network to accrue further value. In a loosely connected network like Cosmos, if we assume same cross-chain transaction demand and fees, higher cost of staking participation lowers the expected return for validators, discouraging further staking participation.

With its layered approach, the Nervos Network is also a multi-chain network. Architecturally, Nervos uses the cell model and a low-level virtual machine to support true customization and user-created cryptographic primitives, enabling interoperability across heterogeneous blockchains (covered in 4.4.1). Crypto-economically, the Nervos Network concentrates value (instead of message passing) to its root chain. This mechanism raises the network's security budget as the aggregate value secured by the network rises. This is covered in detail in (4.4).

## 3. Core Principles of the Nervos Network

Nervos is a layered network built to support the needs of the decentralized economy. There are several reasons that we believe a layered approach is the right way to build a blockchain network. There are many well known trade-offs in building blockchain systems, such as decentralization vs. scalability, neutral vs. compliant, privacy vs. openness, store of value vs. transaction cost and cryptographic soundness vs. user experience. We believe that all of these conflicts arise because of attempts to address completely opposing concerns with a single blockchain. 

We believe that the best way to construct a system is not to build an all-encompassing single layer, but rather to decouple concerns and address them at different layers. By doing this, the layer 1 blockchain can focus on being secure, neutral, decentralized and open public infrastructure, while smaller, layer 2 networks can be specially-designed to best suit the context of their usage.

In the Nervos Network, the layer 1 protocol (the Common Knowledge Base) is the value preservation layer of the entire network. It is philosophically inspired by Bitcoin and is an open, public and proof of work-based blockchain, designed to be maximally secure and censorship-resistant, to serve as a decentralized custodian of value and crypto-assets. Layer 2 protocols leverage the security of the layer 1 blockchain to provide unbounded scalability and minimal transaction fees, and also allow for application-specific trade-offs in regard to trust models, privacy and finality.

Here are the core principles that led to the design of the Nervos Network:

- A sustainable, multi-asset layer 1 blockchain has to be crypto-economically designed to be a store of value.
- Layer 2 offers the best scaling options, bringing nearly unlimited transactional capabilities, minimal transaction costs and an improved user experience. Layer 1 blockchains should be designed to complement, not compete with layer 2 solutions.
- Proof of Work as a Sybil resistance method is essential for layer 1 blockchains.
- The layer 1 blockchain must provide a generic programming model for interactive protocols and blockchain interoperability, and to allow the protocol to be maximally customizable and easy to upgrade.
- To best allocate resources and avoid the "tragedy of the commons", state storage has to have a clear and fine-grained ownership model. To deliver consistent long-term rewards to miners (regardless of transaction demand), state occupation must have an ongoing cost.

## 4. The Nervos Common Knowledge Base

### 4.1 Overview

"Common knowledge" is defined as knowledge that is known by everyone or nearly everyone, usually with reference to the community in which the term is used. In the context of blockchains in general, and the Nervos Network in particular, "common knowledge" refers to state verified by global consensus and accepted by all in the network.

The properties of common knowledge allow us to collectively treat the cryptocurrency stored on public blockchains as money. For example, the balances and history of all addresses on Bitcoin are common knowledge for Bitcoin users, because they are able to independently replicate the shared ledger, verify the global state since the genesis block, and know that anyone else can do the same. This common knowledge allows people to transact completely peer-to-peer without putting trust in any third party.

The Nervos Common Knowledge Base (CKB) is designed to store all kinds of common knowledge, not limited to money. For example, the CKB could store user-defined crypto-assets, such as fungible and non-fungible tokens, as well as valuable cryptographic proofs that provide security for higher-layer protocols, such as payment channels (5.2) and commit chains (5.4).

Both Bitcoin and the Nervos CKB are common knowledge storage and verification systems. Bitcoin stores its global state as the UTXO set, and verifies state transitions through hard-coded rules and scripts embedded in transactions. The Nervos CKB generalizes Bitcoin's data structure and scripting capabilities, stores global state as the set of active programmable cells, and verifies state transitions through user-defined, Turing-complete scripts that run in a virtual machine.

While the Nervos CKB has full smart contract capabilities like those of Ethereum and other platforms, its economic model is designed for common knowledge preservation, instead of payment for decentralized computation.

### 4.2 Consensus

Bitcoin's Nakamoto Consensus (NC) is well-received due to its simplicity and low communication overhead. However, NC suffers from two drawbacks: 1) its transaction processing throughput is far from satisfactory, and 2) it is vulnerable to selfish mining attacks, in which attackers can gain additional block rewards by deviating from the protocol's prescribed behavior.

The CKB consensus protocol is a variant of NC that raises its performance limit and selfish mining resistance while keeping its merits. By identifying and eliminating the bottleneck in NC's block propagation latency, our protocol supports very short block intervals without sacrificing security. A shortened block interval not only increases throughput, but also lowers transaction confirmation latency. By incorporating all valid blocks into the difficulty adjustment calculation, selfish mining is no longer profitable in our protocol.

#### 4.2.1 Increasing Throughput

Nervos CKB increases the throughput of PoW consensus with a consensus algorithm derived from Nakamoto Consensus. The algorithm uses the blockchain's orphan rate (the percentage of valid blocks that are not part of the canonical chain) as a measurement of connectivity across the network.

The protocol targets a fixed orphan rate. In response to a low orphan rate target difficulty is lowered (increasing the rate of block production) and when the orphan rate crosses a defined threshold, target difficulty is increased (decreasing the rate of block production).

This allows for utilization of the network's entire bandwidth capabilities. A low orphan rate indicates that the network is well-connected and can handle greater data transmission; the protocol then increases throughput under these conditions.

#### 4.2.2 Eliminating the Block Propagation Bottleneck

The bottleneck in any blockchain network is block propagation. The Nervos CKB consensus protocol eliminates the block propagation bottleneck by modifying transaction confirmation into a two step process: 1) propose and 2) commit.

A transaction must first be proposed in the "proposal zone" of a block (or one of its uncles). The transaction will then be committed if it appears in a block's "commitment zone" within a defined window following its proposal. This design eliminates the block propagation bottleneck, as a new block's committed transactions will have already been received and verified by all nodes when proposed.

#### 4.2.3 Mitigating Selfish Mining Attacks

One of the most fundamental attacks on Nakamoto Consensus is selfish mining. In this attack, malicious miners gain unfair block rewards by deliberately orphaning blocks mined by others.

Researchers observe that the unfair profit opportunity is rooted in the difficulty adjustment mechanism of Nakamoto Consensus, which neglects orphaned blocks when estimating the network's computing power. This leads to lower mining difficulty and higher time-averaged block rewards.

The Nervos CKB consensus protocol incorporates uncle blocks into the difficulty adjustment calculation, making selfish mining no longer profitable. This holds regardless of attack strategy or duration; a miner is unable to gain unfair rewards through any combination of honest and selfish mining.

Our analysis shows that with a two-step transaction confirmation process, de facto selfish mining is also eliminated via a limited attack time window.

For a detailed understanding of our consensus protocol, please read [here](https://github.com/nervosnetwork/rfcs/blob/master/rfcs/0020-ckb-consensus-protocol/0020-ckb-consensus-protocol.md).

#### 4.2.4 Proof of Work vs Proof of Stake

Proof of Work (PoW) and Proof of Stake (PoS) systems are both vulnerable to concentrations of power, however the qualities of the systems provide very different operating realities for those in power.

PoW mining incurs real-world expenses that can exceed mining proceeds without diligent cost supervision. Those in power are required to stay innovative, pursue sound business strategies and continue to invest in infrastructure to remain dominant. Mining equipment, mining pool operations and access to cheap energy are all subject to changes from technological innovation. It is difficult to maintain monopolization of all three over long periods of time.

In contrast, block creators in PoS systems are rewarded in a deterministic way, based on amount staked, with very low operational capital requirements. As the system grows, the impact of natural advantages provided to first moving businesses and individuals will grow. In a PoS system, it is possible that power concentrates in the hands of a few stakers. Though PoW systems have a similar problem with mining concentration, the cost to remain in power in a PoS system is significantly lower.

In addition, PoS validators have one unique power: control of the validator set. Acceptance of a transaction that allows a validator to join the consensus group is in the hands of existing validators. Colluding efforts to influence the validator set through transaction censorship and ordering manipulation would be difficult to detect, as well as difficult to punish. Conversely, consensus participation in PoW systems is truly open and isn't subject to the current power structure. Advantages are not given to early participants of the system.

Regarding token economics, while it is believed that staking can attract capital looking to earn yield (and therefore increase demand for the native token), this is not the whole picture. All PoS projects will eventually see their staking rate stabilize, and capital entering and leaving the pool of staked capital would then be roughly the same. The staking mechanism by itself will not increase demand for the native token. In other words, though the introduction of staking provides demand for the native token in the initial phase of a project (as the staking rate rises), staking alone can't provide long-term demand for the native token and therefore can't be a native token's only intrinsic value.

Long-term token holders in a PoS system have 3 options: they can 1) manage infrastructure and run a validating node on their own to receive new issuance, 2) delegate their tokens to a third party and trust their integrity and infrastructure, or 3) have the value of their tokens diluted by ongoing issuance. None of these options are particularly attractive to long-term, store of value oriented token holders.

We believe that PoW's permissionless participation is a requirement for infrastructure at the foundation of global economic activity. The foremost goal of layer 1 is to ensure that the blockchain is as decentralized, secure and neutral as possible. While PoS systems have a role to play in the decentralized economy, in our opinion they do not meet the requirements of a truly open and decentralized layer 1.

#### 4.2.5 Proof of Work Function

Nervos CKB blocks can be proposed by any node, provided that 1) the block is valid; and 2) the proposer has solved a computationally difficult puzzle called the proof-of-work. The proof-of-work puzzle is defined in terms of the block that is being proposed; this guarantees that the solution to the puzzle uniquely identifies a block.

Bitcoin's proof-of-work requires finding a valid nonce such that the result of applying a hash function on the block header satisfies a certain level of difficulty. For Bitcoin, the hash function is twice-iterated SHA2–256. While SHA2 was a good choice for Bitcoin, the same is not true for cryptocurrencies that come after it. A large amount of dedicated hardware has been developed to mine Bitcoin, a great deal of which sits idle, having been rendered obsolete by efficiency improvements. 

A new cryptocurrency utilizing the same proof-of-work puzzle would make this deprecated hardware useful once again. Even up-to-date hardware can be rented and re-purposed to mine a new coin. The distribution of mining power for a SHA2-based coin would be very difficult to predict and susceptible to sudden and large changes. This argument also applies to algorithmic optimizations tailored to SHA2, which have been developed to make software computation of the function cheaper as well.

For a new cryptocurrency, it makes sense to define the proof-of-work puzzle in terms of a function that has not yet been used by other cryptocurrencies. For Nervos CKB, we went a step further and chose to define it in terms of a proof-of-work function that could not have been the subject of premature optimization, because it is new.

However, the intended unavailability of mining hardware is only the case initially. In the long run, deployments of dedicated mining hardware are beneficial, significantly increasing the challenges of attacking the network. Therefore, in addition to being new, an ideal proof-of-work function for a new cryptocurrency is also simple, significantly lowering the barrier for hardware development.

Security is the obvious third design goal. While a known vulnerability could be exploited by all miners equally, and would merely result in a higher difficulty, an undisclosed vulnerability could lead to a mining optimization that provides the discoverer(s) an advantage in excess of their contributed mining power share. The best way to avoid this situation is to make a strong argument for invulnerability.

#### 4.2.6 Eaglesong

Eaglesong is a new hash function developed specifically for Nervos CKB proof-of-work, but is also suitable in other use cases in which a secure hash function is needed. The design criteria were exactly as listed above: novelty, simplicity and security. We wanted a design that was simultaneously novel enough to constitute a small step forward for science, as well as close enough to existing designs to make a strong security argument. 

To this end, we chose to instantiate the sponge construction (as used in Keccak/SHA3) with a permutation built from ARX operations (addition, rotation, and xor); the argument for its security is based on the wide trail strategy (the same argument underlying AES).

To the best of our knowledge, Eaglesong is the first hash function (or function, for that matter) that successfully combines all three design principles.

You can read more about Eaglesong [here](https://medium.com/nervosnetwork/the-proof-of-work-function-of-nervos-ckb-3cc8364464d9).


### 4.3 Cell Model

Nervos CKB utilizes the Cell Model, a new construction that can provide many of the benefits of the Account model (utilized in Ethereum), while preserving the asset ownership and proof-based verification properties of the UTXO model (utilized in Bitcoin).

The cell model is focused on state. Cells contain arbitrary data, which could be simple, such as a token amount and an owner, or more complex, such as code specifying verification conditions for a token transfer. The CKB's state machine executes scripts associated with cells to ensure the integrity of a state transition.

In addition to storing data of their own, cells can reference data in other cells. This allows for user-owned assets and the logic governing them to be separated. This is in contrast to account-based smart contract platforms, in which state is internal property of a smart contract and has to be accessed through smart contract interfaces. On Nervos CKB, cells are independent state objects that are owned, and can be referenced and passed around directly. Cells can express true "bearable assets", belonging to their owners (just as UTXOs are bearable assets to Bitcoin owners), while referencing a cell that holds logic ensuring the integrity of state transitions.

Cell model transactions are also state transition proofs. A transaction's input cells are removed from the set of active cells and output cells are added to the set. Active cells comprise the global state of the Nervos CKB, and are immutable: once cells have been created, they cannot be changed. 

The Cell model is designed to be adaptable, sustainable, and flexible. It can be described as a generalized UTXO model and can support user-defined tokens, smart contracts and diverse layer 2 protocols.

For deeper understanding of the Cell Model, please see [here](https://medium.com/nervosnetwork/https-medium-com-nervosnetwork-cell-model-7323fca57571).


### 4.4 Virtual Machine

While many next-generation blockchain projects utilize WebAssembly as the foundation of a blockchain virtual machine, Nervos CKB includes the unique design choice of a virtual machine (CKB-VM) based on the RISC-V instruction set.

RISC-V is an open-source RISC instruction set architecture that was created in 2010 to facilitate development of new hardware and software, and is a royalty-free, widely understood and widely audited instruction set.

We have found numerous advantages to using RISC-V in a blockchain context:

- Stability: The RISC-V core instruction set has been finalized and frozen, as well as widely implemented and tested. The core RISC-V instruction set is fixed and will never require an update.
- Open and Supported: RISC-V is provided under a BSD license and supported by compilers such as GCC and LLVM, with Rust and Go language implementations under development. The RISC-V Foundation includes more than 235 member organizations furthering the instruction set's development and support.
- Simplicity and Extensibility: The RISC-V instruction set is simple. With support for 64-bit integers, the set contains only 102 instructions. RISC-V also provides a modular mechanism for extended instruction sets, enabling the possibility of vector computing or 256-bit integers for high-performance cryptographic algorithms.
- Accurate Resource Pricing: The RISC-V instruction set can be run on a physical CPU, providing an accurate estimation of the machine cycles required for executing each instruction and informing virtual machine resource pricing.

CKB-VM is a low-level RISC-V virtual machine that allows for flexible, Turing-complete computation. Through use of the widely implemented ELF format, CKB-VM scripts can be developed with any language that can be compiled to RISC-V instructions.

#### 4.4.1 CKB-VM and the Cell Model

Once deployed, existing public blockchains are more or less fixed. Upgrading foundational elements, such as cryptographic primitives, involve multi-year undertakings or are simply not possible.

CKB-VM takes a step back, and moves primitives previously built into custom VMs to cells on top of the virtual machine. Though CKB scripts are more low-level than smart contracts in Ethereum, they carry the significant benefit of flexibility, enabling a responsive platform and foundation for the progressing decentralized economy.

Cells can store executable code and reference other cells as dependencies. Almost all algorithms and data structures are implemented as CKB scripts stored within cells. By keeping the VM as simple as possible and offloading program storage to cells, updating key algorithms is as simple as loading the algorithm into a new cell and updating existing references.

#### 4.4.2 Running Other Virtual Machines on the CKB-VM

Thanks to the low-level nature of the CKB-VM and the availability of tooling in the RISC-V community, it's easy to compile down other VMs (such as Ethereum's EVM) directly into the CKB-VM. This has several advantages: 

- Smart contracts written in specialized languages running on other virtual machines can be easily ported to run on the CKB-VM. (Strictly speaking, they'd be running on their own VM that's compiled to run inside of the CKB-VM.)
- The CKB can verify dispute resolution state transitions of layer 2 transactions, even if the rules of the state transitions are written to run in a virtual machine other than CKB-VM. This is one of the key requirements to support trustless layer 2 general purpose side-chains. 

For a technical walkthrough of the CKB-VM, please see [here](https://medium.com/nervosnetwork/an-introduction-to-ckb-vm-9d95678a7757).

### 4.5 Economic Model

The native token of the Nervos CKB is the "Common Knowledge Byte", or CKByte for short. CKBytes entitle a token holder to occupy part of the total state storage of the blockchain. For example, by holding 1000 CKBytes, a user is able to create a cell of 1000 bytes in capacity or multiple cells adding up to 1000 bytes in capacity. 

Using CKBytes to store data on the CKB creates an opportunity cost to CKByte owners; they will not be able to deposit occupied CKBytes into the NervosDAO to receive a portion of the secondary issuance. CKBytes are market priced, and thus an economic incentive is provided for users to voluntarily release state storage to meet the high demand of expanding state. After a user releases state storage, they will receive an amount of CKBytes equivalent to the size of state (in bytes) their data was occupying.

The economic model of the CKB allows issuance of the native token to bound state growth, maintaining a low barrier of participation and ensuring decentralization. As CKBytes become a scarce resource, they can be priced and allocated most efficiently.

The genesis block of the Nervos Network will contain 33.6 billion CKBytes, of which 8.4 billion will be immediately burned. New issuance of CKBytes includes two parts - base issuance and secondary issuance. Base issuance is limited to a finite total supply (33.6 billion CKBytes), with an issuance schedule similar to Bitcoin. The block reward halves approximately every 4 years, until reaching 0 new issuance. All base issuance is awarded to miners as incentives to protect the network. The secondary issuance has a constant issuance rate of 1.344 billion CKBytes per year and is designed to impose an opportunity cost for state storage occupation. After the base issuance stops, there will only be secondary issuance.

Nervos CKB includes a special smart contract called the NervosDAO, which functions as an "inflation shelter" against the effects of the secondary issuance. CKByte owners can deposit their tokens into the NervosDAO and receive a portion of secondary issuance that exactly offsets inflationary effects from secondary issuance. For long-term token holders, as long as they lock their tokens in the NervosDAO, the inflationary effect of secondary issuance is only nominal. With the effects of secondary issuance mitigated, these users are effectively holding hard-capped tokens like Bitcoin.

While CKBytes are being used to store state, they cannot be used to earn secondary issuance rewards through the NervosDAO. This makes the secondary issuance a constant inflation tax, or "state rent" on state storage occupation. This economic model imposes state storage fees proportional to both the space and time of occupation. It is more sustainable than the "pay once, occupy forever" model used by other platforms, and is more feasible and user-friendly than other state rent solutions that require explicit payments.

Miners are compensated with both block rewards and transaction fees. For block rewards, when a miner mines a block, they would receive the block's full base issuance reward, and a portion of secondary issuance. The portion is based on state occupation, for example: if half of all native tokens are being used to store state, a miner would receive half of the secondary issuance reward for the block. Additional information about the distribution of secondary issuance is included in the next section (4.6). In the long term, when base issuance stops, miners will still receive "state rent" income that's independent of transactions, but tied to the adoption of the Nervos Common Knowledge Base.

In an analogy, CKBytes can be thought of as land, while crypto-assets stored on the CKB can be thought of as houses. Land is required to build a house, and CKBytes are required to store assets on the CKB. As demand to store assets on CKB rises, demand for CKBytes rises as well. As the value of assets stored rises, the value of CKBytes rises as well.

The Nervos CKB is designed to translate demand for a multitude of assets into demand for a single asset, and use it to compensate the miners to secure the network.

For more detailed explanation on the economic model, please see [here](https://github.com/nervosnetwork/rfcs/blob/master/rfcs/0015-ckb-cryptoeconomics/0015-ckb-cryptoeconomics.md). 


### 4.6 Treasury

The portion of secondary issuance that doesn't go to 1) miners or 2) long-term holders with tokens locked in the NervosDAO, will go toward a treasury fund. To demonstrate: if 60% of issued CKBytes are used to store state and 30% of the CKBytes are deposited into the NervosDAO, miners will receive 60% of the secondary issuance, the NervosDAO (long-term holders) will receive 30% of the secondary issuance, and 10% of the secondary issuance will go to the treasury. 

The treasury fund will be used to fund ongoing research and development of the protocol, as well as building the ecosystem of the Nervos Network. The use of the treasury funds will be open, transparent and on-chain for everyone to see. Compared to an inflation-based treasury funding model, this model doesn't dilute long-term token holders (who have deposited their tokens into the NervosDAO). Funding of protocol development is strictly derived from the opportunity cost to short-term token holders.

The treasury won't be activated immediately upon the main-net launch of the Nervos Common Knowledge Base. With the community's approval, it will be activated with a hard-fork later, only after the Nervos Foundation has exhausted the Ecosystem Fund, included in the Genesis block. Prior to activation of the treasury, this portion of the secondary issuance will be burned.


### 4.7 Governance

Governance is how society or groups within it organize to make decisions. Every relevant party with an interest in the system should be involved in this process. In regard to a blockchain, this should include not only users, holders, miners, researchers and developers, but also service providers such as wallets, exchanges and mining pools as well. Various stakeholder groups have diverse interests and it is almost impossible to align everyone's incentives. This is why blockchain governance is a complicated and controversial topic. If we consider a blockchain as a large social experiment, governance requires a more sophisticated design than any other part of the system. After ten years of evolution, we still haven't identified general best practices or sustainable processes for blockchain governance.

Some projects conduct governance via a "benevolent dictator for life" (such as Linus Torvalds to Linux). We acknowledge that this makes a project highly efficient, cohesive, and also charming: people love heroes; however, this is contradictory to decentralization, the core value of blockchain. 

Some projects entrust a distinguished off-chain committee with far-reaching decision-making power, such as the ECAF (EOSIO Core Arbitration Forum) on EOS. However, these committees lack the essential power to guarantee participants will abide by their decisions, which could have played a role in the decision to shut down the ECAF earlier this year. 

Some projects, such as Tezos, go further, and implement on-chain governance to ensure all participants abide by voted upon decisions. This also avoids any impacts of discord between developers and miners (or full node users). Note that on-chain governance is different from a simple on-chain vote, if a proposed feature or patch has acquired enough votes through on-chain governance, the chain code will be updated automatically, miners or full nodes do not have any means of controlling this change. Polkadot takes an even more sophisticated approach to on-chain governance, utilizing an elected council, referendum process for stake-weighted voting and positive/negative bias mechanisms to account for voter turnout. 

However, despite its straightforwardness, on-chain governance in practice is not as elegant as it is presented. First of all, votes only reflect the interest of token holders, while simply ignoring all other parties. Secondly, a low voting rate is a long-standing problem in both the blockchain world and real world. How can results be in the best interest of the majority if only a minority vote? Last but most importantly, a hard fork should always be considered as final recourse for all stakeholders. Given the excellent data availability provided by the wide replication of a permissionless blockchain, forking away from the existing chain with full data preservation and without interruption should always be an option. A hard fork could never be implemented via on-chain governance.

There are not yet viable answers to the questions of governance, so for Nervos Network we will take an evolving approach. We expect the community to develop organically in the early days and over time, as more tokens are mined, mining becomes more distributed, and more developers are engaged, governance responsibilities will gradually become more decentralized. Over the long term, community-based governance will manage the protocol upgrade process and resource allocation from the treasury.

Nervos CKB is designed to be decentralized autonomous infrastructure that could last for hundreds of years, which means there are certain things that demand our best effort as a community to hold true, no matter how this network evolves. The 3 core invariants are:

- Issuance schedule is completely fixed, thus shall never change.
- State/data stored in cells shall not be tampered with.
- Existing scripts' semantics shall not be changed.

Community-based governance for blockchains is a very new field and there are many worthy on-going experiments. We recognize that this is not a trivial topic, and time is required to fully study, observe, and iterate to arrive at an optimal approach. We're taking a conservative approach to community-based governance in the short-term, while remaining fully committed to this direction in the long run.

## 5. Overview of Layer 2 Solutions

### 5.1 What is Layer 2?

A blockchain network's layer 1 is defined by constraints. An ideal layer 1 blockchain makes no compromises on security, decentralization and sustainability, however, this creates challenges related to scalability and transaction costs. Layer 2 solutions are built on top of layer 1 protocols, allowing computation to be moved off-chain with mechanisms to securely settle back to the layer 1 blockchain.

This is similar to net settlement in today's banking system or SEC-mandated regulatory filings. By reducing the amount of data requiring global consensus, the network can serve more participants and facilitate more economic activity than it would have been able to otherwise, while still maintaining the properties of decentralization.

Layer 2 users depend on security provided by the layer 1 blockchain, and utilize this security when moving assets between layers or settling a dispute. This function is similar to a court system: the court doesn't have to monitor and validate all transactions, but only serves as a place to record key evidence and to settle disputes. Similarly, in a blockchain context, the layer 1 blockchain allows participants to transact off-chain, and in the case of a disagreement provides them with the ability to bring cryptographic evidence to the blockchain and penalize dishonesty. 

### 5.2 Payment and State Channels

Payment channels are created between two parties that transact often. They provide a low-latency, immediate payment experience that transactions done directly on a global blockchain could never provide. Payment channels function similar to a bar tab - you can open a tab with a bartender and keep ordering drinks, but only settle the tab and pay the final amount when you're ready to leave the bar. In the operation of a payment channel, participants exchange messages containing cryptographic commitments to their balances and can update these balances an unlimited number of times off-chain, before they're ready to close the channel and settle balances back on the blockchain.

Payment channels can be unidirectional or bidirectional. Unidirectional payment channels flow from Party A to Party B, similar to the bar tab example above. Party A deposits the maximum amount they might spend with Party B, and then slowly signs over funds as they receive goods or services.

Bidirectional payment channels are more complicated, but start to show the scope of possibilities for layer 2 technologies. In these payment channels, funds flow back and forth between parties. This allows for "rebalancing" of payment channels and opens up the possibility of payments across channels through a shared counterparty. This enables networks of payment channels, such as Bitcoin's Lightning Network. Funds can be transferred from Party A to Party B without a direct channel between them, as long as Party A can find a path through an intermediary with connections open to both parties.

Just as payment channels can scale on-chain payments, state channels can scale any on-chain transactions. While a payment channel is limited to managing balances between two parties, a state channel is an agreement on arbitrary state, enabling everything from a game of trustless chess to scalable decentralized applications.

Similar to a payment channel, the parties open a channel, exchange cryptographic signatures over time and submit a final state (or result) to an on-chain smart contract. The smart contract will then execute based on this input, settling the transaction according to rules encoded in the contract.
 
A "generalized state channel" is a powerful state channel construction, allowing a single state channel to support state transitions across multiple smart contracts. This reduces the state bloat inherent in a "one channel per application" architecture and also allows for easy on-boarding with the ability to utilize state channels users already have open. 

### 5.3 Side-chains

A side-chain is a separate blockchain that's attached to a trustless blockchain (main-chain) with a two-way peg. To utilize the side-chain, a user would send funds to a specified address on the main-chain, locking these funds under control of the side-chain operators. Once this transaction is confirmed and a safety period has passed, a proof can be communicated to side-chain operators detailing the deposit of funds. The operators will then create a transaction on the side-chain, distributing the appropriate funds. These funds can then be spent on the side-chain with low fees, fast confirmation and high throughput.

The main drawback of side-chains is that they require additional security mechanisms and security assumptions. The simplest side-chain construction, a federated side-chain, places trust in a multi-signature group of operators. On smart contract platforms, security models can be fine-tuned with token incentives or bonding/challenging/slashing economic games. 

Compared to other off-chain general purpose scaling solutions, side-chains are easier to understand and implement. For types of applications that allow creation of a trust model that's acceptable to their users, side-chains can be a practical solution.

### 5.4 Commit-chains

On commit-chains[6], such as Plasma[7], a layer 2 chain is constructed that leverages a trust root on a layer 1 blockchain (root-chain) with broad global consensus. These commit-chains are secure; in the event a chain operator is malicious or dysfunctional, users can always withdraw their assets through a mechanism on the root-chain.

A commit-chain operator is trusted to execute transactions correctly and publish periodic updates to the root-chain. Under all conditions, except for a prolonged censorship attack on the root-chain, assets on the commit-chains will remain safe. Similar to federated side-chains, commit-chain designs offer a superior user experience compared to trustless blockchains. However, they do so while maintaining stronger security guarantees.
  
The commit-chain is secured by a set of smart contracts running on the root-chain. Users deposit assets into this contract and the commit-chain operator then provides them assets on the commit-chain. The operator will periodically publish commitments to the root-chain, which users can later utilize to prove asset ownership through Merkle proofs, an "exit", in which commit-chain assets are withdrawn to the root-chain.

This describes the general notion of commit-chain designs, the basis of an emerging family of protocols including Plasma. The Plasma white paper[7] released by Vitalik Buterin and Joseph Poon in 2017 lays out an ambitious vision. Though all Plasma chains are currently asset-based, and can only store fungible and non-fungible token ownership (and transfers), trustless code execution (or smart contracts) is an active area of research.

### 5.5 Verifiable Off-Chain Computations

Cryptography provides a tool seemingly tailored to the dynamics of expensive on-chain verification and inexpensive off-chain computation: interactive proof systems. An interactive proof system is a protocol with two participants, the Prover and the Verifier. By sending messages back and forth, the Prover will provide information to convince the Verifier that a certain claim is true, whereas the Verifier will examine what is provided and reject false claims. Claims that the Verifier cannot reject are accepted as true.

The principal reason why the Verifier does not simply verify the claim naïvely on his own is efficiency — by interacting with a Prover, the Verifier can verify claims that would be prohibitively expensive to verify otherwise. This complexity gap can come from a variety of sources: 1) the Verifier may be running lightweight hardware that can support only space-bounded or time-bounded (or both) computations, 2) naïve verification may require access to a long sequence of nondeterministic choices, 3) naïve verification may be impossible because the Verifier does not possess certain secret information. 

While the secrecy of important information is certainly a relevant constraining factor in the context of cryptocurrencies, a more relevant constraining factor in the context of scalability is the cost of on-chain verification, especially in contrast to relatively cheap off-chain computation.

In the context of cryptocurrencies, significant attention has been directed towards zk-SNARKs (zero-knowledge, succinct non-interactive arguments of knowledge). This family of non-interactive proof systems revolves around the arithmetic circuit, which encodes an arbitrary computation as a circuit of additions and multiplications over a finite field. For instance, the arithmetic circuit can encode "I know a leaf in this Merkle tree".

zk-SNARK proofs are constant-size (hundreds of bytes) and verifiable in constant time, although this Verifier-efficiency comes at a cost: a trusted setup and a structured reference string are required, in addition to pairing-based arithmetic (of which concrete cryptographic hardness remains an object of concern). 

Alternative proof systems provide different trade-offs. For instance, Bulletproofs have no trusted setup and rely on the much more common discrete logarithm assumption, however have logarithmic-size proofs (though still quite small) and linear-time Verifiers. zk-STARKs provide an alternative to zk-SNARKs in terms of scalability, without a trusted setup and rely only on rock-solid cryptographic assumptions, although the produced proof is logarithmic in size (and quite large: hundreds of kilobytes).

In the context of a multi-layer cryptocurrency ecosystem such as the Nervos Network, interactive proofs offer the ability to offload expensive Prover-side computations to layer 2 while requiring only modest Verifier-side work from layer 1. This intuition is captured, for instance, in Vitalik Buterin's ZK Rollup protocol[8]: a permissionless relayer gathers transactions off-chain and periodically updates a Merkle root stored on chain. Every such root update is accompanied by a zk-SNARK that shows that only valid transactions were accumulated into the new Merkle tree. A smart contract verifies the proof and allows the Merkle root to be updated only if the proof is valid.

The construction outlined above should be able to support more complex state transitions beyond simple transactions, including DEX's, multiple tokens, and privacy-preserving computation.

### 5.6 Economic Model of Layer 2 Solutions

While layer 2 solutions provide impressive scalability, the token economics of these systems may pose design challenges.

Layer 2 token economics may involve compensation for critical infrastructure (such as validators and watchtowers), as well as application-specific incentive design. Critical layer 2 infrastructure tends to work better with a duration-based, subscription model. In the Nervos Network, this pricing structure can be easily implemented through the CKB's opportunity cost-based payment method. Service providers can collect fees on their users' "deposits" through the NervosDAO. Layer 2 developers can then focus token economic models on incentives specific to their applications.

In a way, this pricing model is exactly how users pay for state storage on the CKB as well. They're essentially paying a subscription fee to miners with the distribution of their inflation rewards issued by the NervosDAO.

## 6. The Nervos Network

### 6.1 Layer 1 as a Multi-asset Store of Value Platform

We believe that a layer 1 blockchain has to be built as a store of value. To maximize long-term decentralization, it has to be based on proof of work consensus with an economic model designed around state storage occupation, instead of transaction fees. The Common Knowledge Base (CKB) is a proof of work-based, multi-asset, store of value blockchain with both its programming and economic models designed around state.

The CKB is the base layer of the Nervos Network, with the highest security and highest degree of decentralization. Owning and transacting assets on the CKB comes with the highest cost, however provides the most secure and accessible asset storage in the network and allows for maximum composability. The CKB is best suited for high value assets and long-term asset preservation.

The Common Knowledge Base is the first layer 1 blockchain built specifically to support layer 2 protocols:

- The CKB is designed to complement layer 2 protocols, focusing on security and decentralization, instead of overlapping layer 2 priorities such as scalability.
- The CKB models its ledger around state, instead of accounts. Cells are essentially self-contained state objects that can be referenced by transactions and passed around between layers. This is ideal for a layered architecture, where the objects referenced and passed between layers are pieces of state, instead of accounts.
- The CKB is designed as a generalized verification machine, instead of computation engine. This allows the CKB to serve as a cryptographic court, that verifies off-chain state transitions.
- The CKB allows developers to easily add custom cryptographic primitives. This future-proofs the CKB, allowing for verification of proofs generated by a variety of layer 2 solutions.

The Common Knowledge Base aims to be the infrastructure to store the world's most valuable common knowledge, with the best-in-class layer 2 ecosystem providing the most scalable and efficient blockchain transactions.

### 6.2 Scale with Layer 2 Solutions

With its layered architecture, the Nervos Network can scale on layer 2 to any number of participants, while still maintaining the vital properties of decentralization and asset preservation. Layer 2 protocols can make use of any type of layer 1 commitment or cryptographic primitive, enabling great flexibility and creativity in designing transactional systems to support a growing layer 2 user base. Layer 2 developers can choose their own trade-offs in regard to throughput, finality, privacy and trust models that work best in the context of their applications and users.

In the Nervos Network, layer 1 (CKB) is used for state verification, while layer 2 is responsible for state generation. State channels and side-chains are examples of state generation, however any type of generate-verify pattern is supported, such as a zero-knowledge proof generation cluster. Wallets also operate at layer 2, running arbitrary logic, generating new state and submitting state transitions to the CKB for validation. Wallets in the Nervos Network are very powerful because they are state generators, with full control over state transitions.

Side-chains are developer-friendly and provide a good user experience. They do however, rely on the honesty of their validators. If the validators behave maliciously, users are in danger of losing their assets. Nervos Network provides an open-source and easy-to-use side-chain stack for launching side-chains on the CKB, consisting of a Proof-of-Stake blockchain framework called "Muta" and a side-chain solution based on it called "Axon".

Muta is a highly customizable, high-performance blockchain framework designed to support Proof-of-Stake, BFT consensus and smart contracts. It features a high throughput and low latency BFT consensus "Overlord", and supports various virtual machines including CKB-VM, EVM and WASM. Different virtual machines can be used in a single Muta blockchain simultaneously, with cross-VM interoperability. Muta greatly lowers the barrier for developers to build high performance blockchains, while still allowing maximum flexibility to customize their protocols.

Axon is a complete solution built with Muta to provide developers a turnkey side-chain on top of the Nervos CKB, with a practical security and token economic model. Axon solutions use the CKB for secure asset custody, and use token-based governance mechanism to manage the side-chain validators. Cross-chain protocols for interactions between an Axon side-chain and the CKB, as well as between Axon side-chains will also be built-in. With Axon, developers can focus on building applications, instead of building infrastructure and cross-chain protocols. 

Both Muta and Axon are currently under heavy development. We'll open source the frameworks soon, and RFCs for both Muta and Axon are also on the way.

Layer 2 protocols are a flourishing area of research and development. We foresee a future in which all layer 2 protocols are standardized and seamlessly interoperate. However, we acknowledge that layer 2 solutions are still maturing, and we're often still pushing the boundaries of what they can do, as well as finding their acceptable trade-offs. We've seen early promising solutions, but there's still plenty of research to conduct on subjects such as interoperability, security and economic models in layer 2 designs.

### 6.3 Sustainability

In the interest of long-term sustainability, the Nervos Common Knowledge Base bounds state, imposes a cost on on-chain storage and provides incentives for users to clear their state storage. A bounded state keeps the requirements for full node participation low, ensuring nodes can be run on low-cost hardware. Robust full node participation increases decentralization and in turn, security.

By imposing a time-proportional "state-rent" cost on state storage, the Nervos Common Knowledge Base mitigates the tragedy of the commons faced by many blockchains in a "pay once, store forever" paradigm. Implemented through "targeted inflation", this state rent mechanism provides a smooth user experience while imposing a cost on state storage.

This inflation cost can be targeted because users own the consensus space their data occupies. This model also includes a native mechanism for users to remove their state from the consensus space. Coupled with the economic incentives of state rent, this ensures that state size will always be moving toward the minimum amount of data required by network participants.

Individually owned state also significantly reduces developers' costs. Instead of being required to purchase CKBytes for the state requirements of all their users, developers only have to purchase enough CKBytes to store the verification code required by their application. Each user would use their own cells to store their tokens and would be fully responsible for their assets.

Finally, state rent provides an ongoing reward to miners through new token issuance. This predictable income incentivizes miners to advance the blockchain, instead of forking profitable blocks to take the transaction fees.

### 6.4 Aligned Incentives

The economic model of the Common Knowledge Base is designed to align incentives for all participants in the ecosystem.

The Nervos Common Knowledge Base is built explicitly for secure value preservation, instead of cheap transaction fees. This critical positioning will attract store of value users, similar to the user community of Bitcoin, instead of medium of exchange users.

Medium of exchange use cases have a tendency to always push a blockchain network toward centralization, in pursuit of greater efficiency and low fees. Without significant fee income for infrastructure operators that secure the network (miners or validators), security must be funded through monetary inflation, or is simply under-funded. Monetary inflation is detrimental to long-term holders, and under-funded security is detrimental to any stakeholder of the network.

Store of value users however, have strong demands for censorship resistance and asset security. They rely on miners to provide this, and in turn compensate them for their role. In a store of value network, these parties have aligned interests.

By aligning the incentives of all participants, a united Nervos community can grow, and the aligned economic system of the network is also expected be hard-fork resistant.

### 6.5 Value Capture and Value Generation

For any blockchain to remain secure as the value of assets secured by the platform increases, the system must have a mechanism to capture value as the value of assets secured grows. By bounding state, the CKB makes the state space a scarce and market-priced resource. As demand for asset storage on the network rises, the system is expected to better compensate the miners for securing such assets.

As a value preserving platform, the intrinsic value of the CKB as a platform is determined by the amount of security it provides to the assets it preserves. As the value of assets secured rises, the value capture mechanism of the CKB economic model is able to automatically raise the CKB's security budget to attract more mining resources, making the platform more secure. Not only is this important to make the platform sustainable, it also provides a path of growth for the platform's intrinsic value - as the platform becomes more secure, it also becomes more attractive to higher-value assets, generating more demand. Obviously, this is bound by the overall aggregate value that will eventually move to the blockchain space.

Over time, we expect the economic density of the CKB to increase. CKBytes will be used for high-value asset storage and low-value assets will to move to blockchains connected to the CKB, such as layer 2 side-chains. Instead of directly securing assets, the CKB can be used as a trust root to secure an entire side-chain’s ecosystem through, for example,  a few hundred bytes of cryptographic proofs. The economic density of such proofs is extraordinarily high, further supporting the demand curve of storage space: analogous to a small parcel of land significantly increasing its economic density by supporting a skyscraper.

Finally, through the design of the NervosDAO and its "inflation shelter" function, long-term token holders will always retain a fixed percentage of total issuance, making the native token itself a robust store of value.

### 6.6 Bridging the Regulatory Gap

Permissionless blockchains allow total decentralization in asset issuance and transaction. This is what makes them valuable, but is also the reason they aren't compatible with real-world financial and judicial systems.

The emergence of a layered architecture provides the opportunity to create regulatory compliant portions of an unregulated, permissionless blockchain. For example, users can store their decentralized assets on layer 1, enjoy absolute property ownership of these assets, and can also process real-world business on layer 2, where they are subject to regulatory and legal constraints.

Take for example cryptocurrency exchanges - countries such as Japan and Singapore have issued licenses to exchanges and created regulatory requirements. A compliant exchange or a branch of a global exchange could build a layer 2 trading chain, import user identities and assets and then conduct legal business in accordance with local regulatory requirements.

Issuance and transaction of real-world assets become possible within a layered blockchain construction. Real-world assets can flow to the blockchain ecosystem through a regulated layer 2 side-chain to the permissionless layer 1 blockchain, allowing these assets access to the largest ecosystem of composable, decentralized financial services.

In the future, it is expected that the Nervos Network will also use layer 2 side-chains and applications as the foundation of large-scale user adoption, in cooperation with leading companies in this space.

# References

[1] Satoshi Nakamoto. "Bitcoin: A Peer-to-Peer Electronic Cash System". 31 Oct 2008, https://bitcoin.org/bitcoin.pdf

[2] Vitalik Buterin. "Ethereum White Paper: A Next Generation Smart Contract & Decentralized Application Platform". Nov 2013 http://blockchainlab.com/pdf/Ethereum_white_paper-a_next_generation_smart_contract_and_decentralized_application_platform-vitalik-buterin.pdf 

[3] With an average Bitcoin transaction size of 250 bytes:
(2 * 250 * 7,500,000,000) / (24 * 6) = 26,041,666,666 byte blocks (every 10 minutes); 
26,041,666,666 * (24 * 6) = 3,750,000,000,000 bytes (blockchain growth each day);
3,750,000,000,000 * 365.25 = 1,369,687,500,000,000 bytes (blockchain growth each year)

[4] Gur Huberman, Jacob Leshno, Ciamac C. Moallemi. "Monopoly Without a Monopolist: An Economic Analysis of the Bitcoin Payment System". Bank of Finland Research Discussion Paper No. 27/2017. 6 Sep 2017, https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3032375

[5] Miles Carlsten, Harry Kalodner, S. Matthew Weinberg, Arvind Narayanan. "On the Instabiliity of Bitcoin Without the Block Reward". Oct 2016, https://www.cs.princeton.edu/~smattw/CKWN-CCS16.pdf

[6] Lewis Gudgeon, Perdo Moreno-Sanchez, Stefanie Roos, Patrick McCorry, Arthur Gervais. "SoK: Off The Chain Transactions". 17 Apr 2019, https://eprint.iacr.org/2019/360.pdf

[7] Joseph Poon, Vitalik Buterin. "Plasma: Scalable Autonomous Smart Contracts". 11 Aug 2017, https://plasma.io/plasma.pdf

[8] Vitalik Buterin. "On-chain scaling to potentially ~500 tx/sec through mass tx validation". 22 Sep 2018, https://ethresear.ch/t/on-chain-scaling-to-potentially-500-tx-sec-through-mass-tx-validation/3477


================================================
File: rfcs/0002-ckb/0002-ckb.md
================================================
---
Number: "0002"
Category: Informational
Status: Final
Author: Jan Xie <jan@cryptape.com>
Created: 2018-01-02
---

# Nervos CKB: A Common Knowledge Base for Crypto-Economy

## Abstract

Nervos is a layered crypto-economy network. Nervos separates the infrastructure of a crypto-economy into two layers: a verification layer (layer 1) that serves as a trust root and smart custodian, and a generation layer (layer 2) for high-performance transactions and privacy protection.

This document provides an overview of the Nervos Common Knowledge Base (CKB), a public permissionless blockchain and layer 1 of Nervos. CKB generates trust and extends this trust to upper layers, making Nervos a trust network. It's also the value store of the Nervos network, providing public, secure and censorship-resistant custody services for assets, identities and other common knowledge created in the network.

## Contents

1. [Motivation](#1-motivation)
2. [Overview](#2-overview)
3. [Consensus](#3-consensus)
4. [Programming Model](#4-programming-model)
    1. [State Generation and Verification](#41-state-generation-and-verification)
    2. [Cell](#42-cell)
    3. [VM](#43-vm)
    4. [Transaction](#44-transaction)
5. [Economic Model](#5-economic-model)
    1. [State Cost and Cell Capacity](#51-state-cost-and-cell-capacity)
    2. [Computation Cost and Transaction Fees](#52-computation-cost-and-transaction-fees)
6. [Network](#6-network)
7. [Summary](#7-summary)
8. [References](#8-references)
9. [Appendix](#9-appendix)

## 1. Motivation

We want a peer-to-peer crypto-economy network.

In such a network, people can not only collaborate but also have incentives to do so. We need the ability to define, issue, transfer, and own assets in a peer-to-peer network to create such incentives. Blockchain technology brings us the last piece of the puzzle.

Bitcoin[1] was the first public permissionless blockchain, designed to be used solely as peer-to-peer cash. Ethereum[2] extends the use case of blockchain to create a general purpose trust computing platform on which people have built all kinds of decentralized applications. The booming applications on the Bitcoin and Ethereum networks have proven the concept of the future crypto-economy. However, these networks also suffer from the notorious scalability problem, their transaction processing capability cannot scale with the number of participants in the network, which severely limits their potential.

The blockchain community has proposed many scalability solutions in recent years. In general, we can divide these solutions into two categories, on-chain scaling and off-chain scaling. On-chain scaling solutions are those that try to scale at the same layer where consensus runs. The consensus process is the core of a blockchain protocol, in which nodes exchange network messages and reach agreement eventually. A consensus is slow almost by definition, because message exchange on a public and open network is slow and uncertain, nodes must wait and retry to reach agreement in the consensus process. To scale at this layer, we can either "scale up" by increasing the processing ability and network bandwidth of nodes (but sacrifice decentralization due to high hardware and infrastructure costs), or "scale out" by sharding. The idea of sharding is to divide nodes into many small "shards", and ask each shard to process only a fraction of network transactions. Sharding is widely adopted by Internet giants, as they face the same scalability issues when serving millions of users. However, sharding is well known for the complexity of shard coordination and cross-shard transactions, which even in a trusted environment, leads to performance degradation as the number of shards grows.

In contrast, off-chain scaling solutions acknowledge the inherent complexity of the consensus process. They recognize that consensus within different scopes incur different costs, and the global consensus created by a public permissionless blockchain is the most expensive consensus. While it is hard to scale a global consensus, we can use it wisely. Most transactions between two or more parties don't need to be known by every node in the network, except when they are securely settled; in other words, when users want to turn their transactions into common knowledge of the network. This network scales by offloading most of the work to upper layers, with no limit on scalability. Processing transactions off-chain also brings additional benefits, such as lower latency and higher privacy.

While we agree with the general ideas of off-chain scaling, we have found that there is no existing blockchain designed for it. For example, though the lightning network is one of the earliest explorations in off-chain scaling, it has taken years to launch its testnet and is still far from mass-adoption due to the limitations of the underlying Bitcoin protocol. Ethereum provides powerful programming ability, but its computation-oriented economic model doesn't fit well with off-chain scaling. Because off-chain participants handle most of the computation, what is required is a blockchain that can keep their assets in secure custody and move assets according to the final state of their computation. The computation-oriented design of Ethereum also makes it difficult to execute transactions in parallel, which is an impediment to scalability.

The economic models of current blockchains also face challenges. With more users and applications moving to blockchain platforms, the amount of data stored on blockchains also increases. Current blockchain solutions are concerned more with the cost of consensus and computation, and allow a user to pay once and have their data occupy full nodes’ storage forever. Cryptocurrency prices also are highly volatile, and users may find it difficult to pay high transaction fees as the price of a cryptocurrency increases.

We propose Nervos CKB, a public permissionless blockchain designed for a layered crypto-economy network.

## 2. Overview

Nervos CKB (Common Knowledge Base) is a layer 1 blockchain, a decentralized and secure layer that provides common knowledge custody for the network. Common knowledge refers to states that are verified by global consensus. Crypto-assets are an example of common knowledge.

In Nervos, the CKB and all layer 2 protocols work together to serve the crypto-economy. CKB (or layer 1) is where state is stored and defined, and layer 2 is the generation layer (or computation layer, these two terms are interchangeable) that processes most transactions and generates new states. Layer 2 participants submit newly generated states to the CKB eventually at the time they deem necessary. If those states pass the corresponding verification performed by nodes in a global network, the CKB stores them in a peer-to-peer node securely.

The layered architecture separates state and computation, providing each layer more flexibility and scalability. For example, blockchains on the generation layer (layer 2) may use different consensus algorithms. CKB is the lowest layer with the broadest consensus and provides the most secure consensus in the Nervos network. However, different applications might prefer different consensus scopes and forcing all applications to use CKB’s consensus would be inefficient. Applications can choose the appropriate generation methods based on their particular needs. The only time these applications will need to submit states to CKB for broader agreement is when they need to make these states common knowledge that has been verified by the CKB's global consensus.

Possible state generation methods include (but are not limited to) the following:

- Local generators on the client: Generators run directly on the client’s devices. Developers can implement the generator in any programming language.
- Web services: Users may use traditional web services to generate new states. All current web services may work with CKB in this way to gain more trust and liquidity for the generated states. For example, game companies may define in-game items as assets in CKB, the game itself functions as a web service that generates game data, which is then verified and stored in CKB.
- State channels: Two or more users may use peer-to-peer communication to generate new states.
- Generation chains: A generation chain is a blockchain that generates new states and stores them in CKB. Generation chains may be permissionless blockchains or permissioned blockchains. In each generation chain, nodes reach consensus in smaller scopes, providing better privacy and performance.

![Figure 1. Layered Architecture](images/layered-architecture.png)
*Figure 1. Layered Architecture*

CKB consists of a Proof-of-Work based consensus, a RISC-V instruction set based virtual machine, a state model based on cells, a state-oriented economic model, and a peer-to-peer network. The Proof-of-Work based consensus makes the CKB a public and censorship-resistant service. The combination of CKB VM and the Cell model creates a stateful Turing-complete programming model for developers, making state generation (or layer 2) on CKB practical. The CKB economic model is designed for common knowledge custody and long-term sustainability. The CKB peer-to-peer network provides secure and optimal communication between different types of nodes.

## 3. Consensus

CKB consensus is an improved Nakamoto consensus based on Proof-of-Work, that aims to achieve openness, correctness and high performance in distributed environments with network delay and Byzantine node faults.

Permissionless blockchains run in open networks where nodes can join and exit freely, with no liveness assumptions. These are severe problems for traditional BFT consensus algorithms to solve. Satoshi Nakamoto introduced economic incentives and probabilistic consensus to solve these problems. Nakamoto consensus in Bitcoin uses blocks as votes, which takes longer (up to 10 minutes to an hour) to confirm transactions and leads to an inferior user experience.

CKB consensus is a Nakamoto consensus variant, which means it allows nodes to join and exit the network freely. Every node can participate in the consensus process either by mining (running a specific algorithm to find the Proof-of-Work) to produce new blocks, or by verifying new blocks are valid. CKB uses an ASIC-neutral Proof-of-Work function, with the goals of distributing tokens as evenly as possible and making the network as secure as possible.

Correctness includes eventual consistency, availability, and fairness. Eventual consistency guarantees every node sees an identical copy of state. Availability makes sure the network responds to users' requests within a reasonable time. Fairness ensures mining nodes get fair returns for their efforts to keep the network functioning securely.

High performance includes transaction latency, the time between the submission of a request and the confirmation of its execution results, and transaction throughput, the number of transactions the system is capable of processing per second. Both of these measures depend on block time, which is the average time between two consecutive blocks.

Please check the CKB [Consensus RFC] for more details.

[Consensus RFC]: https://github.com/nirenzang/rfcs/blob/master/rfcs/0020-ckb-consensus-protocol/0020-ckb-consensus-protocol.md

## 4. Programming Model

CKB provides a stateful Turing-complete programming model based on CKB VM and cell model.

| | Bitcoin | Ethereum | CKB |
|-|---------|----------|------------|
|Instruction Set|Script|EVM|RISC-V|
|Cryptographic Primitive|Opcode|Precompile|Assembly|
|Stateful|No|Yes|Yes|
|State Type|Ledger|General|General|
|State Model|UTXO|Account|Cell|
|State Verification|On-chain|On-chain|On-chain|
|State Generation|Off-chain|On-chain|Off-chain|

*Table 1. Comparison of Bitcoin, Ethereum and CKB Programming Model*

The CKB programming model consists of three parts:

- state generation (off-chain)
- state verification (CKB VM)
- state storage (Cell model)

In this model, decentralized application logic is split into two parts (generation and verification), running in different places. State generation logic runs off-chain on the client side; new states are packaged into transactions and broadcasted to the entire network. CKB transactions have an inputs/outputs based structure like Bitcoin. Transaction inputs are references to previous outputs, along with proofs to unlock them. The client includes generated new states as transaction outputs, which are called cells in CKB. Cells are the primary state storage units in CKB and are assets owned by users that must follow associated application logic specified by scripts. CKB VM executes these scripts and verifies proofs included in inputs to make sure the user is permitted to use referenced cells and the state transition is valid under specified application logic. In this way, all nodes in the network verify that new states are valid and keep these states in custody.

State in CKB is a first-class citizen, states are included in transactions and blocks and synchronized directly among nodes. Although the programming model is stateful, scripts running in CKB VM are pure functions with no internal state, which makes CKB scripts deterministic, conducive to parallel execution, and easy to compose.

### 4.1 State Generation and Verification

Decentralized applications on Nervos separate the generation and verification of state. While these processes occur in different places, CKB provides the additional flexibility to utilize different algorithms for state generation and verification.

Utilizing the same algorithm on both generation and verification sides is a straightforward choice that works for general problems. In this model, the same algorithm has two implementations, one that runs off-chain in any execution environment targeted by the application, and the other one runs on-chain in CKB VM. New states are generated off-chain with this algorithm (based on previous states and user inputs), packaged as a transaction, and then broadcasted to the network. CKB nodes run this same algorithm on-chain, provide it the same previous states and user inputs, and then verify the result matches the transaction-specified outputs.

There are several advantages to this separation of state generation and validation:

- Deterministic transactions: Certainty of transaction execution is one of the core pursuits of decentralized applications. If transactions include only user input and new states are the result of computation on nodes (as seen in Ethereum), the transaction creator cannot be certain about the on-chain computation context, which may lead to unexpected results. In CKB, users generate new states on the client side. They can confirm the new states before broadcasting their state transition to the network. The transaction outcome is certain: either the transaction passes on-chain verification and the new state is accepted, or the transaction is deemed invalid and no state change is made to CKB (Figure 1).

- Parallelism: If transactions only include user inputs and new states are generated by nodes, then nodes will not know what state is going to be accessed by the verification process, and cannot determine dependencies between transactions. In CKB, because transactions explicitly include previous states and new states, nodes can see dependencies between transactions prior to verification, and can process transactions in parallel.

- Higher resource utilization: As application logic is split and run in different places, the network can distribute computational workload more evenly across nodes and clients, and thus utilize system resources more efficiently.

- Flexible state generation: Even when the same algorithms are used, developers can implement generation and validation in different ways. On the client side there is the flexibility to choose the programming language that provides for better performance and fast development.

In some scenarios, state verification can utilize a different (but associated) algorithm that is much more efficient than the one used for state generation. The most typical example is seen in Bitcoin transactions: Bitcoin transaction construction consists mainly of a searching process to identify appropriate UTXOs to use, while verification is the addition of numbers and simple comparison. Other interesting examples include sorting and searching algorithms: the computational complexity for quicksort, one of the best sorting algorithms for the average case, is O(Nlog(N)), but the algorithm to verify the result is just O(N). Searching for the index of an element in a sorted array is O(log(N)) with binary search, but its verification only takes O(1). The more complex the business rules, the higher probability that there can be asymmetric generation and validation algorithms with differing computational complexity.

System throughput can be improved by utlizing asymmetry between state generation and validation. Moving details of computation to the client side is also valuable for algorithm protection and privacy. With the advancement of technologies such as zero-knowledge proofs, we may find efficient generation and verification solutions to general problems, and CKB is a natural fit for these types of solutions.

We refer to programs that generate new states and create new cells as Generators. Generators run locally on the client side (off-chain). They utilize user input and existing cells as program inputs, to create new cells with new states as outputs. The inputs that Generators use and the outputs they produce together form a transaction.

![Figure 2. Separation of state generation and verification](images/separation-of-generation-verification.png)
*Figure 2. Separation of state generation and verification*

### 4.2 Cell

Cells are the primary state units in CKB, within them users can include arbitrary states. A cell has the following fields:

- `capacity` - Size limit of the cell. A cell's size is the total size of all fields contained in it.
- `data` - State data stored in this cell. It could be empty, however the total bytes used by a cell (including data), must always be less than or equal to its capacity.
- `type`: State verification script.
- `lock`: Script that represents the ownership of the cell. Owners of cells can transfer cells to others.

A cell is an immutable object, no one can modify it after creation. Every cell can only be used once, it cannot be used as input for two different transactions. Cell ‘updates’ mark previous cells as history and create new cells with the same capacity to replace them. By constructing and sending transactions, users provide new cells with new states in them and invalidate previous cells that store old states atomically. The set of all current (or live) cells represents the latest version of all common knowledge in CKB, and the set of history (or dead) cells represents all historical versions of common knowledge.

CKB allows users to transfer a cell's capacity all at once, or transfer only a fraction of a cell's capacity, which would in turn lead to more cells being created (e.g., a cell whose capacity is 10 bytes can become two cells whose capacity is 5 bytes each).

Two kinds of scripts (type and lock) are executed in CKB VM. CKB VM executes the `type` script when a cell is created in a transaction output, to guarantee the state in the cell is valid under specific rules. CKB VM executes the lock script, taking proofs as arguments, when the cell is referenced by a transaction input, to make sure the user has appropriate permissions to update or transfer the cell. If the execution of the lock script returns true, the user is allowed to transfer the cell or update its data according to validation rules that are specified by the `type` script.

This `type` and `lock` script pair allows all kinds of possibilities, for example:

- Upgradable cryptography - Anyone can deploy useful cryptography libraries written in languages such as C or C++ and use them in `type` and `lock` scripts. In CKB VM, there are no hardcoded cryptographic primitives, users are free to choose any cryptographic signature scheme they'd like to use to sign transactions.
- Multisig - Users can easily create M-of-N multisig or more complex `lock` scripts.
- Lending - Cell owners can lend cells for others to use while still maintaining their ownership of the cells.

The Cell model is a more generic state model compared to the UTXO or Account model. Both the UTXO and the Account model can express relationships between assets and their owners. The UTXO model defines ownership of assets (with the lock script), while the Account model defines ownership of assets by owner (with the account balance). The UTXO model makes the ledger history more clear, but its lack of generic state storage makes its already inexpressive scripts harder to use. The Account model is easy to understand and can support authorizations and identities well, but it presents challenges to processing transactions in parallel. The Cell model with `lock` and `type` scripts takes the best of both models to provide a more generic state model.

### 4.3 VM

CKB VM is a RISC-V instruction set based VM for executing type and lock scripts. It uses only standard RISC-V instructions, to maintain a standard compliant RISC-V software implementation which can embrace the broadest industrial support. CKB implements cryptographic primitives as ordinary assembly running on its VM, instead of customized instructions. It supports syscall, by which scripts can read metadata such as current transaction and general blockchain information from CKB. CKB VM defines `cycles` for each instruction, and provides total cycles executed during transaction verification to help miners determine transaction fees.

Existing blockchains hardcode cryptographic primitives in the protocol. For example, Bitcoin has special cryptographic opcodes such as `OP_CHECK*`, and Ethereum uses special 'precompiled' contracts located at a special address (e.g. `0000000000000000000000000000000000000001`) to support cryptographic operations such as `ecrecover`. To add new cryptographic primitives to these blockchains, we can only soft-fork (as Bitcoin re-uses opcodes to support new primitives) or hard-fork.

CKB VM is a crypto-agnostic virtual machine. There are no special cryptographic instructions hardcoded in CKB VM. New cryptographic primitives can always be deployed and used by scripts like an ordinary library. Being a RISC-V standard compliant implementation means existing cryptographic libraries written in C or other languages can be easily ported to CKB VM and used by cell scripts. CKB even implements the default hash function and public-key cryptography used in transaction verification this way. Being crypto-agnostic allows decentralized application developers on Nervos to use any new cryptography (such as Schnorr signatures, BLS signatures, and zkSNARKs/zkSTARKs) they'd like without affecting other users, and allows CKB users to keep their assets secure even in the post-quantum era.

CKB VM chooses a hardware targeting ISA because blockchain is hardware-like software. Though its creation is as easy as software, its upgrade is as difficult as hardware. As an ISA designed for chips, RISC-V is very stable, its core instruction set is implausible to change in the future. The ability to keep compatibility with the ecosystem without the need of a hard-fork is a key feature of a blockchain virtual machine like CKB VM. The simplicity of RISC-V also makes runtime cost modeling easy, which is crucial for transaction fee calculations.

Please check [RFC 0003](https://github.com/nervosnetwork/rfcs/blob/master/rfcs/0003-ckb-vm/0003-ckb-vm.md) for more details of CKB VM.

### 4.4 Transaction

Transactions express state transitions, resulting in cell transfer, update, or both. In a single transaction, users can update data in one or more cells or transfer their cells to other users. All state transitions in the transaction are atomic, they will either all succeed or all fail.

A transaction includes the following:

- `deps`: Dependent cell set, provides read-only cells required by transaction verification. These must be references to living cells.
- `inputs`: Cell references and proofs. Cell references point to live cells that are transferred or updated in the transaction. Proofs (e.g., signature) prove that the transaction creator has the permission to transfer or update the referenced cells.
- `outputs`: New cells created in this state transition.

The design of the CKB cell model and transactions is friendly to light clients. Since all the states are in blocks, block synchronization also accomplishes state synchronization. Light clients only need to synchronize blocks and do not need additional state synchronization or state transition computation. If only events were stored in blocks, full nodes would be required for state synchronization. State synchronization can be difficult across large networks because there are weak incentives to synchronize. This is different from block synchronization, in which miners are incentivized to broadcast blocks as widely as possible. With no need for extra state synchronization, the protocol makes light nodes and full nodes more equal peers, leading to a more robust and decentralized system.

![Figure 3. Transaction Parallelism and Conflict Detection](images/transaction-parallelism.png)
*Figure 3. Transaction Parallelism and Conflict Detection*

The `deps` and `inputs` in CKB transactions make it easier for nodes to determine transaction dependencies and perform parallel transaction processing (Figure 3). Different types of cells can be mixed and included in a single transaction to achieve atomic operation across types.

## 5. Economic Model

A well-designed economic model should incentivize all participants to contribute to the success of the crypto-economy and maximize the utility of the blockchain.

The CKB economic model is designed to motivate users, developers and node operators to work toward the common goal of common knowledge custody. The subject of the CKB economic model is state instead of computation, by using cell capacity and transaction fees as incentives for stakeholders.

Please check the CKB Token [Economics RFC] for more details.

[Economics RFC]: https://github.com/nervosnetwork/rfcs/blob/master/rfcs/0015-ckb-cryptoeconomics/0015-ckb-cryptoeconomics.md

### 5.1 State Cost and Cell Capacity

The creation and storage of states on the CKB incur costs. The creation of new states needs to be verified by full nodes (which incur computational costs), and the storage of states requires full nodes to provide disk space on an ongoing basis. Current permissionless blockchains only charge one-time transaction fees, but allow states to be stored on all full nodes, occupying storage space indefinitely.

In CKB, cells are basic storage units of state. A cell owner can use the cell to store state himself or lend it out to others. Because a cell's capacity can only be utilized by one user at a time, an owner utilizing the capacity himself would give up the opportunity to earn interest by lending the capacity out (either to CKB or to other users). With this opportunity cost, users pay for storage with a cost that is proportional to both space and time - the larger the capacity and the longer time they occupy it, the higher opportunity cost they incur. The advantage of CKB's implicit state cost model, when compared to an upfront payment model (such as storage rent discussed in the Ethereum community), is that it avoids the problem that upfront payments could be used up and the system would have to recycle the state and break any applications or contracts depend on it.

Cell metadata (`capacity`, `type` and `lock`) are states, which will occupy users' cell capacity and incur a state cost as well. This meta cost would incentivize users to create fewer cells when possible, increasing capacity efficiency.

### 5.2 Computation Cost and Transaction Fees

Updating a cell’s data or transferring cell ownership incurs transaction fees. Miners can set the transaction fee level that they are willing to accept based on CKB VM cycles used and state changes in transaction verification, allowing the market to determine transaction fees. With the programming model described above, cell owners can also pay transaction fees on behalf of their users.

As cell capacity is the only native asset in CKB, it is the most convenient asset users can use to pay transaction fees. However, users can also use any other user-defined assets as long as miners accept them; there is no hard-coded payment method in CKB transactions. This is allowed in CKB because its economic model and native asset do not center on computation, but states. Although cell capacity can be used as a means of paying transaction fees, its primary function is secure common knowledge storage, which can store state and hold it long-term. Payment method competition in the fee market does not compromise its value.

Restricting the transaction fee payment method to a blockchain's native asset is a significant obstacle preventing blockchains' mass adoption. This requires users to acquire native assets before using any of the blockchain's services, raising the barrier of entry for new users. By allowing cell owners to pay fees on behalf of their users and allowing payment with any user-defined assets, CKB can provide a better experience to users and wider choices of business models for developers.

Please check the Nervos CKB Economic Paper ([RFC 0015](https://github.com/nervosnetwork/rfcs/blob/master/rfcs/0015-ckb-cryptoeconomics/0015-ckb-cryptoeconomics.md)) for details of the economic model.

## 6. Network

We can categorize CKB nodes into three types:

- Mining Node: They participate in the CKB consensus process. Mining nodes collect new transactions, package them into blocks and produce new blocks when they have found a Proof-of-Work. Mining nodes do not have to store the entire transaction history, only the current cell set.
- Full Node: They verify new blocks and transactions, relay blocks and transactions, and select the chain fork on which they agree. Full nodes are the verifiers of the network.
- Light Node: They trust full nodes, only subscribe and store a subset of cells that they are concerned with. They use minimal resources. Users increasingly rely on mobile devices and mobile apps to access the Internet, the light node is designed to run on mobile devices.

Uniform blockchain networks (in which each node has the same role and performs the same function) are currently facing severe challenges. Full nodes validate all blocks and transaction data, requiring minimum external trust, but they incur a higher cost and are inconvenient to run. Light clients trade minimal trust for a substantial cost reduction on transaction verification, leading to a much better user experience. In a mature crypto-economy network, the largest group of nodes would be light nodes, followed by full nodes and mining nodes. Because light nodes depend on full nodes for state and state verification, a large number of light nodes would require a large number of full nodes to serve them. With CKB's economic model, both computation and storage resources required by a full node can be kept at a reasonable level, and the barriers to running a full node low, leading to a large group of service providers for light nodes and a highly decentralized network.

## 7. Summary

We envision a layered crypto-economy and CKB is its base layer. CKB is the decentralized trust root of this crypto-economy, it ensures the security of the trustless activities of the upper layers. It's a common knowledge custody network, in which states are verified by global consensus and stored in a highly available peer-to-peer network. CKB is designed from scratch to meet the needs of a layered architecture, and its design focuses on states rather than computation. In CKB, users and developers can define, issue, transfer and store crypto-assets, they can also create digital identities and utilize these identities in the crypto-economy. Only our imagination is the bounds of its use.

## 8. References

1. Satoshi Nakamoto, “Bitcoin A Peer-to-Peer Electronic Cash System”, 2008
2. Vitalik Buterin, "Ethereum A Next-Generation Smart Contract and Decentralized Application Platform", 2014

## 9. Appendix

Common Knowledge is the knowledge that’s accepted by everyone in a community. Participants in the community not only accept the knowledge themselves but know that others in the community also accept the knowledge.

In the past, common knowledge was scattered across individual's minds, and its formation required repeated communication and confirmation. Today, with the advancement of cryptography and distributed ledger technology, algorithms and machines are replacing humans as the medium for the formation and storage of common knowledge. Every piece of data in the blockchain, including digital assets and smart contracts, is a piece of common knowledge.

Blockchains are common knowledge bases. Participating in a blockchain network implies accepting and helping validate the common knowledge contained in it. Blockchains store transactions with their proofs, users can trust the validity of these transactions and know other users trust it too.

*The various ways in which the knowledge on which people base their plan is communicated to them is the crucial problem for any theory explaining the economic process, and the problem of what is the best way to utilizing knowledge initially dispersed among all the people is at least one of the main problems of economic policy - or of designing an efficient economic system.*

*- The Use of Knowledge in Society, Friedrich A. Hayek, 1945*


================================================
File: rfcs/0002-ckb/images/.keep
================================================



================================================
File: rfcs/0003-ckb-vm/0003-ckb-vm.md
================================================
---
Number: "0003"
Category: Informational
Status: Final
Author: Xuejie Xiao <xxuejie@gmail.com>
Created: 2018-08-01
---

# CKB-VM

## Overview

VM layer in CKB is used to perform a series of validation rules to determine if transaction is valid given transaction's inputs and outputs.

CKB uses [RISC-V](https://riscv.org/) ISA to implement VM layer. To be more precise, CKB uses rv64imc architecture: it is based on core [RV64I](https://riscv.org/specifications/) ISA with M standard extension for integer multiplication and division, and C standard extension for RCV(RISC-V Compressed Instructions). Note that CKB doesn't support floating point instructions, a CKB script developer can choose to pack a softfloat implementation into the binary if needed.

CKB relies on dynamic linking and syscalls to provide additional capabilities required by the blockchain, such as reading external cells or other crypto computations. Any compilers with RV64I support, such as [riscv-gcc](https://github.com/riscv/riscv-gcc), [riscv-llvm](https://github.com/lowRISC/riscv-llvm) or [Rust](https://github.com/rust-embedded/wg/issues/218) can be used to generate CKB compatible scripts.

## RISC-V Runtime Model

CKB leverages 64-bit RISC-V virtual machine to run contracts. We provide the core instructions in 64-bit address space, with additional integer multiplication/division extension instructions. CKB also supports RISC-V Compressed Instructions to reduce contract size. For maximum tooling and debugging support, CKB leverages Linux ELF format directly as contract format.

CKB virtual machine has a maximum of 4 MB runtime memory for running contracts. VM's runtime memory provides space for executable code pages mapped from contracts, stack space, head space and mmapped pages of external cell.

Running a contract is almost the same as running an executable in single core Linux environment:

```c
int main(int argc, char* argv[]) {
  uint64_t input_cell_length = 10000;
  void *input_cell = malloc(input_cell_length);
  ckb_load_cell(input_cell, &input_cell_length, 0, 0, CKB_SOURCE_INPUT);

  uint64_t output_cell_length = 10000;
  void *output_cell = malloc(output_cell_length);
  ckb_load_cell(output_cell, &output_cell_length, 0, 0, CKB_SOURCE_OUTPUT);

  // Consume input & output cell

  return 0;
}
```

Contract starts from main function in the ELF formatted contract file, arguments are passed in via standard argc and argv. When main returns 0, the contract is treated as success. Note that due to space consideration, we might not store full inputs and outputs data in argv. Instead, we might just provide metadata in argv, and leverages additional libraries and syscalls to support input/output loading. This way the runtime cost can be minimized. CKB VM is a strict single-threaded model, contract can ship with coroutines of their own.

For simplicity and deterministic behavior, CKB doesn't support floating point numbers. We suggest a softfloat solution if floating point number is really needed. Since CKB runs in a single threaded environment, atomic instructions are not needed.

## W^X Memory

CKB VM does not have an MMU unit. It works quite like CPUs in the very early era or CPUs in some embedded systems: the whole memory block can be readable, writable and executable at the same time. Nothing prevents a script from changing the code running next, or jumping to the stack section and assume the stack just contains code to run.

However, like the early day computers, this architecture has certain problems:

1. It makes the script running in CKB VM very prone to security problems. A buffer overflow, when not managed well, can easily lead to rewriting of code section, which changes the script behavior. On the other hand, specially crafted scripts can also be used to corrupt data section.
2. It also complicates the implementation of CKB VM, when we apply certain optimizations such as [trace cache](https://en.wikipedia.org/wiki/Trace_Cache), we have to add [special code](https://github.com/nervosnetwork/ckb-vm/blob/16207caf5755b5edde6df8228a2366a553960a10/src/machine.rs#L431) to make sure memory writes also invalidates certain trace cache, which is both error-prone and time consuming.

As a result, a small feature named [W^X](https://en.wikipedia.org/wiki/W%5EX) is added to CKB VM, basically, it ensures the memory is either writable or executable. Syscalls will be provided to make the conversion between writable memory and executable memory.

For a more complete CPU model with proper MMU unit, it might not be necessary to make this feature mandatory, but we argue that in the sense of CKB VM, having mandatory W^X can actually be extremely useful here:

1. It provides a way for the script to avoid most easily made mistakes out there by having clear distinction between writable memory, and executable memory. Obviously, attacks like [ROP](https://en.wikipedia.org/wiki/Return-oriented_programming) are still possible but W^X can already help with many types of exploits and beginner mistakes.
2. It also simplifies the implementation significantly. In a VM with proper MMU, this won't make much difference, but for CKB VM which already lacks MMU, this can help reduce the last complicated piece in the memory part. In addition, it also enables us to more easily build JIT or even AOT solutions for CKB VM.

### W^X Specification

Following RISC-V specification, CKB VM will divide its running memory into multiple 4KB memory pages. The memory pages will be aligned on a 4KB boundary, meaning the memory pages would start at 0x0, 0x1000, 0x2000, etc. For each memory page, CKB VM will maintain separate flag denoting if the page is writable or executable. Notice the 2 flags will be mutual exclusive, meaning a memory page can either be writable or executable, but not both. The following checks will also be added:

* Before executing an instruction, CKB VM will ensure the memory page containing current instruction is marked executable.
* Before issuing a memory write, CKB VM will ensure the memory page that is written to is marked writable.

Violating either rule above will result in page faults. Handling page faults will be discussed below.

When booting CKB VM, all memory pages will be marked as writable, except for the `LOAD` code sections marked as `executable` in ELF. CKB VM will return immediately with an error when the ELF file tries to load a code section that is both `writable` and `executable`.

When loading a executable code section that is not page aligned in ELF, CKB VM will enlarge the code section just enough to make it aligned to page boundaries. For example, loading an executable code section starting from 0x139080 which spans 0x1320 bytes will result in the memory from 0x139000 till 0x13b000 be marked as executable.

## Libraries and bootloader

CKB provides additional libraries in the form of VM libraries, and system cell. This is to make sure contract size can be reduced to bare minimum. Those libraries might include: libc, crypto libraries, IO libraries for reading/writing inputs/outputs, and additional tools for working with Cell. All those libraries would be implemented via dynamic linking to reduce contract size.

In addition, we will provide custom bootloader which might be used in compiler(gcc/llvm) linking phase to further reduce unnecessary cost.

Based on current architecture, the following minimal C contract can be shrinked to 628 bytes uncompressed, and 313 bytes gzipped:

```c
int main()
{
  return 0;
}
```

We can think this as the intrinsic cost of RISC-V model.

## Languages

CKB only defines the low level virtual machine. In theory, any languages with RISC-V backend can be used for CKB contract development:

* CKB can leverage standard riscv-gcc, riscv-llvm or even upstream gcc/llvm for C/C++ contract development. Executables emitted by those compilers can be directly used as CKB contracts.
* C-based Bitcoin or Ethereum VM can also be compiled into RISC-V binaries as common cells, contracts can then load those common cells to run Bitcoin or Ethereum compatible contracts.
* Higher-level language VMs, such as [duktape](http://duktape.org/) or [mruby](https://github.com/mruby/mruby) can also be compiled and loaded to run contracts running by JavaScript or Ruby
* [Rust](https://github.com/riscv-rust/rust) can also be used to write contracts with recent development in this space

## Runtime Cost

CKB will leverage suitable open source RISC-V CPU implementation as the CPI(cycle per instruction) model. CPU cycles will be gathered while running each instruction of a contract. The total cycles accumulated when contract is completed will then be treated as the runtime cost of the contract.

In addition, we will also record running costs of reading/writing additional cells while running a contract.

## Example

Here a user defined token(UDT) issuing process will be used as an example. Note that the UDT implementation used here is simplified here:

* 64-bit integer is used to store token number instead of 256-bit integer
* Simple linear array is used instead of hashtable as account data structure. A strict upper bound is also used for simplicity
* Alphabetical order is used to store accounts, so a simple memcmp can be used to determine data structure equality in exchange for slight performance penalty
* Instead of a serialization step, C layout is used for storage

In production, the above assumptions won't be made in CKB

### Data structure

Following data structure is used to store token account information:

```c
#define ADDRESS_LENGTH 32
#define MAX_BALANCES 100
#define MAX_ALLOWED 100

typedef struct {
  char address[ADDRESS_LENGTH];
  int64_t tokens;
} balance_t;

typedef struct {
  char address[ADDRESS_LENGTH];
  char spender[ADDRESS_LENGTH];
  int64_t tokens;
} allowed_t;

typedef struct {
  balance_t balances[MAX_BALANCES];
  int used_balance;
  allowed_t allowed[MAX_ALLOWED];
  int used_allowed;

  char owner[ADDRESS_LENGTH];
  char newOwner[ADDRESS_LENGTH];
  int64_t total_supply;
} data_t;
```

Following APIs are provided to work on the above data structures:

```c
int udt_initialize(data_t *data, char owner[ADDRESS_LENGTH], int64_t total_supply);
int udt_total_supply(const data_t *data);
int64_t udt_balance_of(data_t *data, const char address[ADDRESS_LENGTH]);
int udt_transfer(data_t *data, const char from[ADDRESS_LENGTH], const char to[ADDRESS_LENGTH], int64_t tokens);
int udt_approve(data_t *data, const char from[ADDRESS_LENGTH], const char spender[ADDRESS_LENGTH], int64_t tokens);
int udt_transfer_from(data_t *data, const char from[ADDRESS_LENGTH], const char spender[ADDRESS_LENGTH], const char to[ADDRESS_LENGTH], int64_t tokens);
```

It's both possible to compile implementations of those functions directly into the contract, or as dynamic linking cell code. Both solutions will be introduced below.

### Issuing tokens

Assume CKB has the following method for reading cell data:

```c
int ckb_read_cell_data(size_t index, size_t source, void** buffer, size_t* size);
```

Given a cell ID, CKB VM will mmap cell content to address space of current virtual machine, and returns pointer to the content and size.

Following contract can then be used for issuing tokens:

```c
int udt_initialize(data_t *data, char owner[ADDRESS_LENGTH], int64_t total_supply)
{
  memset(&data, 0, sizeof(data_t));
  memcpy(data->owner, owner, ADDRESS_LENGTH);
  memcpy(data->balances[0].address, owner, ADDRESS_LENGTH);

  data->balances[0].tokens = total_supply;
  data->used_balance = 1;
  data->used_allowed = 0;
  data->total_supply = total_supply;

  return 0;
}

int main(int argc, char* argv[]) {
  data_t data;
  ret = udt_initialize(&data, "<i am an owner>", 10000000);
  if (ret != 0) {
    return ret;
  }

  data_t *output_data = NULL;
  ret = ckb_read_cell(0, CKB_SOURCE_OUTPUT, (void **) &output_data, NULL);
  if (ret != 0) {
    return ret;
  }

  if (memcmp(&data, output_data, sizeof(data_t)) != 0) {
    return -1;
  }
  return 0;
}
```

It ensures generated data is legit by validating that contents in output cell match contents generated in token initializing steps.

### Transfer

In the above example, function implementation for validating cell is directly compiled into input contract script. It's also possible to reference and call code from external cell for validation.

First, the following implementation can be provided for transferring UDT tokens:

```c
int udt_transfer(data_t *data, const char from[ADDRESS_LENGTH], const char to[ADDRESS_LENGTH], int64_t tokens)
{
  balance_t *from_balance = NULL, *to_balance = NULL;
  int ret = _udt_find_balance(data, from, 1, &from_balance);
  if (ret != 0) {
    return ret;
  }
  ret = _udt_find_balance(data, to, 1, &to_balance);
  if (ret != 0) {
    return ret;
  }
  if (from_balance->tokens < tokens) {
    return ERROR_NOT_SUFFICIENT_BALANCE;
  }
  int target = to_balance->tokens + tokens;
  if (target < to_balance->tokens) {
    return ERROR_OVERFLOW;
  }
  from_balance->tokens -= tokens;
  to_balance->tokens = target;
  return 0;
}
```

`_udt_find_balance` here is used to locate `balance_t` data structure given an address, and also create an entry if the address doesn't already exist. Here we omit the full implementation for this function, please refer to CKB codebase for full example.

Following binary code is compiled result of this function:

```c
00000000 <_udt_find_balance>:
   0:   7179                    addi    sp,sp,-48
   2:   d606                    sw      ra,44(sp)
   4:   d422                    sw      s0,40(sp)
   6:   1800                    addi    s0,sp,48
   8:   fca42e23                sw      a0,-36(s0)
   c:   fcb42c23                sw      a1,-40(s0)
  10:   fcc42a23                sw      a2,-44(s0)
  14:   fcd42823                sw      a3,-48(s0)
  18:   fe042623                sw      zero,-20(s0)
  1c:   57fd                    li      a5,-1
  1e:   fef42423                sw      a5,-24(s0)
  22:   a835                    j       5e <.L2>

00000024 <.L5>:
  24:   fec42703                lw      a4,-20(s0)
  28:   87ba                    mv      a5,a4
  2a:   078a                    slli    a5,a5,0x2
  2c:   97ba                    add     a5,a5,a4
  2e:   078e                    slli    a5,a5,0x3
  30:   fdc42703                lw      a4,-36(s0)
  34:   97ba                    add     a5,a5,a4
  36:   02000613                li      a2,32

<omitted ...>
```

Tools will be provided by CKB to encode the binary code here as cell data. Following input contract script can then be used:

```c
typedef int *transfer(data_t *, const char*, const char*, int64_t);

int main(int argc, char* argv[]) {
  data_t *input_data = NULL;
  ret = ckb_read_cell(0, CKB_SOURCE_INPUT, (void **) &input_data, NULL);
  if (ret != 0) {
    return ret;
  }

  data_t *output_data = NULL;
  ret = ckb_read_cell(1, CKB_SOURCE_OUTPUT, (void **) &output_data, NULL);
  if (ret != 0) {
    return ret;
  }

  transfer *f = (transfer *) ckb_mmap_cell(function_cell_id, 0, -1, PROT_EXEC);
  ret = f(input_data, from, to, 100);
  if (ret != 0) {
    return ret;
  }

  if (memcmp(input_data, output_data, sizeof(data_t)) != 0) {
    return -1;
  }
  return 0;
}
```

With mmap, we load a cell directly as a callable function, this function is then used to complete the transfer. This way we can ensure contract size stays minimal while reusing the same method across multiple transactions.

## Multi-function support via dynamic linking

Even though transfer method is stored as an external cell in the above example, one disadvantage here is that the memory address of the mmapped function is unknown at compile time. As a result, internal implementation within that method can only leverage local jumps. In addition, only one function is supported this way, there's no way to store multiple function in a single cell.

Dynamic linking is provide to solve this problem: assuming we have all UDT functions compiled as a shared library in one cell:

```c
int udt_initialize(data_t *data, char owner[ADDRESS_LENGTH], int64_t total_supply);
int udt_total_supply(const data_t *data);
int64_t udt_balance_of(data_t *data, const char address[ADDRESS_LENGTH]);
int udt_transfer(data_t *data, const char from[ADDRESS_LENGTH], const char to[ADDRESS_LENGTH], int64_t tokens);
int udt_approve(data_t *data, const char from[ADDRESS_LENGTH], const char spender[ADDRESS_LENGTH], int64_t tokens);
int udt_transfer_from(data_t *data, const char from[ADDRESS_LENGTH], const char spender[ADDRESS_LENGTH], const char to[ADDRESS_LENGTH], int64_t tokens);
```

With dynamic linking, following input script can be used:

```c
int main(int argc, char* argv[])
{
  data_t *input_data = NULL;
  ret = ckb_read_cell(0, CKB_SOURCE_INPUT, (void **) &input_data, NULL);
  if (ret != 0) {
    return ret;
  }

  data_t *output_data = NULL;
  ret = ckb_read_cell(0, CKB_SOURCE_OUTPUT, (void **) &output_data, NULL);
  if (ret != 0) {
    return ret;
  }

  if (strcmp(argv[4], "initialize") == 0) {
    // processing initialize arguments
    ret = udt_initialize(...);
    if (ret != 0) {
      return ret;
    }
  } else if (strcmp(argv[4], "transfer") == 0) {
    // processing transfer arguments
    ret = udt_transfer(input_data, ...);
    if (ret != 0) {
      return ret;
    }
  } else if (strcmp(argv[4], "approve") == 0) {
    // processing approve arguments
    ret = udt_approve(input_data, ...);
    if (ret != 0) {
      return ret;
    }
  }
  // more commands here

  if (memcmp(input_data, output_data, sizeof(data_t)) != 0) {
    return -1;
  }
  return 0;
}
```

Here all UDT functions are linked dynamically from external cells, current contract can be minimized in terms of size.

### Update

Any new version of CKB-VM will not affect the execution results of the old transactions. We released CKB-VM version 1 in the CKB hardfork [1].

# Reference

* [1]: [CKB-VM version 1][1]

[1]: ../0033-ckb-vm-version-1/0033-ckb-vm-version-1.md


================================================
File: rfcs/0003-ckb-vm/0003-ckb-vm.zh.md
================================================
---
Number: "0003"
Category: Informational
Status: Final
Author: Xuejie Xiao <xxuejie@gmail.com>
Created: 2018-08-01
---

# CKB-VM

## 概述

CKB 的 VM 层用于在给定 transaction 的 inputs 与 outputs 的情况下，执行一系列验证条件，以判断 transaction 是否合法并返回结果。

CKB 使用 [RISC-V](https://riscv.org/) 指令集来实现虚拟机层。更精确的说，CKB 使用 rv64imc 指令集架构：基于 [RV64I](https://riscv.org/specifications/) 核心指令集，并添加 RV32M 整型乘除法扩展以及 RVC 指令压缩功能。注意 CKB 不支持浮点数运算，合约开发者如有需要，可以通过添加 softfloat 实现来完成相应功能。

CKB 通过动态链接库的方式，依赖 syscall 来实现链上运算所需的其他功能，比如读取 Cell 的内容，或是其他与 block 相关的普通运算及加密运算。任何支持 RV64I 的编译器 (如 [riscv-gcc](https://github.com/riscv/riscv-gcc), [riscv-llvm](https://github.com/lowRISC/riscv-llvm), [Rust](https://github.com/rust-embedded/wg/issues/218)) 生成的可执行文件均可以作为 CKB VM 中的 script 来运行。

## RISC-V 运行模型

CKB 中使用 64 位的 RISC-V 虚拟机作为 VM 来执行合约。VM 运行在 64 位地址空间下，提供了 RV32I 定义的核心指令集，以及 RV64M 扩展中的整型乘除法的扩展指令。为减小生成的合约大小，CKB 还支持 RVC 指令压缩功能，尽可能减小指令的存储开销。合约会直接使用 Linux 的 ELF 可执行文件格式，以方便对接开源社区的工具及离线调试。

每个合约在 gzip 后最大提供 1MB 的存储空间，解压后的原始合约最大限制为 10 MB。合约运行时，CKB 虚拟机会为合约提供 128 MB 的运行空间，其中包含合约可执行文件映射到虚拟机上的代码页，合约运行时需要的栈空间，堆空间以及外部的 Cell 通过 mmap 映射后的地址页。

为保证合约运行的唯一性及安全性，CKB 虚拟机中的内存及所有寄存器在未被访问之前，均全部写入 0。

合约的运行等同于 Linux 环境下一个可执行文件在单核 CPU 下的运行：

```c
int main(int argc, char* argv[]) {
  uint64_t input_cell_length = 10000;
  void *input_cell = malloc(input_cell_length);
  ckb_load_cell(input_cell, &input_cell_length, 0, 0, CKB_SOURCE_INPUT);

  uint64_t output_cell_length = 10000;
  void *output_cell = malloc(output_cell_length);
  ckb_load_cell(output_cell, &output_cell_length, 0, 0, CKB_SOURCE_OUTPUT);

  // Consume input & output cell

  return 0;
}
```

合约运行从合约 ELF 文件中的 main 函数开始执行，通过 argc 与 argv 提供输入参数进行合约的执行，当 main 函数返回值为 0 时，认为合约执行成功，否则合约执行失败。注意这里的 argc 与 argv 并不保存完整的 inputs 以及 outputs 数据，而是只保留相应的 metadata，对 inputs 与 outputs 的读取则通过单独定义的库与 syscalls 来实现，以便减少不必要的开销。同时 CKB VM 仅为单线程模型，合约文件可以自行提供 coroutine 实现，但是在 VM 层不提供 threading。

基于简化实现以及确定性的考虑，CKB 不提供浮点数运算。如果有对浮点数的需要，我们建议通过引入 softfloat 来实现需求。同时由于 CKB VM 仅为单线程模型，不提供对于原子性操作的支持。

## 辅助库与 Bootloader

为了尽可能减小合约本身的存储开销，CKB 会在 VM 层及 system cell 中提供合约运行所需的辅助库，包括但不限于：libc 中提供的函数，加密库，读写 inputs，outputs 以及其他 Cell 的工具库。所有这些库通过动态链接的形式提供，以确保不占用合约自身的空间。

与此同时 CKB 会提供定制的简化版 bootloader 用于 gcc, llvm 等编译器的链接步骤，以确保省去不必要的开销。

在目前的条件下，对于如下最简单的合约 C 代码：

```c
int main()
{
  return 0;
}
```

编译后的合约代码大小为 628 字节，gzip 后为 313 字节。可以认为这 313 字节为 RISC-V 合约模型下的固定开销。

## 开发语言

CKB 核心只定义了底层的虚拟机模型，理论上任何提供了 RISC-V 后端的语言均可以用来开发 CKB 合约:

* CKB 可以直接使用标准的 riscv-gcc 以及 riscv-llvm 以 C/C++ 语言来进行开发。编译后的可执行文件可以直接作为 CKB 的合约来使用
* 与此相应的，可以将 C 实现的 Bitcoin 以及 Ethereum VM 编译成 RISC-V 二进制代码，保存在公共 Cell 中，然后在合约中引用公共 Cell 来运行 Bitcoin 或者 Ethereum 的合约
* 其他的高级语言 VM 如 [duktape](http://duktape.org/) 及 [mruby](https://github.com/mruby/mruby) 在编译后，也可以用来相应的运行 JavaScript 或者 Ruby 编写的合约
* 相应的也可以使用 [Rust](https://github.com/riscv-rust/rust) 作为实现语言来编写合约

## Runtime Cost

CKB 会选取合适的 RISC-V 开源实现作为运行模型。在执行合约时，可以收集每条指令执行所需的时钟周期。合约执行完毕后，累积的总时钟周期既可作为合约运行的开销。与此同时，我们还会针对读取 Cell 中内容的操作收取合适的运行开销。

## 示例

以下通过一个用户自定义代币(user defined token, or UDT)的发行过程来介绍 CKB 中虚拟机的执行过程。需要注意的是，为了简化说明，这里描述的 UDT 实现经过了一定程度的简化：

* 使用 64 位整数，而不是 256 位整数来保存代币数目
* 使用简化的线性数组与顺序查询的方式代替哈希数据结构存储代币发行情况。同时对代币最多能发给的账户数直接做上限限制
* 同时这里假设所有的账户信息是按字典序顺序排列，于是判断两组数据结构是否相同就简化成了 memcmp 操作，不需要依次遍历数据结构来判断
* 使用 C 的 struct layout 来直接保存数据，省去序列化的步骤

注意，在生产环境 CKB 不会有以上的假设。

### 数据结构

代币信息保存在如下数据结构内：

```c
#define ADDRESS_LENGTH 32
#define MAX_BALANCES 100
#define MAX_ALLOWED 100

typedef struct {
  char address[ADDRESS_LENGTH];
  int64_t tokens;
} balance_t;

typedef struct {
  char address[ADDRESS_LENGTH];
  char spender[ADDRESS_LENGTH];
  int64_t tokens;
} allowed_t;

typedef struct {
  balance_t balances[MAX_BALANCES];
  int used_balance;
  allowed_t allowed[MAX_ALLOWED];
  int used_allowed;

  char owner[ADDRESS_LENGTH];
  char newOwner[ADDRESS_LENGTH];
  int64_t total_supply;
} data_t;
```

对于数据结构有如下的 API 来提供各种操作：

```c
int udt_initialize(data_t *data, char owner[ADDRESS_LENGTH], int64_t total_supply);
int udt_total_supply(const data_t *data);
int64_t udt_balance_of(data_t *data, const char address[ADDRESS_LENGTH]);
int udt_transfer(data_t *data, const char from[ADDRESS_LENGTH], const char to[ADDRESS_LENGTH], int64_t tokens);
int udt_approve(data_t *data, const char from[ADDRESS_LENGTH], const char spender[ADDRESS_LENGTH], int64_t tokens);
int udt_transfer_from(data_t *data, const char from[ADDRESS_LENGTH], const char spender[ADDRESS_LENGTH], const char to[ADDRESS_LENGTH], int64_t tokens);
```

这些方法的实现既可以直接编译到合约中，也可以保存在 Cell 中，通过动态链接的方式来提供。以下会分别介绍两种使用方式。

### 代币发行

假设 CKB 提供如下的方法用来读取 Cell 中的内容：

```c
int ckb_read_cell_data(size_t index, size_t source, void** buffer, size_t* size);
```

即给定 Cell ID，CKB 的虚拟机读取 Cell 中的内容，并映射到当前虚拟机的地址空间中，返回相应的指针，与 Cell 的大小。

这样就可以通过如下的合约来发行代币：

```c
int udt_initialize(data_t *data, char owner[ADDRESS_LENGTH], int64_t total_supply)
{
  memset(&data, 0, sizeof(data_t));
  memcpy(data->owner, owner, ADDRESS_LENGTH);
  memcpy(data->balances[0].address, owner, ADDRESS_LENGTH);

  data->balances[0].tokens = total_supply;
  data->used_balance = 1;
  data->used_allowed = 0;
  data->total_supply = total_supply;

  return 0;
}

int main(int argc, char* argv[]) {
  data_t data;
  ret = udt_initialize(&data, "<i am an owner>", 10000000);
  if (ret != 0) {
    return ret;
  }

  data_t *output_data = NULL;
  ret = ckb_read_cell(0, CKB_SOURCE_OUTPUT, (void **) &output_data, NULL);
  if (ret != 0) {
    return ret;
  }

  if (memcmp(&data, output_data, sizeof(data_t)) != 0) {
    return -1;
  }
  return 0;
}
```

通过验证 Output Cell 中的数据与自行初始化后的 UDT 代币数据是否一致，这里可以确保当前合约及生成数据均是正确的。

### 转账

上述发行代币模型中，验证 Cell 的脚本直接保存在了 input script 中。这里其实也可以通过引用外部 Cell 的方式，调用外部代码来实现验证 Cell 的方法。

考虑 UDT 代币的转账模型，首先有如下基于 C 的实现：

```c
int udt_transfer(data_t *data, const char from[ADDRESS_LENGTH], const char to[ADDRESS_LENGTH], int64_t tokens)
{
  balance_t *from_balance = NULL, *to_balance = NULL;
  int ret = _udt_find_balance(data, from, 1, &from_balance);
  if (ret != 0) {
    return ret;
  }
  ret = _udt_find_balance(data, to, 1, &to_balance);
  if (ret != 0) {
    return ret;
  }
  if (from_balance->tokens < tokens) {
    return ERROR_NOT_SUFFICIENT_BALANCE;
  }
  int target = to_balance->tokens + tokens;
  if (target < to_balance->tokens) {
    return ERROR_OVERFLOW;
  }
  from_balance->tokens -= tokens;
  to_balance->tokens = target;
  return 0;
}
```

其中 `_udt_find_balance` 的作用是给定地址，从当前代币数据结构中找到该地址对应的 `balance_t` 数据结构。如果该地址不存在的话，则在数据结构中创建该地址的条目。在这里我们略去实现，完整的例子可以参考 CKB 代码库。

可以将该函数编译，得到对应的二进制代码：

```c
00000000 <_udt_find_balance>:
   0:   7179                    addi    sp,sp,-48
   2:   d606                    sw      ra,44(sp)
   4:   d422                    sw      s0,40(sp)
   6:   1800                    addi    s0,sp,48
   8:   fca42e23                sw      a0,-36(s0)
   c:   fcb42c23                sw      a1,-40(s0)
  10:   fcc42a23                sw      a2,-44(s0)
  14:   fcd42823                sw      a3,-48(s0)
  18:   fe042623                sw      zero,-20(s0)
  1c:   57fd                    li      a5,-1
  1e:   fef42423                sw      a5,-24(s0)
  22:   a835                    j       5e <.L2>

00000024 <.L5>:
  24:   fec42703                lw      a4,-20(s0)
  28:   87ba                    mv      a5,a4
  2a:   078a                    slli    a5,a5,0x2
  2c:   97ba                    add     a5,a5,a4
  2e:   078e                    slli    a5,a5,0x3
  30:   fdc42703                lw      a4,-36(s0)
  34:   97ba                    add     a5,a5,a4
  36:   02000613                li      a2,32

<omitted ...>
```

CKB 会提供工具链，可以将这里的二进制代码直接作为数据生成 Cell，于是可以有如下的 input script:

```c
typedef int *transfer(data_t *, const char*, const char*, int64_t);

int main(int argc, char* argv[]) {
  data_t *input_data = NULL;
  ret = ckb_read_cell(0, CKB_SOURCE_INPUT, (void **) &input_data, NULL);
  if (ret != 0) {
    return ret;
  }

  data_t *output_data = NULL;
  ret = ckb_read_cell(0, CKB_SOURCE_OUTPUT, (void **) &output_data, NULL);
  if (ret != 0) {
    return ret;
  }

  transfer *f = (transfer *) ckb_mmap_cell(function_cell_id, 0, -1, PROT_EXEC);
  ret = f(input_data, from, to, 100);
  if (ret != 0) {
    return ret;
  }

  if (memcmp(input_data, output_data, sizeof(data_t)) != 0) {
    return -1;
  }
  return 0;
}
```

这里通过 mmap 的方式将一个 Cell 中的内容映射为可以调用的方法，然后调用这个方法来完成转账的目的。这样可以保证方法得到重用，同时也可以减小合约的大小。

### 多方法支持

上面的示例中，虽然转账方法放在了 Cell 中，但是这里的验证方法仍然有一个问题：由于方法是直接 mmap 到内存中，在编译期并不知道 mmap 之后方法所处的内存地址，所以方法的内部实现只能使用局部跳转，无法使用全局跳转。同时在一段内存空间内也只能放入一个验证方法，没有办法支持有多个方法的调用库。

这里我们也可以通过动态链接的方式来使用外部 Cell 提供的辅助库。假设在某一个 Cell 中已经提供了 UDT 代币的所有实现:

```c
int udt_initialize(data_t *data, char owner[ADDRESS_LENGTH], int64_t total_supply);
int udt_total_supply(const data_t *data);
int64_t udt_balance_of(data_t *data, const char address[ADDRESS_LENGTH]);
int udt_transfer(data_t *data, const char from[ADDRESS_LENGTH], const char to[ADDRESS_LENGTH], int64_t tokens);
int udt_approve(data_t *data, const char from[ADDRESS_LENGTH], const char spender[ADDRESS_LENGTH], int64_t tokens);
int udt_transfer_from(data_t *data, const char from[ADDRESS_LENGTH], const char spender[ADDRESS_LENGTH], const char to[ADDRESS_LENGTH], int64_t tokens);
```

于是可以在编译期时直接指定链接方式为动态链接，这样便可以有如下的 input script:

```c
int main(int argc, char* argv[])
{
  data_t *input_data = NULL;
  ret = ckb_read_cell(0, CKB_SOURCE_INPUT, (void **) &input_data, NULL);
  if (ret != 0) {
    return ret;
  }

  data_t *output_data = NULL;
  ret = ckb_read_cell(0, CKB_SOURCE_OUTPUT, (void **) &output_data, NULL);
  if (ret != 0) {
    return ret;
  }

  if (strcmp(argv[4], "initialize") == 0) {
    // processing initialize arguments
    ret = udt_initialize(...);
    if (ret != 0) {
      return ret;
    }
  } else if (strcmp(argv[4], "transfer") == 0) {
    // processing transfer arguments
    ret = udt_transfer(input_data, ...);
    if (ret != 0) {
      return ret;
    }
  } else if (strcmp(argv[4], "approve") == 0) {
    // processing approve arguments
    ret = udt_approve(input_data, ...);
    if (ret != 0) {
      return ret;
    }
  }
  // more commands here

  if (memcmp(input_data, output_data, sizeof(data_t)) != 0) {
    return -1;
  }
  return 0;
}
```

这里所有的 UDT 函数均通过动态链接的方式引用其他 Cell 里的内容，不占用当前 Cell 的空间。

### 更新

任何 CKB-VM 的新版本均不会影响旧有交易的执行结果. 我们在 CKB 硬分叉版本中发布了 CKB-VM version 1 [1].

# 参考

* [1]: [CKB-VM version 1][1]

[1]: ../0033-ckb-vm-version-1/0033-ckb-vm-version-1.md


================================================
File: rfcs/0004-ckb-block-sync/0004-ckb-block-sync.md
================================================
---
Number: "0004"
Category: Information
Status: Final
Author: Ian Yang <@doitian>
Organization: Nervos Foundation
Created: 2018-07-25
---

# CKB Block Synchronization Protocol

Glossary of Terms

- Chain: a list of blocks starting with genesis block and consisted of successive blocks.
- Best Chain: a chain with the most accumulated PoW, and starting with a common genesis block which nodes agree with the consensus.
- Best Header Chain: a chain with the most PoW and consisted only of blocks in the status of Connected, Downloaded or Accepted. Please refer to block status part for more details.
- Tip: the latest block of a chain and Tip can be used to determine a specific chain.
- Best Chain Tip: the tip of Best Chain.
- Chain: a list of blocks starting with genesis block and consisted of successive blocks.

## Abstract

Block synchronization **must** be performed in stages with [Bitcoin Headers First](https://bitcoin.org/en/glossary/headers-first-sync) style. Blocks are downloaded in parts in each stage and are validated using the obtained parts.

1. Connecting Header: Get block header, and validate format and PoW.
2. Downloading Block: Get and validate the complete block. Transactions in ancestor blocks are not required.
3. Accepting Block: Validate the block in the context of the chain.

The purpose of stage execution is trying to preclude most of the attacks with the least cost. For example, in the first step, header connecting only accounts for 5% workload while there would be 95% possibility to say the block is valid.

According to the execution stages, there are 5 statuses of blocks:

1. Unknown: the status of a block is unknown before header connecting.
2. Invalid: A block and all of its descendant blocks are marked as 'Invalid' if any above steps failed.
3. Connected: A block succeeds in stage Connecting Header, and all its ancestor blocks are in a status of Connected, Downloaded or Accepted.
4. Downloaded: A block succeeds in stage Downloading Block, and all its ancestor blocks are in a status of Downloaded or Accepted.
5. Accepted: A block succeeds in stage Accepting Block, and all its ancestor blocks are in the status of Accepted.

Block status is propagated from the previous block to the later ones. Using the list index number above, the status number of a block is always less than or equal to its parent block. Here are conditions, if a block is invalid, all of its descendant blocks must be invalid. The cost of every step for synchronization is higher than the previous one and every step may fail. In this scenario, work will be wasted if a child block enters a later status before its parent block, and parent block is approved to be Invalid later.

Initially, Genesis block is in status Accepted and the rest is in status Unknown.

Below figures are used to indicate blocks in different status later on.

![](images/block-status.jpg "Block Status")

Genesis block of the nodes synchronizing **must be** the same, and all blocks can be constructed as a tree with the genesis block being the root. Blocks will be removed if they cannot connect to the root eventually.

Every participating node forms its local status tree where the chain consisting of Accepted blocks with the most PoW is considered as Best Chain. The chain that consists of blocks in the status of connected, downloaded or accepted with the most PoW is Best Header Chain.

The graph below is an example of Status Tree formed by Alice and blocks signed with name Alice is this node's current Best Chain Tip.

![](images/status-tree.jpg "Status Tree by Alice")

## Connecting Header

Headers first synchronization helps to validate PoW with the least cost. Since it costs the same work to construct PoW whether the included transactions are valid or not, attackers may use other more efficient ways. It means it's highly possible to regard the whole block as valid when the PoW is valid. This is why headers first synchronization would avoid resource-wasting on invalid blocks.

Because of the low cost, Headers synchronization can be processed in parallel with all peers and construct a highly reliable global graph. In this way, block downloading can be scheduled in the most efficient way to avoid wasting resource on lower PoW branch.

The goal of connecting header is demonstrated using the following example. When Alice connects to Bob, Alice asks Bob to send all block headers in Bob's Best Chain but not in Alice's **Best Header Chain** and then validate them to decide the blocks status are either Connected or Invalid.

When Alice connects header, keeping Best Header Chain Tip updated could help to decrease numbers of receiving headers already existed.

![](images/seq-connect-headers.jpg)

The graph above instructs the process of connecting headers. After a round of connecting headers, nodes are supposed to keep up-to-date using new block notification.

Take Alice and Bob above as an example, firstly Alice samples blocks from her Best Header Chain and sent the hashes to Bob. The basic principle of sampling is that later blocks are more possible to be selected than early blocks. For example, choose latest 10 blocks from the chain, then sample other blocks backward with 2's exponential increased intervals, a.k.a, 2, 4, 8, and etc. The list of hashes of the sampled blocks is called a Locator. In the following figure, the undimmed blocks are sampled. The genesis block should be always in the Locator.

![](images/locator.jpg)

Bob can get the latest common block between these two chains according to Locator and his own Best Chain. Because the genesis block is identical, there must be such kind of block. Bob will send all block headers from the common block to his Best Chain Tip to Alice.

![](images/connect-header-conditions.jpg)

In the figure above, blocks with undimmed color should be sent from Bob to Alice, and golden bordered one is the latest common block. There are three possible cases in the process:

1. If Bob's Best Chain Tip is in Alice's Best Header Chain, the latest common block will be Bob's Best Chain Tip and there are no block headers for Bob to send.
2. If Alice's Best Header Chain Tip is in Bob's Best Chain but is not the Tip, the latest common block will be Alice's Best Header Chain Tip.
3. If Alice's Best Header Chain and Bob's Best Chain fork, the latest common block will be the one before the fork occurs.

If there are too many blocks to send, pagination is required. Bob sends the first page, Alice will ask Bob for the next page if she finds out that there are more block headers. A simple pagination solution is to limit the maximum number of block headers returned each time, 2000 for example. If the number of block headers returned is equal to 2000, it means there may be more block headers could be returned. If the last block of a certain page is the ancestor of Best Chain Tip or Best Header Chain Tip, it can be optimized to get next page starting with the corresponding tip.

Alice could observe Bob's present Best Chain Tip, which is the last block received during each round of synchronization. If Alice's Best Header Chain Tip is exactly Bob's Best Chain Tip, Alice couldn't observe Bob's present Best Chain because Bob has no block headers to send. Therefore, it should start building from the parent block of Best Header Chain Tip when sending the first request in each round.

In the following cases, a new round of connection block header synchronization must be performed.

- Received a new block notification from the others, but the parent block status of the new block is Unknown.

The following exceptions may occur when connecting a block header:

- Alice observed that Bob's Best Chain Tip has not been updated for a long time, or its timestamp is old. In this case, Bob does not provide valuable data. When the number of connections reaches a limit, Bob could be disconnected first.
- Alice observed that the status of Bob's Best Chain Tip is Invalid. This can be found in any page without waiting for the end of a round of Connect Head. There, Bob is on an invalid branch, Alice can stop synchronizing with Bob and add Bob to the blacklist.
- There are two possibilities if the block headers Alice received are all on her own Best Header Chain. One is that Bob sends them deliberately. The other is that Best Chain changes when Alice wants to Connect Head. In this case, those block headers can only be ignored because they are difficult to distinguish. However, the proportion of received blocks already in Best Header Chain would be recorded. If the proportion is above a certain threshold value, Bob may be added to the blacklist.

Upon receiving the block header message, the format should be verified first.

- The blocks in the message are continuous.
- The status of all blocks and the parent block of the first block are not Invalid in the local Status Tree.
- The status of the parent block of the first block is not Unknown in the local Status Tree, which means Orphan Block will not be processed in synchronizing.

In this stage, verification includes checking if block header satisfies the consensus rules and if Pow is valid or not. Since Orphan Blocks are not processed, difficulty adjustment can be verified as well.

![](images/connect-header-status.jpg)

The figure above is the Status Tree of Alice after synchronized with Bob, Charlie, Davis, Elsa. The observed Best Chain Tip of each peer is also annotated in the figure.

If the Unknown status block is considered not on the Status Tree, new blocks in the status of Connected or Invalid will be extended to the leaves of the Status Tree during Connecting Header. As a result, Connecting Header stage explores and extends the status tree.

## Downloading Block

After Connecting Header is completed, the branch of some observed Best Chain Tip ends with one or more Connected block, a.k.a., Connected Chain. Downloading Block stage should start to request complete blocks from peers and perform verification.

With the status tree, synchronization can be scheduled to avoid useless work. An effective optimization is to download the block only if the Best Chain of the observed peer is better than the local Best Chain's. And priority can be ordered that the connected chain with more accumulated PoW should be processed first. The branch with lower PoW can only be attempted if a branch is confirmed to be invalid or if the download times out.

When downloading a branch, earlier blocks should be downloaded firstly due to the dependency of blocks, and should be downloaded concurrently from different peers to utilize full bandwidth. A sliding window can be applied to solve the problem.

Assume that the number of the first Connected status block to be downloaded is M and the length of the sliding window is N, then only the blocks numbered M to M+N-1 can be downloaded. After the block M is downloaded and verified, the sliding window moves to the next Connected block. If verification of block M fails, then the remaining blocks of this branch are all Invalid, and there is no need to continue downloading. If the window does not move towards the right for a long time, it is considered as time out. The node should try again later, or waits until the branch has new connected blocks.

![](images/sliding-window.jpg)

The figure above is an example of an 8 length sliding window. In the beginning, the downloadable block range from 3 to 10. After block 3 is downloaded, the window will move to block 5 because block 4 has already been downloaded in advance (as the figure above illustrated).

The Best Chains of peers are already known in stage Connecting Header, it is assumed that the peer has a block if it is in the peer's Best Chain and that peer is a full node. During the downloading, blocks in the sliding window can be split into several small stripes and those stripes could be scheduled among peers who have the blocks.

The downloaded transactions in a block may be mismatched with the Merkle Hash Root in the header, or the list contains duplicated txid. It doesn't mean that the block is invalid since it can only approve the downloaded block is incorrect. The block content provider could be added to the blacklist, but the block status should not be marked as invalid. Otherwise, the malicious nodes may pollute the nodes' Status Tree by sending the wrong block contents.

Verification of transaction lists and block header matching is required in this stage, but any validation that relies on the transaction contents in the ancestor block is not required, which will be placed in the next stage.

Several validations can be checked in this phase, for example, Merkle Hash validation, transaction txid cannot be repeated, transaction list cannot be empty, inputs and outputs cannot be blank at the same time, or only the first transaction can be generation transaction, etc.

Downloading Block will update the status of blocks in the best Connected Chain, from Connected to Downloaded or Invalid.

## Accepting Block

In the previous stage, there will be some chains which ended with one or more Downloaded status, hereinafter referred to as Downloaded Chain. If those chains' cumulative work is more than Best Chain Tip's, the complete validation in the chain context should be performed in this stage. If there are more than one chains satisfied, the chain with the most work should be performed first.

All the verification must be completed in this stage, including all rules that depend on historical transactions.

Because it involves UTXO (unspent transaction outputs) indexes, the cost of verification is huge in this phase. One set of UTXO indexes is sufficient in this simple solution. First rollback local Best Chain Tip necessarily. After that, verify blocks in the candidate best Downloaded Chain and add them to Best Chain one by one. If there is an invalid block during verification, the remain blocks in Downloaded Chain are also considered as Invalid. If so, Best Chain Tip would even have lower work than the previous Tip. It can be resolved in several different ways:

- If the work of Best Chain before rollback is more than present Tip, then restore the previous Best Chain.
- If the work of other Downloaded Chains is more than Best Chain that before rollback, try rollback and relocate to that chain.

The process of Accepting Block will change the status of blocks in the Downloaded chain, from Downloaded to Accepted or Invalid. The verified Downloaded Chain which has the most work will become the new local Best Chain.

## New block announcement

When the local Best Chain Tip changes, the node should push an announcement to peers. The best header with most cumulative work sent to each peer should be recorded, to avoid sending duplicate blocks in the announcement and sending blocks only peer doesn't know. This does not only record headers sent for new blocks, but also the ones sent as the responses in stage Connecting Header.

It is assumed that the peers already know the Best Sent Header and its ancestors, so these blocks can be excluded when sending new block announcements.

![](images/best-sent-header.jpg "Best Sent Header")

From the above example, Alice's Best Chain Tip is annotated with her name. The best header sent to Bob is annotated as "Best Sent To Bob". The undimmed blocks are the ones Alice should send to Bob as new blocks announcement. Following is the detailed description for each step:

1. In the beginning, Alice only has Best Chain Tip to send
2. Another new block is added to the best chain before Alice has a chance to send the headers. In this case, the last two blocks of Best Chain need to be sent.
3. Alice sends the last two blocks to Bob and updates Best Sent to Bob.
4. Alice's Best Chain relocates to another fork. Only blocks after the last common block should be sent to Bob.

How to send the announcement is determined by connection negotiated parameters and the number of new blocks to be announced:

- If there is only one block and the peer prefers Compact Block [^1], then use Compact Block.
- In other cases, just send block header list with an upper limit on the number of blocks to send. For example, if the limit is 8 and there are 8 or more blocks need to be announced, only the latest 7 blocks will be announced.

When receiving a new block announcement, there may be a situation the parent block's status is Unknown, also called Orphan Block. If so, a new round of Connecting Header is required immediately. When a Compact Block is received, and its parent block is the local Best Chain Tip, then the full block may be recovered from the transaction pool. If the recovery succeeds, the work of these three stages can be compacted into one. Otherwise, it falls back to a header-only announcement.

## Synchronization Status

### Configuration
- `GENESIS_HASH`: hash of genesis block
- `MAX_HEADERS_RESULTS`: the max number of block headers can be sent in a single message
- `MAX_BLOCKS_TO_ANNOUNCE`: the max number of new blocks to be announced
- `BLOCK_DOWNLOAD_WINDOW`: the size of the download window

### Storage
- Block Status Tree
- Best Chain Tip, decide whether to download blocks and accept blocks
- Best Header Chain Tip, used in Connecting Header to construct the Locator of the first request in each round.

Each connection peer should store:
- Observed Best Chain Tip
- The block header hash with the most work sent last time —— Best Sent Header

### Message Definition

Only related message and fields are listed here. See completed definition and documentation in the reference implementation.

The message passing is completely asynchronous. For example, sending `GetHeaders` does not block other requests. Also, there is no need to guarantee the order relationship between the requests and the responses. For example, node A sends `GetHeaders` and `GetBlocks` to B, and B can replies `SendBlock` firstly, and then `SendHeaders` to A.

Compact Block [^1] messages will be described in related Compact Block documentation.

### GetHeaders

It is used to request a block header from a peer in stage Connecting Header. The first-page request, and subsequent pages request can share the same GetHeaders message format. The difference between them is that the first page requests generate a Locator from the parent block of the local Best Header Chain Tip, and the subsequent page request generates the Locator using the last block in the last received page.

- `hash_stop`: tells peer to early return when building `SendHeaders` response.
- `block_locator_hashes`: Sampled hashes of the already known blocks

### SendHeaders

It is used to reply `GetHeaders`. It returns a headers list containing the headers of blocks starting right after the last common hash via the Locator, up to `hash_stop` or `MAX_BLOCKS_TO_ANNOUNCE` blocks, whichever comes first.

- `headers`：block headers list

### GetBlocks

It is used in Downloading Block stage.

- `block_hashes`:  list of block hashes to download.

### SendBlock

It is used to reply block downloading request of `GetBlocks`

- `block`: the requested block content

[^1]: Compact Block is a technique for compressing and transferring complete blocks. It is based on the fact that when a new block is propagated, the transactions should already be in the pool of other nodes. Under this circumstances, Compact Block only contains the list of transaction txid list and complete transactions which are predicated unknown to the peers. The receiver can recover the complete block using the transaction pool. Please refer to [Block and Compact Block Structure](../0020-ckb-consensus-protocol/0020-ckb-consensus-protocol.md#block-and-compact-block-structure) and related Bitcoin [BIP](https://github.com/bitcoin/bips/blob/master/bip-0152.mediawiki) for details.


================================================
File: rfcs/0004-ckb-block-sync/0004-ckb-block-sync.zh.md
================================================
---
Number: "0004"
Category: Information
Status: Final
Author: Ian Yang <@doitian>
Organization: Nervos Foundation
Created: 2018-07-25
---

# 链同步协议

术语说明

- Chain: 创世块开头，由连续的块组成的链。
- Best Chain: 节点之间要达成最终一致的、满足共识验证条件的、PoW 累积工作量最高的、以共识的创世块开始的 Chain。
- Best Header Chain: 累积工作量最高，由状态是 Connected, Downloaded 或者 Accepted 的块组成的 Chain。详见下面块状态的说明。
- Tip: Chain 最后一个块。Tip 可以唯一确定 Chain。
- Best Chain Tip: Best Chain 的最后一个块。

## 同步概览

块同步**必须**分阶段进行，采用 [Bitcoin Headers First](https://bitcoin.org/en/glossary/headers-first-sync) 的方式。每一阶段获得一部分块的信息，或者基于已有的块信息进行验证，或者两者同时进行。

1.  连接块头 (Connect Header): 获得块头，验证块头格式正确且 PoW 工作量有效
2.  下载块 (Download Block): 获得块内容，验证完整的块，但是不依赖祖先块中的交易信息。
3.  采用块 (Accept Block): 在链上下文中验证块，会使用到祖先块中的交易信息。

分阶段执行的主要目的是先用比较小的代价排除最大作恶的可能性。举例来说，第一步连接块头的步骤在整个同步中的工作量可能只有 5%，但是完成后能有 95% 的可信度认为块头对应的块是有效的。

按照已经执行的阶段，块可以处于以下 5 种状态：

1.  Unknown: 在连接块头执行之前，块的状态是未知的。
2.  Invalid：任意一步失败，块的状态是无效的，且当一个块标记为 Invalid，它的所有子孙节点也都标记为 Invalid。
3.  Connected: 连接块头成功，且该块到创世块的所有祖先块都必须是 Connected, Downloaded 或 Accepted 的状态。
4.  Downloaded: 下载块成功，且该块到创世块的所有祖先块都必须是 Downloaded 或者 Accepted 的状态。
5.  Accepted: 采用块成功，且该块到创世块的所有祖先块都必须是 Accepted 的状态。

块的状态是会沿着依赖传递的。按照上面的编号，子块的状态编号一定不会大于父块的状态编号。首先，如果某个块是无效的，那依赖它的子孙块自然也是无效的。另外，同步的每一步代价都远远高于前一步，且每一步都可能失败。如果子节点先于父节点进入下一阶段，而父节点被验证为无效，那子节点上的工作量就浪费了。而且，子块验证是要依赖父块的信息的。

初始时创世块状态为 Accepted，其它所有块为 Unknown。

之后会使用以下图示表示不同状态的块：

![](images/block-status.jpg "Block Status")

参与同步的节点创世块**必须**相同，所有的块必然是组成由创世块为根的一颗树。如果块无法最终连接到创世块，这些块都可以丢弃不做处理。

参与节点都会在本地构造这颗状态树，其中全部由 Accepted 块组成的累积工作量最大的链就是 Best Chain。而由状态可以是 Connected, Downloaded 或 Accepted 块组成的累积工作量最大的链就是 Best Header Chain.

下图是节点 Alice 构造的状态树的示例，其中标记为 Alice 的块是该节点当前的 Best Chain Tip。

![](images/status-tree.jpg "Status Tree by Alice")

## 连接块头

先同步 Headers 可以用最小的代价验证 PoW 有效。构造 PoW 时，不管放入无效的交易还是放入有效的交易都需要付出相同的代价，那么攻击者会选择其它更高性价比的方式进行攻击。可见，当 PoW 有效时整个块都是有效的概率非常高。所以先同步 Headers 能避免浪费资源去下载和验证无效块。

因为代价小，同步 Headers 可以和所有的节点同时进行，在本地能构建出可信度非常高的、当前网络中所有分叉的全局图。这样可以对块下载进行规划，避免浪费资源在工作量低的分支上。

连接块头这一步的目标是，当节点 Alice 连接到节点 Bob 之后，Alice 让 Bob 发送所有在 Bob 的 Best Chain 上但不在 Alice 的 **Best Header Chain** 上的块头，进行验证并确定这些块的状态是 Connected 还是 Invalid。

Alice 在连接块头时，需要保持 Best Header Chain Tip 的更新，这样能减少收到已有块头的数量。

![](images/seq-connect-headers.jpg)

上图是一轮连接块头的流程。完成了一轮连接块头后，节点之间应该通过新块通知保持之后的同步。

以上图 Alice 从 Bob 同步为例，首先 Alice 将自己 Best Header Chain 中的块进行采样，将选中块的哈希作为消息内容发给 Bob。采样的基本原则是最近的块采样越密，越早的块越稀疏。比如可以取最后的 10 个块，然后从倒数第十个块开始按 2, 4, 8, … 等以 2 的指数增长的步长进行取样。采样得到的块的哈希列表被称为 Locator。下图中淡色处理的是没有被采样的块，创世块应该始终包含在 Locator 当中。

![](images/locator.jpg)

Bob 根据 Locator 和自己的 Best Chain 可以找出两条链的最后一个共同块。因为创世块相同，所以一定存在这样一个块。Bob 把共同块之后一个开始到 Best Chain Tip 为止的所有块头发给 Alice。

![](images/connect-header-conditions.jpg)

上图中未淡出的块是 Bob 要发送给 Alice 的块头，金色高亮边框的是最后共同块。下面列举了同步会碰到的三种情况：

1.  Bob 的 Best Chain Tip 在 Alice 的 Best Header Chain 中，最后共同块就是 Bob 的 Best Chain Tip，Bob 没有块头可以发送。
2.  Alice 的 Best Header Chain Tip 在 Bob 的 Best Chain 中并且不等于 Tip，最后共同块就是 Alice 的 Best Header Chain Tip。
3.  Alice 的 Best Header Chain 和 Bob 的 Best Chain 出现了分叉，最后共同块是发生发叉前的块。

如果要发送的块很多，需要做分页处理。Bob 先发送第一页，Alice 通过返回结果发现还有更多的块头就继续向 Bob 请求接下来的页。一个简单的分页方案是限制每次返回块头的最大数量，比如 2000。如果返回块头数量等于 2000，说明可能还有块可以返回，就接着请求之后的块头。如果某页最后一个块是 Best Header Chain Tip 或者 Best Chain Tip 的祖先，可以优化成用对应的 Tip 生成 Locator 发送请求，减少收到已有块头的数量。

在同步的同时，Alice 可以观察到 Bob 当前的 Best Chain Tip，即在每轮同步时最后收到的块。如果 Alice 的 Best Header Chain Tip 就是 Bob 的 Best Chain Tip ，因为 Bob 没有块头可发，Alice 就无法观测到 Bob 目前的 Best Chain。所以在每轮连接块头同步的第一个请求时，**应该**从 Best Header Chain Tip 的父块开始构建，而不包含 Tip。

在下面的情况下**必须**做新一轮的连接块头同步。

- 收到对方的新块通知，但是新块的父块状态是 Unknown

连接块头时可能会出现以下一些异常情况：

- Alice 观察到的 Bob Best Chain Tip 很长一段时间没有更新，或者时间很老。这种情况 Bob 无法提供有价值的数据，当连接数达到限制时，可以优先断开该节点的连接。
- Alice 观察到的 Bob Best Chain Tip 状态是 Invalid。这个判断不需要等到一轮 Connect Head 结束，任何一个分页发现有 Invalid 的块就可以停止接受剩下的分页了。因为 Bob 在一个无效的分支上，Alice 可以停止和 Bob 的同步，并将 Bob 加入到黑名单中。
- Alice 收到块头全部都在自己的 Best Header Chain 里，这有两种可能，一是 Bob 故意发送，二是 Alice 在 Connect Head 时 Best Chain 发生了变化，由于无法区分只能忽略，但是可以统计发送的块已经在本地 Best Header Chain 上的比例，高于一定阈值可以将对方加入到黑名单中。

在收到块头消息时可以先做以下格式验证：

- 消息中的块是连续的
- 所有块和第一个块的父块在本地状态树中的状态不是 Invalid
- 第一个块的父块在本地状态树中的状态不是 Unknown，即同步时不处理 Orphan Block。

这一步的验证包括检查块头是否满足共识规则，PoW 是否有效。因为不处理 Orphan Block，难度调整也可以在这里进行验证。

![](images/connect-header-status.jpg)

上图是 Alice 和 Bob, Charlie, Davis, Elsa 等节点同步后的状态树情况和观测到的其它节点的 Best Chain Tip。

如果认为 Unknown 状态块是不在状态树上的话，在连接块头阶段，会在状态树的末端新增一些 Connected 或者 Invalid 状态的节点。所以可以把连接块头看作是拓展状态树，是探路的阶段。

## 下载块

完成连接块头后，一些观测到的邻居节点的 Best Chain Tip 在状态树上的分支是以一个或者多个 Connected 块结尾的，即 Connected Chain，这时可以进入下载块流程，向邻居节点请求完整的块，并进行必要的验证。

因为有了状态树，可以对同步进行规划，避免做无用工作。一个有效的优化就是只有当观测到的邻居节点的 Best Chain 的累积工作量大于本地的 Best Chain 的累积工作量才进行下载块。而且可以按照 Connected Chain 累积工作量为优先级排序，优先下载累积工作量更高的分支，只有被验证为 Invalid 或者因为下载超时无法进行时才去下载优先级较低的分支。

下载某个分支时，因为块的依赖性，应该优先下载更早的块；同时应该从不同的节点去并发下载，充分利用带宽。这可以使用滑动窗口解决。

假设分支第一个要下载的 Connected 状态块号是 M，滑动窗口长度是 N，那么只去下载 M 到 M + N - 1 这 N 个块。在块 M 下载并验证后，窗口往右移动到下一个 Connected 状态的块。如果块 M 验证失败，则分支剩余的块也就都是 Invalid 状态，不需要继续下载。如果窗口长时间没有向右移动，则可以判定为下载超时，可以在尝试其它分支之后再进行尝试，或者该分支上有新增的 Connected 块时再尝试。

![](images/sliding-window.jpg)

上图是一个长度为 8 的滑动窗口的例子。开始时可下载的块是从 3 到 10。块 3 下载后，因为 4 已经先下载好了，所以窗口直接滑动到从 5 开始。

因为通过连接块头已经观测到了邻居节点的 Best Chain，如果在对方 Best Chain 中且对方是一个全节点，可以认为对方是能够提供块的下载的。在下载的时候可以把滑动窗口中的块分成小块的任务加到任务队列中，在能提供下载的节点之间进行任务调度。

下载块如果出现交易对不上 Merkle Hash Root 的情况，或者能对上但是有重复的交易 txid 的情况，并不能说明块是无效，只是没有下载到正确的块内容。可以将对方加入黑名单，但是不能标记块的状态为 Invalid，否则恶意节点可以通过发送错误的块内容来污染节点的状态树。

这一阶段需要验证交易列表和块头匹配，但是不需要做任何依赖祖先块中交易内容的验证，这些验证会放在下一阶段进行。

可以进行的验证比如 Merkel Hash 验证、交易 txid 不能重复、交易列表不能为空、所有交易不能 inputs outputs 同时为空、只有第一个交易可以是 generation transaction 等等。

下载块会把状态树中工作量更高的 Connected Chain 中的 Connected 块变成 Downloaded 或者 Invalid。

## 采用块

在上一阶段中会产生一些以一个或多个 Downloaded 状态的块结尾的链，以下简称为 Downloaded Chain。如果这些链的累积工作量大于 Best Chain Tip， 就可以对这条链进行该阶段完整的合法性验证。如果有多个这样的链，选取累积工作量最高的。

这一阶段需要完成所有剩余的验证，包括所有依赖于历史交易内容的规则。

因为涉及到 UTXO (未消耗掉的交易 outputs) 的索引，这一步的验证开销是非常大的。为了简化系统，可以只保留一套 UTXO 索引，尝试将本地的 Best Chain Tip 进行必要回退，然后将 Downloaded Chain 上的块进行一次验证，再添加到 Best Chain 上。如果中间有块验证失败则 Downloaded Chain 上剩余的块也就都是 Invalid 状态不需要再继续。这时 Best Chain Tip 甚至会低于之前的 Tip，如果遇到可以采取以下的方案处理：

- 如果回退之前的 Best Chain 工作量比当前 Tip 更高，恢复之前的 Best Chain
- 如果有其它 Downloaded Chain 比回退之前的 Best Chain 工作量更高，可以继续使用下一个 Downloaded Chain 进行采用块的步骤。

采用块会将工作量更高的 Downloaded Chain 中的 Downloaded 状态块变成 Accepted 或者 Invalid，而累积工作量最高的 Downloaded Chain 应该成为本地的 Best Chain。

## 新块通知

当节点的 Best Chain Tip 发生变化时，应该通过推送的方式主动去通知邻居节点。为了避免通知重复的块，和尽量一次性发送邻居节点没有的块，可以记录给对方发送过的累积工作量最高的块头 (Best Sent Header)。发送过不但指发送过新块通知，也包括发送过在连接块头时给对方的块头的回复。

因为可以认为对方节点已经知道 Best Sent Header，及其祖先节点，所以发送新块通知时可以排除掉这些块。

![](images/best-sent-header.jpg "Best Sent Header")

上面的例子中标记为 Alice 的块是节点 Alice 的 Best Chain Tip。标记为 Best Sent to Bob 是记录的发送给 Bob 工作量最高的块头。其中未淡化的块是 Alice 需要通知给 Bob 的新块。数字对应的每一步说明如下：

1. 开始时 Alice 只有 Best Chain Tip 需要发送
2. Alice 还没有来得及发送，就又多了一个新块，这时需要发送 Best Chain 最后两个块头
3. Alice 将最后两个块头发送给了 Bob 并同时更新了 Best Sent to Bob
4. Alice 的 Best Chain 发生了分支切换，只需要发送和 Best Sent to Bob 最后共同块之后的块。

基于连接的协商参数和要通知的新块数量：

- 数量为 1 且对方偏好使用 Compact Block [^1]，则使用 Compact Block
- 其它情况直接发送块头列表，但要限制发送块的数量不超过某个阈值，比如 8，如果有 8 个或更多的块要通知，只通知最新的 7 个块。

当收到新块通知时，会出现父块状态是 Unknown 的情况，即 Orphan Block，这个时候需要立即做一轮连接块头的同步。收到 Compact Block 且父块就是本地的 Best Chain Tip 的时候可以尝试用交易池直接恢复，如果恢复成功，直接可以将三阶段的工作合并进行，否则就当作收到的只是块头。

## 同步状态

### 配置

- `GENESIS_HASH`: 创世块哈希
- `MAX_HEADERS_RESULTS`: 一条消息里可以发送块头的最大数量
- `MAX_BLOCKS_TO_ANNOUNCE`: 新块通知数量不可超过该阈值
- `BLOCK_DOWNLOAD_WINDOW`: 下载滑动窗口大小

### 存储

- 块状态树
- Best Chain Tip，决定是否要下载块和采用块。
- Best Header Chain Tip，连接块头时用来构建每轮第一个请求的 Locator

每个连接节点需要单独存储的

- 观测到的对方的 Best Chain Tip
- 上一次发送过的工作量最高的块头哈希 Best Sent Header

## 消息定义

具体消息定义见参考实现，这里只列出同步涉及到的消息和必要的一些字段和描述。

消息的发送是完全异步的，比如发送 `GetHeaders` 并不需要等待对方回复 `SendHeaders` 再发送其它请求，也不需要保证请求和回复的顺序关系，比如节点 A 发送了 `GetHeaders` 和 `GetBlocks` 给 B，B 可以先发送 `SendBlock`，然后再发送 `SendHeaders` 给 A。

Compact Block [^1] 需要使用到的消息会在 Compact Block 相关文档中说明。

### GetHeaders

用于连接块头时向邻居节点请求块头。请求第一页，和收到后续页使用相同的 getheaders 消息，区别是第一页是给本地的 Best Header Chain Tip 的父块生成 Locator，而后续页是使用上一页的最后一个块生成 Locator。

- `hash_stop`: 通知对端构建 `SendHeaders` 时如果处理到指定 hash 的区块应该提前返回。
- `block_locator_hashes`: 对 Chain 上块采样，得到的哈希列表。

### SendHeaders

用于回复 `GetHeaders`。返回块头列表。从 Locator 获得的最后一个共同块开始到 `hash_stop` 或者数量达到 `MAX_BLOCKS_TO_ANNOUNCE`。两个条件满足任意一个就必须停止添加并返回结果。

- `headers`：块头列表

### GetBlocks

用于下载块阶段

- `block_hashes`: 要下载的区块哈希列表

### SendBlock

回复 `GetBlocks` 的块下载请求

- `block`: 请求的块的完整内容

[^1]:	Compact Block 是种压缩传输完整块的技术。它基于在传播新块时，其中的交易应该都已经在对方节点的交易池中。这时只需要包含 交易 txid 列表，和预测对方可能没有的交易的完整信息，接收方就能基于交易池恢复出完整的交易。详细请查阅 [Block and Compact Block Structure](../0020-ckb-consensus-protocol/0020-ckb-consensus-protocol.md#block-and-compact-block-structure) 和 Bitcoin 相关 [BIP](https://github.com/bitcoin/bips/blob/master/bip-0152.mediawiki)。



================================================
File: rfcs/0005-priviledged-mode/0005-priviledged-mode.md
================================================
---
Number: "0005"
Category: Informational
Status: Withdrawn
Author: Xuejie Xiao <xxuejie@gmail.com>
Created: 2018-11-26
---

# Privileged architecture support for CKB VM

## Abstract

This RFC aims to introduce privileged architecture support for CKB VM. While CKB VM doesn't require a privileged model since it only runs one contract at a time, privileged model can help bring MMU support, which can be quite useful in the following cases:

* Implementing sophisticated contracts that require dynamic memory allocation, MMU can be used here to prevent invalid memory access for better security.
* Beginners can leverage MMU to trade some cycles for better security.

Specifically, we plan to add the following features to CKB VM:

* Just enough CSR(control and status register) instructions and VM changes to support a) privilege mode switching and b) page fault function installation.
* A [TLB](https://en.wikipedia.org/wiki/Translation_lookaside_buffer) structure

Notice privileged architecture here is an opt-in feature that is closed by default: while CKB VM will always have this feature, it's up to contract writers to decide if they need it. Contracts optimized purely for minimum cycles should have no problem completely ignoring privileged mode.

## Privileged mode support via CSR instructions

To ensure maximum compatibility, we will use the exact instructions and workflows defined in the [RISC-V spec](https://riscv.org/specifications/privileged-isa/) to implement privilege mode support here:

* First, CSR instructions as defined in RISC-V will be implemented in CKB VM to implement read/write on control and status registers(CSR).
* For simplicity reasons, we might not implement every control and status register as defined in RISC-V spec. For now, we are planning to implement `Supervisor Trap Vector Base Address Register(stvec)` and any other register that might be used in the trap phase. As documented in the spec, reading/writing other registers will result in illegal instruction exception, it's up to contract writer how they want to handle this.
* For now, CKB VM will only use 2 privileged modes: `machine` privileged mode and `user` privileged mode. In machine mode, the contract is free to do anything, in user mode, on the other hand, the operations will be limited.

The trap function installed in `stvec` is nothing but a normal RISC-V function except that it runs with machine privileged mode. As a result, we will also add proper permission checkings to prevent certain operations in user mode, which might include but are not limited to:

* CSR instructions
* Accessing memory pages belonging to machine privileged mode
* Accessing memory pages without correct permissions, for example, it's forbidden to execute a memory page which doesn't have `EXECUTE` permission

Note that when CKB VM first loads, it will be in machine privileged mode, hence contracts that don't need privileged mode support can act as if privileged mode doesn't exist. Contracts that do leverage privileged mode, however, can first setup metadata, then switch to user privileged mode by leveraging RISC-V standard `mret` instruction.

## TLB

To help with MMU, a [Transaction lookaside buffer](https://en.wikipedia.org/wiki/Translation_lookaside_buffer) (TLB) structure will also be included in CKB VM. For simplicity, we will implement a TLB now with the following characteristics:

* The TLB entry will have 64 entries, each entry is 4KB(exactly 1 memory page).
* The TLB implemented will be one-way associative, meaning if 2 memory pages have the same value for the last 6 bits, they will evict each other.
* Whenever we are switching between different privileged levels, the TLB will be fully flushed.

Notice TLB will only be instantiated when CKB VM is generating the first page fault trap, that means if a contract keeps running in machine mode, the contract might never interact with the TLB.

After a TLB is instantiated, there's no way to turn it down in current CKB VM's lifecycle.


================================================
File: rfcs/0005-priviledged-mode/0005-priviledged-mode.zh.md
================================================
---
Number: "0005"
Category: Informational
Status: Withdrawn
Author: Xuejie Xiao <xxuejie@gmail.com>
Created: 2018-11-26
---

# CKB VM 中的特权架构支持

## 概要

本 RFC 的目标是为 CKB VM 添加特权架构支持。虽然由于 CKB VM 每次只运行一个合约，特权模式在 CKB VM 本身的运行中并不需要，但特权模式对添加 MMU 的支持是很有帮助的，MMU 的存在有利于以下几个场景：

* 实现需要动态内存分配的复杂合约时，MMU 可以帮助避免内存越界错误，增加安全性
* MMU 可以帮助初学者在消耗一定 cycle 的情况下增加安全性

具体来说，我们提议为 CKB VM 增加如下部分：

* 为支持特权模式切换功能，以及指定 page fault 函数功能添加刚刚好足够的 CSR(控制与状态寄存器，control and status register) 指令以及 VM 修改
* [TLB](https://en.wikipedia.org/wiki/Translation_lookaside_buffer) 结构

注意这里实现的特权架构是一个默认关闭，可选开启的功能：虽然这个功能在 CKB VM 中一直存在，但是合约设计者可以自由决定是否使用这一功能。为最小 cycle 使用数优化的合约可以完全忽略这一功能。

## 基于 CSR 指令的特权模式支持

为尽最大可能确保兼容性，我们会用 [RISC-V 标准](https://riscv.org/specifications/privileged-isa/) 中定义的指令以及流程来实现特权指令支持：

* 首先，我们会实现 RISC-V 标准中定义的 CSR 指令，用于读写控制与状态寄存器 (CSR)。
* 出于简化实现的考虑，我们不会实现 RISC-V 中定义的每一个控制与状态寄存器。目前为止，我们只计划实现 `Supervisor Trap Vector Base Address Register(stvec)` 以及其他在 trap 阶段会被用到的寄存器。在 CKB VM 中读写其他寄存器会参照 spec 中的定义，抛出违法指令的异常，合约开发者可以自行决定如何处理异常。
* 目前 CKB VM 只用到了两个特权模式级别：`machine` 特权模式以及 `user` 特权模式，在 machine 特权模式中，合约可以自由做任何操作，相应的在 user 特权模式中，合约只可以进行允许的操作。

`stvec` 中指定的 trap 方法 其实就是一个普通的 RISC-V 函数，他与其他普通函数的唯一区别在于它运行在 machine 特权模式上。相对应的，我们也会在 user 特权模式中禁止某些操作，这包括但不限于：

* CSR 指令
* 访问属于 machine 特权级别的内存页
* 用错误的权限访问内存页，如执行没有执行权限内存页上的代码

注意 CKB VM 加载时首先会进入 machine 特权模式，因此不需要特权模式支持的合约可以假装特权模式不存在而继续运行。需要特权模式的合约则可以先进行初始化操作，然后通过 RISC-V 的标准指令 `mret` 切换到 user 特权模式。

## TLB

CKB VM 会添加 [Transaction lookaside buffer](https://en.wikipedia.org/wiki/Translation_lookaside_buffer) (TLB) 结构辅助 MMU 实现。出于简化实现的考虑，我们会实现具有如下特性的 TLB：

* TLB 中有 64 个条目，每个条目为 4KB (即正好一个内存页)
* TLB 为单路组相联，即两个末尾 6 个 bit 相同的内存页会相互竞争一个条目位置
* 切换特权级别时，整个 TLB 会被全部清空

注意 TLB 只会在 CKB VM 第一次生成 page fault trap 操作时才被初始化。这意味着如果一个合约一直在 machine 特权模式下运行的话，该合约可能永远也不会与 TLB 交互。

TLB 成功初始化之后，在当前 CKB VM 运行期间会持续存在，无法在初始化之后关闭 TLB。


================================================
File: rfcs/0006-merkle-tree/0006-merkle-tree.md
================================================
---
Number: "0006"
Category: Standards Track
Status: Active
Author: Ke Wang <k@bll.io>
Created: 2018-12-01
---

# Merkle Tree for Static Data

## Complete Binary Merkle Tree

CKB uses Complete Binary Merkle Tree(CBMT) to generate *Merkle Root*  and *Merkle Proof* for a static list of items. Currently, CBMT is used to calculate *Transactions Root*. Basically, CBMT is a ***complete binary tree***, in which every level, except possibly the last, is completely filled, and all nodes are as far left as possible. And it is also a ***full binary tree***, in which every node other than the leaves has two children. Compare with other Merkle trees, the hash computation of CBMT is minimal, as well as the proof size.

## Nodes Organization

For the sake of illustration, we order the tree nodes from ***top to bottom*** and ***left to right*** starting at zero. In CBMT with *n* items, root is the *first* node, and the first item's hash is *node 0*, second is *node n+1*, etc. We choose this nodes organization because it is easy to calculate the node order for an item.

For example, CBMT with 6 items(suppose the hashes are `[T0, T1, T2, T3, T4, T5]`) and CBMT with 7 items(suppose the hashes are `[T0, T1, T2, T3, T4, T5, T6]`) is shown below:

```
        with 6 items                       with 7 items

              B0 -- node 0                       B0 -- node 0
             /  \                               /  \
           /      \                           /      \
         /          \                       /          \
       /              \                   /              \
      B1 -- node 1    B2 -- node 2       B1 -- node 1    B2 -- node 2
     /  \            /  \               /  \            /  \
    /    \          /    \             /    \          /    \
   /      \        /      \           /      \        /      \
  B3(3)   B4(4)  TO(5)    T1(6)      B3(3)   B4(4)   B5(5)   T0(6)
 /  \    /  \                       /  \    /  \    /  \
T2  T3  T4  T5                     T1  T2  T3  T4  T5  T6
(7) (8) (9) (10)                   (7) (8) (9)(10)(11) (12)
```

Specially, the tree with 0 item is empty(0 node) and its root is `H256::zero`.

## Tree Struct

CBMT can be represented in a very space-efficient way, using an array alone. Nodes in the array are presented in ascending order.

For example, the two trees above can be represented as:

```
// an array with 11 elements, the first element is node 0(BO), second is node 1, etc.
[B0, B1, B2, B3, B4, T0, T1, T2, T3, T4, T5]

// an array with 13 elements, the first element is node 0(BO), second is node 1, etc.
[B0, B1, B2, B3, B4, B5, T0, T1, T2, T3, T4, T5, T6]
```

Suppose a CBMT with *n* items, the size of the array would be *2n-1*, the index of item i(start at 0) is *i+n-1*. For node at *i*, the index of its parent is *(i-1)/2*, the index of its sibling is *(i+1)^1-1*(*^* is xor) and the indexes of its children are *[2i+1, 2i+2]*.

## Merkle Proof

Merkle Proof can provide a proof for existence of one or more items. Only sibling of the nodes along the path that form leaves to root, excluding the nodes already in the path, should be included in the proof. We also specify that ***the nodes in the proof is presented in descending order***(with this, algorithms of proof's generation and verification could be much simple). Indexes of item that need to prove are essential to complete the root calculation, since the index is not the inner feature of item, so the indexes are also included in the proof, and in order to get the correct correspondence, we specify that the indexes are ***presented in ascending order by corresponding hash***. For example, if we want to show that `[T1, T4]` is in the list of 6 items above, only nodes `[T5, T0, B3]` and indexes `[9, 6]` should be included in the proof.

### Proof Structure

The schema of proof struct is:

```
table Proof {
  // indexes of items
  indexes: [uint32];
  // nodes on the path which can not be calculated, in descending order by index
  nodes: [H256];
}
```

### Algorithm of proof generation

```c++
Proof gen_proof(Hash tree[], U32 indexes[]) {
  Hash nodes[];
  U32 tree_indexes[];
  Queue queue;

  int size = len(tree) >> 1 + 1;
  indexes.desending_sort();

  for index in indexes {
    queue.push_back(index + size - 1);
  }

  while(queue is not empty) {
    int index = queue.pop_front();
    int sibling = calculate_sibling(index);

    if(sibling == queue.front()) {
      queue.pop_front();
    } else {
      nodes.push_back(tree[sibling]);
    }

    int parent = calculate_parent(index);
    if(parent != 0) {
      queue.push_back(parent);
    }
  }

  add (size-1) for every index in indexes;
  sort indexes in ascending order by corresponding hash;

  return Proof::new(indexes, nodes);
}
```

### Algorithm of validation

```c++
bool validate_proof(Proof proof, Hash root, Item items[]) {
  Queue queue;
  ascending_sort_by_item_hash(items);

  for (index,item) in (proof.indexes, items) {
    queue.push_back((item.hash(), index));
  }

  descending_sort_by_index(queue);

  int i = 0;
  while(queue is not empty) {
    Hash hash, hash1, hash2;
    int index1, index2;

    (hash1, index1) = queue.pop_front();
    (hash2, index2) = queue.front();
    int sibling = calculate_sibling(index1);

    if(sibling == index2) {
      queue.pop_front();
      hash = merge(hash2, hash1);
    } else {
      hash2 = proof.nodes[i++];

      if(is_left_node(index1)) {
        hash = merge(hash1, hash2);
      } else {
        hash = merge(hash2, hash1);
      }
    }

    int parent = calculate_parent(index);
    if(parent == 0) {
      return root == hash;
    }
    queue.push_back((hash, parent))
  }

  return false;
}
```


================================================
File: rfcs/0006-merkle-tree/0006-merkle-tree.zh.md
================================================
---
Number: "0006"
Category: Standards Track
Status: Active
Author: Ke Wang <k@bll.io>
Created: 2018-12-01
---

# 静态 Merkle Tree

## Complete Binary Merkle Tree

CKB 使用 ***Complete Binary Merkle Tree(CBMT)*** 来为静态数据生成 *Merkle Root* 及 *Merkle Proof*，目前 CBMT 被用于 *Transactions Root* 的计算中。它是一棵完全二叉树，同时也是一棵满二叉树，相比于其它的 Merkle Tree，***Complete Binary Merkle Tree*** 具有最少的 Hash 计算量及最小的 proof size。

## 节点组织形式

规定 CBMT 中节点的排列顺序为从上到下、从左到右（从零开始标号），在一棵由 *n* 个 item 生成的 CBMT 中，下标为 *0* 的节点为 *Merkle Root*，下标为 *n* 的节点为第 *1* 个 item 的 hash，下标 *n+1* 的节点为第 2 个 item 的 hash，以此类推。之所以采用这种排列方式，是因为从 item 的位置很容易计算出其在 CBMT 中节点对应的位置。

举例来说，6 个 item (假设 item 的 Hash 为 `[T0, T1, T2, T3, T4, T5]`)与 7 个 item (假设 item 的 hash 为 `[T0, T1, T2, T3, T4, T5, T6]`)生成的 Tree 的结构如下所示：

```
        with 6 items                       with 7 items

              B0 -- node 0                       B0 -- node 0
             /  \                               /  \
           /      \                           /      \
         /          \                       /          \
       /              \                   /              \
      B1 -- node 1    B2 -- node 2       B1 -- node 1    B2 -- node 2
     /  \            /  \               /  \            /  \
    /    \          /    \             /    \          /    \
   /      \        /      \           /      \        /      \
  B3(3)   B4(4)  TO(5)    T1(6)      B3(3)   B4(4)   B5(5)   T0(6)
 /  \    /  \                       /  \    /  \    /  \
T2  T3  T4  T5                     T1  T2  T3  T4  T5  T6
(7) (8) (9) (10)                   (7) (8) (9)(10)(11) (12)
```

此外，我们规定对于只有 0 个 item 的情况，生成的 tree 只有 0 个 node，其 root 为 `H256::zero`。

## 数据结构

CBMT 可以用一个数组来表示，节点按照升序存放在数组中，上面的两棵 tree 用数组表示分别为：

```
// 11 个元素的数组，数组第一个位置放 node0, 第二个位置放 node1，以此类推。
[B0, B1, B2, B3, B4, T0, T1, T2, T3, T4, T5]
// 13 个元素的数组，数组第一个位置放 node0, 第二个位置放 node1，以此类推。
[B0, B1, B2, B3, B4, B5, T0, T1, T2, T3, T4, T5, T6]
```

在一个由 n 个 item 生成的 CBMT 中，其数组的大小为 *2n-1*，*item i* 在数组中的下标为（下标从 0 开始）*i+n-1*。对于下标为 *i* 的节点，其父节点下标为 *(i-1)/2*，兄弟节点下标为 *(i+1)^1-1*（^为异或），子节点的下标为 *2i+1*、*2i+2*。

## Merkle Proof

Merkle Proof 能为一个或多个 item 提供存在性证明，Proof 中应只包含从叶子节点到根节点路径中无法直接计算出的节点，并且我们规定这些节点按照降序排列，采用降序排列的原因是这与节点的生成顺序相符且 *proof* 的生成及校验算法也会变得非常简单。此外，计算 root 时还需要知道要证明的 item 的 index，因此这些 index 也应包含在 Proof 中，且为了能够使这些 index 能够正确的对应到 item，因此规定这些 index 按对应的 item 的 hash 升序排列，如在 6 个 item 的 Merkle Tree 中为 `[T1, T4]` 生成的 Proof 中应只包含 `[T5, T0, B3]` 和 `[9,6]`。

### Proof 结构

Proof 结构体的 schema 形式为：

```
table Proof {
  // indexes of items
  indexes: [uint32];
  // nodes on the path which can not be calculated, in descending order by index
  nodes: [H256];
}
```

### Proof 生成算法

```c++
Proof gen_proof(Hash tree[], U32 indexes[]) {
  Hash nodes[];
  U32 tree_indexes[];
  Queue queue;

  int size = len(tree) >> 1 + 1;
  indexes.desending_sort();

  for index in indexes {
    queue.push_back(index + size - 1);
  }

  while(queue is not empty) {
    int index = queue.pop_front();
    int sibling = calculate_sibling(index);

    if(sibling == queue.front()) {
      queue.pop_front();
    } else {
      nodes.push_back(tree[sibling]);
    }

    int parent = calculate_parent(index);
    if(parent != 0) {
      queue.push_back(parent);
    }
  }

  add (size-1) for every index in indexes;
  sort indexes in ascending order by corresponding hash;

  return Proof::new(indexes, nodes);
}
```

### Proof 校验算法

```c++
bool validate_proof(Proof proof, Hash root, Item items[]) {
  Queue queue;
  ascending_sort_by_item_hash(items);

  for (index,item) in (proof.indexes, items) {
    queue.push_back((item.hash(), index));
  }

  descending_sort_by_index(queue);

  int i = 0;
  while(queue is not empty) {
    Hash hash, hash1, hash2;
    int index1, index2;

    (hash1, index1) = queue.pop_front();
    (hash2, index2) = queue.front();
    int sibling = calculate_sibling(index1);

    if(sibling == index2) {
      queue.pop_front();
      hash = merge(hash2, hash1);
    } else {
      hash2 = proof.nodes[i++];

      if(is_left_node(index1)) {
        hash = merge(hash1, hash2);
      } else {
        hash = merge(hash2, hash1);
      }
    }

    int parent = calculate_parent(index);
    if(parent == 0) {
      return root == hash;
    }
    queue.push_back((hash, parent))
  }

  return false;
}
```


================================================
File: rfcs/0007-scoring-system-and-network-security/0007-scoring-system-and-network-security.md
================================================
---
Number: "0007"
Category: Standards Track
Status: Withdrawn
Author: Jinyang Jiang <@jjyr>
Created: 2018-10-02
---

# P2P Scoring System And Network Security

## Abstract

This document describes the scoring system of CKB P2P Networking layer and several networking security strategies based on it.


## Motivation

CKB network is designed as an open peer-to-peer network and any node can join the network without permission. This openness, however, also makes it possible for malicious nodes to join and attack the peer-to-peer network.

There were "Eclipse Attack" security issues in both Bitcoin network and Ethereum network, which also designed as the open peer-to-peer network.
The principle of Eclipse Attack is that the attacker would occupy all Peers connection slots of the victim node by manipulating malicious nodes, then filter the victim's view of the blockchain network.

Via "Eclipse Attack" the attacker can take down a victim node with low cost. After that, the attacker could control the victim's mining power for its nefarious purposes, or cheat this victim node to launch a double spent attack.

Reference paper -- [Eclipse Attacks on Bitcoin’s Peer-to-Peer Network][2] 

There are several strategies to prevent "Eclipse attack" introduced in this paper and parts of them have already been implemented in the Bitcoin network. That is to say, this document will describe how to deploy these strategies to CKB network.

In addition, this document also describes the scoring system of CKB P2P Networking layer and we want to handle more generalized network security cases by combining it with more sophisticated security strategies from the Bitcoin network.

Based on the scoring system, we can follow several rules below to handle malicious peers:

1. Nodes should store peers information as much as possible.
2. Nodes need to score Peers' good and bad behavior continuously.
3. Nodes should retain good (high-score) peers and evict bad (low-score) peers out.

CKB client should implement the scoring system and following security strategies.


## Specification

### Terminology

* `Node`
* `Peer` - Other nodes connected through the network
* `PeerInfo` - A data struct used for describing information of `Peer`
* `PeerStore` - A component used to store `PeerInfo`
* `outbound peer` - describe a peer which initiates a connection.
* `inbound peer` - describe a peer which accepts a connection.
* `max_outbound` - Max number of outbound peers.
* `max_inbound` - Max number of inbound peers.
* `network group` - A concept which used when to evict out peers, calculating from the peer's IP address(prefix 16 bits of IPv4 and prefix 32 bits of IPv6).

### Peer Store and Peer Info

PeerStore should be persistent storage and store PeerInfos as more as possible.

PeerInfo should include fields below at least:

```
PeerInfo { 
  NodeId,
  ConnectedIP,
  Direction,  // Inbound or Outbound
  LastConnectedAt, // The time of the last connection 
  Score
}
```

### Scoring System

Parameters below are required in Scoring System:

* `PEER_INIT_SCORE` - the initial score of peers
* `BEHAVIOURS` - a set of peer's possible behaviors, such as: `UNEXPECTED_DISCONNECT`, `TIMEOUT`, `CONNECTED`
* `SCORING_SCHEMA` - describe different scores corresponding to different behaviors, such as: `{"TIMEOUT": -10, "CONNECTED": 10}`
* `BAN_SCORE` - a peer will be banned when its score is lower than this value.

Network layer should provide the scoring interface, allow upper sub-protocols (such as: `sync`, `relay`) to report behaviors of a peer, and update peer's score based on `SCORING_SCHEMA`.

``` ruby
peer.score += SCOREING_SCHEMA[BEHAVIOUR]
```

Peer's behaviors can be distinguished into three categories:

1. Correct behaviors which follow the specification:
    * For example, a node downloads a new block from a peer; a node connects to a peer successfully. Considering a bad peer may pretend like a good one before launching an attack, we should give the peer a relatively low positive score instead of giving a high score at once to encourage the peer to accumulate his credit by performing good behaviors for a long time.
2. Incorrect behaviors which may be caused by network exception:
    * For example, a peer disconnect unexpectedly; a node failed to connect to a peer; ping timeout. Since we can't tell whether these behaviors are intentional bad behavior or caused by the network,  we should give the peer a little negative score to keep tolerant.
3. Incorrect behaviors which violate the protocol:
    * For example, a peer sends an illegal encoded content; a peer sends an invalid block; a peer sends an invalid transaction. We should give a peer a negative score when we can be pretty sure its behavior is malicious, and when a peer's score is lower than `BAN_SCORE`, this peer should be banned.

Examples:

* Peer 1 connected successfully. A node reported this peer's `CONNECTED` behavior and peer 1 got a 10 score rewarded.
* Peer 2 gets a connection timeout. A node reports `TIMEOUT` behavior and peer 2 get a -10 score as punishment.
* Peer 1 sent repetitive `GET_BLOCK` messages. A node reported `DUPLICATED_REQUEST_BLOCK` behavior and peer 1 got a -50 score as punishment.
* Peer 1's score is lower than `BAN_SCORE`, node disconnect with peer 1 then ban the peer.

Parameters like `BEHAVIOURS`, `SCORING_SCHEMA` are not a part of consensus protocol, so CKB client should tune these parameters according to the actual situation of the network.

### Outbound peers selection

The "Eclipse Attack" paper describes a critical security issue during Bitcoin node restarting process:

1. The attacker tries to fit the victim node's addrman(Bitcoin's peer store) with attacker's bad nodes' addresses.
2. The attacker waits the victim node to restart (or use several methods to force it).
3. After the restart, the victim node will select some address from addrman to connect.
4. The attack successes if all outbound connections of the victim node are connected to the attacker's bad nodes.

CKB should avoid this problem when initialize the network.

#### The process of initializing outbound peers

Required parameters:

* `TRY_SCORE` - We only try to connect a peer when its score is higher than this value.
* `ANCHOR_PEERS` - the number of anchor peers should be less than `max_outbound`, such as `2`

Required variables:

* `try_new_outbound_peer` - network component checks this variable to decide whether to connect to extra outbound peers or not.

The process of choosing an outbound peer:

1. Execute step 2 if currently connected outbound peers less than `ANCHOR_PEERS`, otherwise execute step 3.
2. Choose an "anchor peer":
    1. Choose recently connected outbound peers from peer store(can select by `LastConnectedAt` field of peer info).
    2. Execute step 3, if `recent_peers` is empty; otherwise, we choose the peer which has got the highest score from `recent_peers` and return it as the new outbound peer.
3. Choose peer info randomly which must have a higher score than `TRY_SCORE` and have different `network group` from all currently connected outbound peers from PeerStore, return it as the new outbound peer and if we can't find anyone, then execute step 4.
4. Choose peer info randomly from boot nodes.

In step 1, we choose an anchor peer if the node has zero or only a few connected outbound peers. This behavior refers to "Anchor Connection" strategy which described in the [Eclipse Attack][2] paper.

Pseudocode:

``` ruby
# return our new outbound peer
def find_outbound_peer
  connected_outbound_peers = connected_peers.select{|peer| peer.outbound? && !peer.feeler? }
  # step 1
  if connected_outbound_peers.length < ANCHOR_PEERS
    find_anchor_peer() || find_random_peer() || random_boot_node()
  else
    find_random_peer() || random_boot_node()
  end
end

# step 2
def find_anchor_peer
  last_connected_peers = peer_store.sort_by{|peer| -peer.last_connected_at}.take(max_outbound)
  # return the higest scored peer info
  last_connected_peers.sort_by(&:score).last
end

# step 3
def find_random_peer
  connected_outbound_peers = connected_peers.select{|peer| peer.outbound? && !peer.feeler? }
  exists_network_groups = connected_outbound_peers.map(&:network_group)
  candidate_peers = peer_store.select do |peer| 
    peer.score >= TRY_SCORE && !exists_network_groups.include?(peer.network_group)
  end
  candidate_peers.sample
end

# step 4
def random_boot_node
  boot_nodes.sample
end
```

The node should repeat this process until the number of connected outbound peers is equal to or greater than  `max_outbound` and `try_new_outbound_peer` is `false`.

``` ruby
check_outbound_peers_interval = 15
# continually check the number of outbound peers
loop do
  sleep(check_outbound_peers_interval)
  connected_outbound_peers = connected_peers.select{|peer| peer.outbound? && !peer.feeler? }
  if connected_outbound_peers.length >= max_outbound && !try_new_outbound_peer 
    next
  end
  new_outbound_peer = find_outbound_peer()
  connect_peer(new_outbound_peer)
end
```

`try_new_outbound_peer` variable is used for some situation where a node can't get any useful messages in a duration time. Then we will set `try_new_outbound_peer` to `true` and allow the node to connect to more extra outbound peers. This strategy would be introduced later.

Under this strategy, the attacker must achieve the following conditions to apply an eclipse attack:

1. The attacker needs to have `n` malicious peers (`n == ANCHOR_PEERS`) to be the victim node's outbound peers and these peers must have the highest scores.
2. The attacker needs to prepare at least `max_outbound - ANCHOR_PEERS` bad peers' addresses in PeerStore. At the same time, the attacker must make sure that the randomly selected `max_outbound - ANCHOR_PEERS` outbound peers are all camouflage nodes of the attacker.


#### Extra outbound peers and eviction

Network component should check the main protocol (for example: `sync` protocol in CKB) status every few minutes.

``` ruby
def sync_maybe_stale
  now = Time.now
  # use block product time to detect network status
  # we consider network maybe stale if block not produced within a predicted time
  last_tip_updated_at < now - block_produce_interval * n
end
```

The network component should set `try_new_outbound_peer` to `true` when `sync` protocol doesn't work and set back to `false` when `sync` protocol puts back.

``` ruby
check_sync_stale_at = Time.now
loop_interval = 30
check_sync_stale_interval = 15 * 60 # 15 minutes

loop do
  sleep(loop_interval)
  # try evict
  evict_extra_outbound_peers()
  now = Time.now
  if check_sync_stale_at >= now
    # update try_new_outbound_peer
    set_try_new_outbound_peer(sync_maybe_stale())
    check_sync_stale_at = now + check_sync_stale_interval
  end
end
```

CKB network will try to connect to extra outbound peers continually when `try_new_outbound_peer` is `true`, and try to evict useless extra peers every few minutes to prevent too many connections.

``` ruby
# eviction logic
def evict_extra_outbound_peers
  connected_outbound_peers = connected_peers.select{|peer| peer.outbound? && !peer.feeler? }
  if connected_outbound_peers.length <= max_outbound
    return
  end
  now = Time.now
  # here use last_block_anoncement_at to evict peers, we assume the oldest one is useless for us
  evict_target = connected_outbound_peers.sort_by do |peer|
    peer.last_block_announcement_at
  end.first
  if evict_target
    if now - evict_target.last_connected_at > MINIMUM_CONNECT_TIME && !is_downloading?(evict_target)
      disconnect_peer(evict_target)
      # prevent connect to too many peers
      set_try_new_outbound_peer(false)
    end
  end
end
```

### The process of accepting inbound peers

In Bitcoin, a node will try to evict connected inbound peers if the number of connected inbound peers reaches `max_inbound` and another new inbound connection tries to connect. (check [Bitcoin source code][1] for details)

This eviction behavior is intended to keep high-quality peers and evict low-quality peers.

CKB refers to Bitcoin's eviction test and steps are as follows:

1. Consider currently connected inbound peers as `candidate_peers`.
2. Protect peers(`N` represent the number of peers to protect in each step):
    1. Delete `N` peers from `candidate_peers` which has the highest score.
    2. Delete `N` peers from `candidate_peers` which has the lowest ping.
    3. Delete `N` peers from `candidate_peers` which most recently sent us messages.
    4. Delete `candidate_peers.size / 2` peers from `candidate_peers` which have the longest connection time.
3. Group `candidate_peers` according to `network group` field.
4. Find out the group which contains the most peers.
5. Evict the lowest scored peer from the group found in step 4 if it is not empty. Otherwise, reject the connection from the new peer.

We protect some peers from eviction based on characteristics that an attacker is hard to simulate or manipulate, to enhence the security of the network.

### Feeler Connection

Feeler Connection is intended to test a peer is connectable or not.

Node will start a feeler connection every few minutes after outbound peers reach `max_outbound` limit.

1. Pick out peer info from PeerStore randomly which we never connected to
2. Connect to this peer
3. Run handshake protocol
4. Disconnect

Feeler peers would be assumed to disconnect soon.

### Delete peer info from PeerStore

Required parameters:

* `PEER_STORE_LIMIT` - max number of PeerInfo in PeerStore
* `PEER_NOT_SEEN_TIMEOUT` - used for protecting peers which recently connected. Only peer info over `last_connected_to` would be deleted. 

When the number of peer info reaches `PEER_STORE_LIMIT`:

1. Group all PeerInfos in PeerStore according to `network group` field
2. Find out the group which contains the most peer infos
3. Search peers have not been connected recently from this group: `peer.last_connected_at < Time.now - PEER_NOT_SEEN_TIMEOUT`
4. Find out the lowest scored peer info as `candidate_peer_info`
5. if `candidate_peer_info.score < new_peer_info.score` then we delete `candidate_peer_info` and add `new_peer_info`, otherwise we do not accept `new_peer_info`

## References

1. [Bitcoin source code][1]
2. [Eclipse Attacks on Bitcoin’s Peer-to-Peer Network][2]

[1]: https://github.com/bitcoin/bitcoin
[2]: https://eprint.iacr.org/2015/263.pdf



================================================
File: rfcs/0007-scoring-system-and-network-security/0007-scoring-system-and-network-security.zh.md
================================================
---
Number: "0007"
Category: Standards Track
Status: Withdrawn
Author: Jinyang Jiang <@jjyr>
Created: 2018-10-02
---

# P2P 评分系统和网络安全

## 简介

本篇 RFC 描述了 CKB P2P 网络层的评分系统，以及基于评分的网络安全策略。


## 目标

CKB 网络被设计为开放的 P2P 网络，任何节点都能无需许可的加入网络，但网络的开放性同时使得恶意节点也能够加入并对 P2P 网络进行攻击。

同样采用开放性 P2P 网络的比特币和以太坊中都曾有「日蚀攻击」的安全问题。
日蚀攻击的原理是攻击者通过操纵恶意节点占领受害者节点所有的 Peers 连接，以此控制受害者节点可见的网络。

攻击者可以用极少成本实施日蚀攻击，攻击成功后可以操纵受害节点的算力做些恶意行为, 或欺骗受害节点进行双花交易。

参考论文 -- [Eclipse Attacks on Bitcoin’s Peer-to-Peer Network][2]

论文中同时提出了几种防范手段, 其中部分已经在比特币主网应用，
本 RFC 参考比特币网络的实现，描述如何在 CKB 网络中正确应用这些措施。

RFC 同时描述了 CKB P2P 网络的评分机制，
结合 CKB 的评分机制，可以使用比特币中成熟的安全措施来处理更加通用的攻击场景。

基于 CKB 的评分机制，我们遵循几条规则来处理恶意 Peers：

1. 节点应尽可能的存储已知的 Peers 信息
2. 节点需要不断对 Peer 的好行为和坏行为进行评分
3. 节点应保留好的(分数高的) Peer，驱逐坏的(分数低) Peer

RFC 描述了客户端应该实现的打分系统和下文的几种安全策略。


## Specification

### 术语

* `Node` - 节点
* `Peer` - 网络上的其他节点
* `PeerInfo` - 描述 Peer 信息的数据结构
* `PeerStore` - 用于存储 PeerInfo 的组件
* `outbound peer` - 主动发起连接的节点
* `inbound peer` - 被动接受连接的节点
* `max_outbound` - 节点主动连接的 Peers 上限
* `max_inbound` - 节点被动接受的 Peers 上限
* `network group` - 驱逐节点时用到的概念，对 Peer 连接时的 IP 计算，IPv4 取前 16 位，Ipv6 取前 32 位


### PeerStore 和 PeerInfo

PeerStore 应该做到持久化存储, 并尽可能多的储存已知的 PeerInfo

PeerInfo 至少包含以下内容

```
PeerInfo { 
  NodeId, // Peer 的 NodeId
  ConnectedIP,  // 连接时的 IP
  Direction,  // Inbound or Outbound
  LastConnectedAt, // 最后一次连接的时间
  Score // 分数
}
```

### 评分系统

评分系统需要以下参数

* `PEER_INIT_SCORE` - Peers 的初始分数
* `BEHAVIOURS` - 节点的行为, 如 `UNEXPECTED_DISCONNECT`, `TIMEOUT`, `CONNECTED` 等
* `SCORING_SCHEMA` - 描述不同行为对应的分数, 如 `{"TIMEOUT": -10, "CONNECTED": 10}`
* `BAN_SCORE` - Peer 评分低于此值时会被加入黑名单

网络层应该提供评分接口，允许 `sync`, `relay` 等上层子协议报告 peer 行为，
并根据 peer 行为和 `SCORING_SCHEMA` 调整 peer 的评分。

``` ruby
peer.score += SCOREING_SCHEMA[BEHAVIOUR]
```

Peer 的评分是 CKB P2P 网络安全的重要部分，peer 的行为可以分为如下三种：

1. 符合协议的行为:
    * 如: 从 peer 获取了新的 block、节点成功连接上 peer 。 当 peer 作出符合协议的行为时，节点应上调对 peer 评分，
考虑恶意 Peer 有可能在攻击前进行伪装，
对好行为奖励的分数不应一次性奖励太多，
而是鼓励 peer 长期进行好的行为来积累信用。

2. 可能由于网络异常导致的行为:
    * 如: peer 异常断开、连接 peer 失败、ping timeout。
对这些行为我们采用宽容性的惩罚，下调对 peer 的评分，但不会一次性下调太多。

3. 明显违反协议的行为:
    * 如: peer 发送无法解码的内容、peer 发送 invalid block, peer 发送 invalid transaction。
当我们可以确定 peer 存在明显的恶意行为时，对 peer 打低分，如果 peer 评分低于 `BAN_SCORE` ，将 peer 加入黑名单并禁止连接。

例子:
* peer 1 连接成功，节点报告 peer1 `CONNECTED` 行为，peer 1 加 10 分
* peer 2 连接超时，节点报告 peer2 `TIMEOUT` 行为，peer 2 减 10 分
* peer 1 通过 `sync` 协议发送重复的请求，节点报告 peer 1 `DUPLICATED_REQUEST_BLOCK` 行为，peer 1 减 50 分
* peer 1 被扣分直至低于 `BAN_SCORE`, 被断开连接并加入黑名单

`BEHAVIOURS`、 `SCORING_SCHEMA` 等参数不属于共识协议的一部分，CKB 实现应该根据网络实际的情况对参数调整。


### 节点 outbound peers 的选择策略

[日蚀攻击论文][2]中提到了比特币节点重启时的安全问题：

1. 攻击者事先利用比特币的节点发现规则填充受害节点的地址列表
2. 攻击者等待或诱发受害者节点重启
3. 重启后，受害者节点会从 addrman (类似 peer store) 中选择一些地址连接
3. 受害节点的所有对外的连接都连接到了恶意 peers 则攻击者攻击成功

CKB 在初始化网络时应该避免这些问题

#### Outbound peers 连接流程

参数说明: 
* `TRY_SCORE` - 设置一个分数，仅当 PeerInfo 分数高于 `TRY_SCORE` 时节点才会去尝试连接
* `ANCHOR_PEERS` - 锚点 peer 的数量，值应该小于 `max_outbound` 如 `2`

变量:
* `try_new_outbound_peer` - 设置节点是否该继续发起新的 Outbound 连接

选择一个 outbound peer 的流程:

1. 如果当前连接的 outbound peers 小于 `ANCHOR_PEERS` 执行 2， 否则执行 3
2. 选择一个锚点 peer:
    1. 从 PeerStore 挑选最后连接过的 `max_bound` 个 outbound peers 作为 `recent_peers`
    2. 如果 `recent_peers` 为空则执行 3，否则从 `recent_peers` 中选择分数最高的节点作为 outbound peer 返回
3. 在 PeerStore 中随机选择一个分数大于 `TRY_SCORE` 且 `NetworkGroup` 和当前连接的 outbound peers 都不相同的 peer info，如果找不到这样的 peer info 则执行 5，否则将这个 peer info 返回
4. 从 `boot_nodes` 中随机选择一个返回

伪代码

``` ruby
# 找到一个 outbound peer 候选
def find_outbound_peer
  connected_outbound_peers = connected_peers.select{|peer| peer.outbound? && !peer.feeler? }
  if connected_outbound_peers.length < ANCHOR_PEERS
    find_anchor_peer() || find_random_peer() || random_boot_node()
  else
    find_random_peer() || random_boot_node()
  end
end

def find_anchor_peer
  last_connected_peers = peer_store.sort_by{|peer| -peer.last_connected_at}.take(max_bound)
  # 返回最高分的 peer info
  last_connected_peers.sort_by(&:score).last
end

def find_random_peer
  connected_outbound_peers = connected_peers.select{|peer| peer.outbound? && !peer.feeler? }
  exists_network_groups = connected_outbound_peers.map(&:network_group)
  candidate_peers = peer_store.select do |peer| 
    peer.score >= TRY_SCORE && !exists_network_groups.include?(peer.network_group)
  end
  candidate_peers.sample
end

def random_boot_node
  boot_nodes.sample
end
```


节点应该重复以上过程，直到节点正在连接的 outbound peers 数量大于等于 `max_outbound` 并且 `try_new_outbound_peer` 为 `false`。

``` ruby
check_outbound_peers_interval = 15
# 每隔几分钟检查 outbound peers 数量
loop do
  sleep(check_outbound_peers_interval)
  connected_outbound_peers = connected_peers.select{|peer| peer.outbound? && !peer.feeler? }
  if connected_outbound_peers.length >= max_outbound && !try_new_outbound_peer 
    next
  end
  new_outbound_peer = find_outbound_peer()
  connect_peer(new_outbound_peer)
end
```

`try_new_outbound_peer` 的作用是在一定时间内无法发现有效消息时，允许节点连接更多的 outbound peers，这个机制在后文介绍。

该策略在节点没有 Peers 时会强制从最近连接过的 outbound peers 中选择，这个行为参考了[日蚀攻击论文][2]中的 Anchor Connection 策略。

攻击者需要做到以下条件才可以成功实施日蚀攻击

1. 攻击者有 `n` 个伪装节点(`n == ANCHOR_PEERS`) 成为受害者节点的 outbound peers，这些伪装节点同时要拥有最高得分
2. 攻击者需要准备至少 `max_outbound - ANCHOR_PEERS` 个伪装节点地址在受害者节点的 PeerStore，并且受害者节点的随机挑选的 `max_outbound - ANCHOR_PEERS` 个 outbound peers 全部是攻击者的伪装节点。

#### 额外的 outbound peers 连接和驱逐

网络组件应该每隔几分钟检测子协议中的主要协议如 `sync` 协议是否工作

``` ruby
def sync_maybe_stale
  now = Time.now
  # 可以通过上次 Tip 更新时间，出块间隔和当前时间判断 sync 是否正常工作
  last_tip_updated_at < now - block_produce_interval * n
end
```

当我们发现 `sync` 协议无法正常工作时，应该设置 `try_new_outbound_peer` 变量为 `true`，当发现 `sync` 协议恢复正常时设置 `try_new_outbound_peer` 为 `false`

``` ruby
check_sync_stale_at = Time.now
loop_interval = 30
check_sync_stale_interval = 15 * 60 #(15 minutes)

loop do
  sleep(loop_interval)
  # try evict
  evict_extra_outbound_peers()
  now = Time.now
  if check_sync_stale_at >= now
    set_try_new_outbound_peer(sync_maybe_stale())
    check_sync_stale_at = now + check_sync_stale_interval
  end
end
```

当 `try_new_outbound_peer` 为 `true` 时 CKB 网络将会持续的尝试连接额外的 outbound peers，并每隔几分钟尝试逐出没有用的额外 outbound peers，这个行为防止节点有过多的连接。

``` ruby
def evict_extra_outbound_peers
  connected_outbound_peers = connected_peers.select{|peer| peer.outbound? && !peer.feeler? }
  if connected_outbound_peers.length <= max_outbound
    return
  end
  now = Time.now
  # 找出连接的 outbound peers 中 last_block_announcement_at 最老的 peer
  evict_target = connected_outbound_peers.sort_by do |peer|
    peer.last_block_announcement_at
  end.first
  if evict_target
    # 至少连接上这个 peer 一段时间，且当前没有从这个 peer 下载块
    if now - evict_target.last_connected_at > MINIMUM_CONNECT_TIME && !is_downloading?(evict_target)
      disconnect_peer(evict_target)
      # 防止连接过多的 outbound peer
      set_try_new_outbound_peer(false)
    end
  end
end
```


### 节点 inbound peers 接受机制

比特币中当节点的被动 peers 连满同时又有新 peer 尝试连接时，节点会对已有 peers 进行驱逐测试(详细请参考 [Bitcoin 源码][1])。

驱逐测试的目的在于节点保留高质量 peer 的同时，驱逐低质量的 peer。

CKB 参考了比特币的驱逐测试，步骤如下:

1. 找出当前连接的所有 inbound peers 作为 `candidate_peers`
2. 保护 peers (`N` 代表每一步中我们想要保护的 peers 数量):
    1. 从 `candidate_peers` 找出 `N` 个分数最高的 peers 删除
    2. 从 `candidate_peers` 找出 `N` 个 ping 最小的 peers 删除
    3. 从 `candidate_peers` 找出 `N` 个最近发送消息给我们的 peers 删除
    4. 从 `candidate_peers` 找出 `candidate_peers.size / 2` 个连接时间最久的 peers 删除
3. 按照 `network group` 对剩余的 `candidate_peers` 分组
4. 找出包含最多 peers 的组
5. 驱逐组中分数最低的 peer，找不到 peer 驱逐时则拒绝新 peer 的连接

我们基于攻击者难以模拟或操纵的特征来保护一些 peers 免受驱逐，以增强网络的安全性。

### Feeler Connection

Feeler Connection 机制的目的在于测试 Peer 是否可以连接。

当节点的 outbound peers 数量达到 `max_outbound` 限制时，
节点会每隔一段时间(一般是几分钟)主动发起 feeler connection：

1. 从 PeerStore 中随机选出一个未连接过的 peer info
2. 连接该 peer
3. 执行握手协议
4. 断开连接

Feeler peer 会被假设为很快断开连接

### PeerStore 清理

设置一些参数：
`PEER_STORE_LIMIT` - PeerStore 最多可以存储的 PeerInfo 数量
`PEER_NOT_SEEN_TIMEOUT` - 用于判断 peer info 是否该被清理，如该值设为 15 天，则表示最近 15 天内连接过的 peer 不会被清理

PeerStore 中存储的 PeerInfo 数量达到 `PEER_STORE_LIMIT` 时需要清理，过程如下：

1. 按照 `network group` 给 PeerStore 中的 PeerInfo 分组
2. 找出包含最多节点的组
3. 在组中搜索最近没有连接过的 peers `peer.last_connected_at < Time.now - PEER_NOT_SEEN_TIMEOUT`
4. 在该集合中找到分数最低的 PeerInfo `candidate_peer_info`
5. 如果 `candidate_peer_info.score < new_peer_info.score` 则删掉 `candidate_peer_info` 并插入 `new_peer_info`，否则不接受 `new_peer_info`


## 参考

1. [Bitcoin source code][1]
2. [Eclipse Attacks on Bitcoin’s Peer-to-Peer Network][2]

[1]: https://github.com/bitcoin/bitcoin
[2]: https://eprint.iacr.org/2015/263.pdf


================================================
File: rfcs/0008-serialization/0008-serialization.md
================================================
---
Number: "0008"
Category: Standards Track
Status: Active
Author: Boyu Yang <yangby@cryptape.com>
Created: 2018-12-17
---

# Serialization

CKB uses two major serialization formats, [Molecule][molecule] and [JSON][json].

[Molecule][molecule] is a canonicalization and zero-copy serialization format.

[JSON][json] is used in node RPC service via [JSON-RPC][jsonrpc].

## Molecule

### Summary

#### Fixed Size or Dynamic Size

| Type | byte  | array | struct | vector  |  table  | option  |  union  |
|------|-------|-------|--------|---------|---------|---------|---------|
| Size | Fixed | Fixed | Fixed  | Dynamic | Dynamic | Dynamic | Dynamic |

#### Memory Layout

```
|  Type  |                      Header                      |               Body                |
|--------+--------------------------------------------------+-----------------------------------|
| array  |                                                  |  item-0 |  item-1 | ... |  item-N |
| struct |                                                  | field-0 | field-1 | ... | field-N |
| fixvec | items-count                                      |  item-0 |  item-1 | ... |  item-N |
| dynvec | full-size | offset-0 | offset-1 | ... | offset-N |  item-0 |  item-1 | ... |  item-N |
| table  | full-size | offset-0 | offset-1 | ... | offset-N | filed-0 | field-1 | ... | field-N |
| option |                                                  | item or none (zero bytes)         |
| union  | item-type-id                                     | item                              |
```

- All items in Header are 32 bit unsigned integers in little-endian.

### Primitive Type

#### `byte`

The `byte` is a byte.

##### Examples

`00` is a `byte`.

### Composite Types

#### `array`

The `array` is a fixed-size type: it has a fixed-size inner type and a fixed length.
The size of an `array` is the size of inner type times the length.

Serializing an `array` only need to serialize all items in it.

There's no overhead to serialize an `array`, which stores all items consecutively, without extra space between two adjacent items.

##### Examples

If we define `array Byte3 [byte; 3];`, and we want to store three bytes: first is `01`, the second is `02` and the last is `03`, then the serialized bytes will be `01 02 03`.

If we define `array Uint32 [byte; 4];` , and we want to store a 32 bit unsigned integer `0x01020304` into it in little-endian, then the serialized bytes will be `04 03 02 01`.

If we define `array TwoUint32 [Uint32; 2];`, and we want to store two 32 bit unsigned integers in little-endian: first is `0x01020304` and second is `0xabcde`, then the serialized bytes will be `04 03 02 01 de bc 0a 00`.

#### `struct`

The `struct` is a fixed-size type: all fields in `struct` are fixed-size and it has a fixed quantity of fields.
The size of a `struct` is the sum of all fields' size.

Serializing a `struct` only need to serialize all fields in it.
Fields in a `struct` are stored in the order they are declared.

There's no overhead to serialize a `struct`, which stores all fields consecutively, without extra space between two adjacent items.

##### Examples

If we define `struct OnlyAByte { f1: byte }`, and we want to store a byte `ab`, then the serialized bytes will be `ab`.

If we define `struct ByteAndUint32 { f1: byte, f2: Uint32 }`, and we want to store a byte `ab` and a 32 bit unsigned integer `0x010203` in little-endian, then the serialized bytes will be `ab 03 02 01 00`.

#### vectors

There are two kinds of vectors: fixed vector `fixvec` and dynamic vector `dynvec`.

Whether a vector is fixed or dynamic depends on the type of its inner item: if the inner item is fixed-size, then it's a `fixvec`; otherwise, it's a `dynvec`.

Both of `fixvec` and `dynvec` are dynamic-size types.

##### `fixvec` - fixed vector

There are two steps of serializing a `fixvec`:
1. Serialize the length as a 32 bit unsigned integer in little-endian.
2. Serialize all items in it.

###### Examples

If we define `vector Bytes <byte>;`:
- the serialized bytes of an empty `Bytes` is `00 00 00 00`(the length of any empty fixed vector is `0`).
- the serialized bytes of `0x12` is `01 00 00 00, 12`.
- the serialized bytes of `0x1234567890abcdef` is `08 00 00 00, 12 34 56 78 90 ab cd ef`.

If we define `vector Uint32Vec <Uint32>;`:
- the serialized bytes of an empty `Uint32Vec` is `00 00 00 00`.
- the serialized bytes of `0x123` is `01 00 00 00, 23 01 00 00`.
- the serialized bytes of `[0x123, 0x456, 0x7890, 0xa, 0xbc, 0xdef]` is
  ```
  # there are 6 items
  06 00 00 00
  # six items
  23 01 00 00, 56 04 00 00, 90 78 00 00, 0a 00 00 00, bc 00 00 00, ef 0d 00 00
  ```

##### `dynvec` - dynamic vector

There are three steps of serializing a `dynvec`:
1. Serialize the full size in bytes as a 32 bit unsigned integer in little-endian.
2. Serialize all offset of items as 32 bit unsigned integer in little-endian.
3. Serialize all items in it.

###### Examples

If we define `vector BytesVec <Bytes>;`:
- the serialized bytes of an empty `BytesVec`  is `04 00 00 00`(the full size of an empty dynamic vector is 4 bytes).
- the serialized bytes of `[0x1234]` is
  ```
  # the full size is 14 bytes
  0e 00 00 00
  # one offset
  08 00 00 00
  # one item
  02 00 00 00 12 34
  ```
- the serialized bytes of `[0x1234, 0x, 0x567, 0x89, 0xabcdef]` is
  ```
  # the full size is 52 (0x34) bytes
  34 00 00 00
  # five offsets (20 bytes in total)
  18 00 00 00, 1e 00 00 00, 22 00 00 00, 28 00 00 00, 2d 00 00 00
  # five items (28 bytes in total)
  02 00 00 00, 12 34
  00 00 00 00,
  02 00 00 00, 05 67
  01 00 00 00, 89
  03 00 00 00, ab cd ef
  ```

#### `table`

The `table` is a dynamic-size type. It can be considered as a `dynvec` but the length is fixed.

The serializing steps are same as `dynvec`:
1. Serialize the full size in bytes as a 32 bit unsigned integer in little-endian.
2. Serialize all offset of fields as 32 bit unsigned integer in little-endian.
3. Serialize all fields in it in the order they are declared.

##### Examples

If we define `table MixedType { f1: Bytes, f2: byte, f3: Uint32, f4: Byte3, f5: Bytes }`
- the serialized bytes of a `MixedType { f1: 0x, f2: 0xab, f3: 0x123, f4: 0x456789, f5: 0xabcdef }`  is
  ```
  # the full size is 43 (0x2b) bytes
  2b 00 00 00
  # five offsets (20 bytes in total)
  18 00 00 00, 1c 00 00 00, 1d 00 00 00, 21 00 00 00, 24 00 00 00
  # five items (19 bytes in total)
  00 00 00 00
  ab
  23 01 00 00
  45 67 89
  03 00 00 00, ab cd ef
  ```

#### `option`

The `option` is a dynamic-size type.

Serializing an `option` depends on whether it is empty or not:
- if it's empty, there is **zero** bytes (the size is `0`).
- if it's not empty, just serialize the inner item (the size is same as the inner item's size).

##### Examples

If we define `option BytesVecOpt (BytesVec);`
- the serialized bytes of `None` is ` ` (empty).
- the serialized bytes of `Some([])` is `04 00 00 00`.
- the serialized bytes of `Some([0x])` is
  ```
  # the full size of BytesVec is 12 bytes
  0c 00 00 00
  # the offset of Bytes
  08 00 00 00
  # the length of Bytes
  00 00 00 00
  ```

#### `union`

The `union` is a dynamic-size type.

Serializing a `union` has two steps:
- Serialize a item type id in bytes as a 32 bit unsigned integer in little-endian.
  The item type id is the index of the inner items, and it's starting at 0.
- Serialize the inner item.

#### Examples

If we define `union HybridBytes { Byte3, Bytes, BytesVec, BytesVecOpt }`
- the serialized bytes of `Byte3 (0x123456)` is `00 00 00 00, 12 34 56`
- the serialized bytes of `Bytes (0x)` is `01 00 00 00, 00 00 00 00`
- the serialized bytes of `Bytes (0x123)` is `01 00 00 00, 02 00 00 00, 01 23`
- the serialized bytes of `BytesVec ([])` is `02 00 00 00, 04 00 00 00`
- the serialized bytes of `BytesVec ([0x])` is `02 00 00 00, 0c 00 00 00, 08 00 00 00, 00 00 00 00`
- the serialized bytes of `BytesVec ([0x123])` is `02 00 00 00, 0e 00 00 00, 08 00 00 00, 02 00 00 00, 01 23`
- the serialized bytes of `BytesVec ([0x123, 0x456])` is
  ```
  # Item Type Id
  02 00 00 00
  # the full size of BytesVec is 24 bytes
  18 00 00 00
  # two offsets of BytesVec (8 bytes in total)
  0c 00 00 00, 12 00 00 00,
  # two Bytes (12 bytes in total)
  02 00 00 00, 01 23
  02 00 00 00, 04 56
  ```
- the serialized bytes of `BytesVecOpt (None)` is `03 00 00 00`
- the serialized bytes of `BytesVecOpt (Some(([])))` is `03 00 00 00, 04 00 00 00`
- the serialized bytes of `BytesVecOpt (Some(([0x])))` is `03 00 00 00, 0c 00 00 00, 08 00 00 00, 00 00 00 00`
- the serialized bytes of `BytesVecOpt (Some(([0x123])))` is `03 00 00 00, 0e 00 00 00, 08 00 00 00, 02 00 00 00, 01 23`
- the serialized bytes of `BytesVecOpt (Some(([0x123, 0x456])))` is
  ```
  # Item Type Id
  03 00 00 00
  # the full size of BytesVec is 24 bytes
  18 00 00 00
  # two offsets of BytesVec (8 bytes in total)
  0c 00 00 00, 12 00 00 00,
  # two Bytes (12 bytes in total)
  02 00 00 00, 01 23
  02 00 00 00, 04 56
  ```

[molecule]: #molecule
[json]: https://www.json.org
[jsonrpc]: https://www.jsonrpc.org/specification


================================================
File: rfcs/0009-vm-syscalls/0009-vm-syscalls.md
================================================
---
Number: "0009"
Category: Standards Track
Status: Active
Author: Xuejie Xiao <xxuejie@gmail.com>
Created: 2018-12-14
---

# VM Syscalls

## Abstract

This document describes all the RISC-V VM syscalls implemented in CKB Lina. Note that 3 new syscalls have been added to ckb2021 [2].

## Introduction

CKB VM syscalls are used to implement communications between the RISC-V based CKB VM, and the main CKB process, allowing scripts running in the VM to read current transaction information as well as general blockchain information from CKB. Leveraging syscalls instead of custom instructions allow us to maintain a standard compliant RISC-V implementation which can embrace the broadest industrial support.

## Partial Loading

With the exception of `Exit`, all syscalls included here use a partial loading design. The following 3 arguments are used in each syscall:

* `addr`: a pointer to a buffer in VM memory space denoting where we would load the syscall data.
* `len`: a pointer to a 64-bit unsigned integer in VM memory space, when calling the syscall, this memory location should store the length of the buffer specified by `addr`, when returning from the syscall, CKB VM would fill in `len` with the actual length of the buffer. We would explain the exact logic below.
* `offset`: an offset specifying from which offset we should start loading the syscall data.

Each syscall might have different ways of preparing syscall return data, when the data is successfully prepared, it is fed into VM via the steps below. For ease of reference, we refer to the prepared syscall return data as `data`, and the length of `data` as `data_length`.

1. A memory read operation is executed to read the value in `len` pointer from VM memory space, we call the read result `size` here.
2. `full_size` is calculated as `data_length - offset`.
3. `real_size` is calculated as the minimal value of `size` and `full_size`
4. The serialized value starting from `&data[offset]` till `&data[offset + real_size]` is written into VM memory space location starting from `addr`.
5. `full_size` is written into `len` pointer
6. `0` is returned from the syscall denoting execution success.

The whole point of this process, is providing VM side a way to do partial reading when the available memory is not enough to support reading the whole data altogether.

One trick here, is that by providing `NULL` as `addr`, and a `uint64_t` pointer with 0 value as `len`, this syscall can be used to fetch the length of the serialized data part without reading any actual data.

## Syscall Specifications

In CKB we use RISC-V's standard syscall solution: each syscall accepts 6 arguments stored in register `A0` through `A5`. Each argument here is of register word size so it can store either regular integers or pointers. The syscall number is stored in `A7`. After all the arguments and syscall number are set, `ecall` instruction is used to trigger syscall execution, CKB VM then transfers controls from the VM to the actual syscall implementation beneath. For example, the following RISC-V assembly would trigger *Exit* syscall with a return code of 10:

```
li a0, 10
li a7, 93
ecall
```

As shown in the example, not all syscalls use all the 6 arguments. In this case the caller side can only fill in the needed arguments.

Syscalls can respond to the VM in 2 ways:

* A return value is put in `A0` if exists.
* Syscalls can also write data in memory location pointed by certain syscall arguments, so upon syscall completion, normal VM instructions can read the data prepared by the syscall.

For convenience, we could wrap the logic of calling a syscall in a C function:

```c
static inline long
__internal_syscall(long n, long _a0, long _a1, long _a2, long _a3, long _a4, long _a5)
{
  register long a0 asm("a0") = _a0;
  register long a1 asm("a1") = _a1;
  register long a2 asm("a2") = _a2;
  register long a3 asm("a3") = _a3;
  register long a4 asm("a4") = _a4;
  register long a5 asm("a5") = _a5;

  register long syscall_id asm("a7") = n;

  asm volatile ("scall"
		: "+r"(a0) : "r"(a1), "r"(a2), "r"(a3), "r"(a4), "r"(a5), "r"(syscall_id));

  return a0;
}

#define syscall(n, a, b, c, d, e, f) \
        __internal_syscall(n, (long)(a), (long)(b), (long)(c), (long)(d), (long)(e), (long)(f))
```

(NOTE: this is adapted from [riscv-newlib](https://github.com/riscv/riscv-newlib/blob/77e11e1800f57cac7f5468b2bd064100a44755d4/libgloss/riscv/internal_syscall.h#L25))

Now we can trigger the same *Exit* syscall more easily in C code:

```c
syscall(93, 10, 0, 0, 0, 0, 0);
```

Note that even though *Exit* syscall only needs one argument, our C wrapper requires us to fill in all 6 arguments. We can initialize other unused arguments as all 0. Below we would illustrate each syscall with a C function signature to demonstrate each syscall's accepted arguments. Also for clarifying reason, all the code shown in this RFC is assumed to be written in pure C.

- [Exit]
- [Load Transaction Hash]
- [Load Transaction]
- [Load Script Hash]
- [Load Script]
- [Load Cell]
- [Load Cell By Field]
- [Load Cell Data]
- [Load Cell Data As Code]
- [Load Input]
- [Load Input By Field]
- [Load Header]
- [Load Header By Field]
- [Load Witness]
- [Debug]

### Exit
[exit]: #exit

As shown above, *Exit* syscall has a signature like following:

```c
void exit(int8_t code)
{
  syscall(93, code, 0, 0, 0, 0, 0);
}
```

*Exit* syscall don't need a return value since CKB VM is not supposed to return from this function. Upon receiving this syscall, CKB VM would terminate execution with the specified return code. This is the only way of correctly exiting a script in CKB VM.

### Load Transaction Hash
[load transaction hash]: #load-transaction-hash

*Load Transaction Hash* syscall has a signature like following:

```c
int ckb_load_tx_hash(void* addr, uint64_t* len, size_t offset)
{
  return syscall(2061, addr, len, offset, 0, 0, 0);
}
```

The arguments used here are:

* `addr`, `len` and `offset` follow the usage descripted in [Partial Loading](#partial-loading) section.

This syscall would calculate the hash of current transaction and copy it to VM memory space based on *partial loading* workflow.

### Load Transaction
[load transaction]: #load-transaction

*Load Transaction* syscall has a signature like following:

```c
int ckb_load_transaction(void* addr, uint64_t* len, size_t offset)
{
  return syscall(2051, addr, len, offset, 0, 0, 0);
}
```

The arguments used here are:

* `addr`, `len` and `offset` follow the usage descripted in [Partial Loading](#partial-loading) section.

This syscall serializes the full transaction containing running script into the Molecule Encoding [1] format, then copy it to VM memory space based on *partial loading* workflow.

### Load Script Hash
[load script hash]: #load-script-hash

*Load Script Hash* syscall has a signature like following:

```c
int ckb_load_script_hash(void* addr, uint64_t* len, size_t offset)
{
  return syscall(2062, addr, len, offset, 0, 0, 0);
}
```

The arguments used here are:

* `addr`, `len` and `offset` follow the usage descripted in [Partial Loading](#partial-loading) section.

This syscall would calculate the hash of current running script and copy it to VM memory space based on *partial loading* workflow.

### Load Script
[load script]: #load-script

*Load Script* syscall has a signature like following:

```c
int ckb_load_script(void* addr, uint64_t* len, size_t offset)
{
  return syscall(2052, addr, len, offset, 0, 0, 0);
}
```

The arguments used here are:

* `addr`, `len` and `offset` follow the usage descripted in [Partial Loading](#partial-loading) section.

This syscall serializes the current running script into the Molecule Encoding [1] format, then copy it to VM memory space based on *partial loading* workflow.

### Load Cell
[load cell]: #load-cell

*Load Cell* syscall has a signature like following:

```c
int ckb_load_cell(void* addr, uint64_t* len, size_t offset, size_t index, size_t source)
{
  return syscall(2071, addr, len, offset, index, source, 0);
}
```

The arguments used here are:

* `addr`, `len` and `offset` follow the usage descripted in [Partial Loading](#partial-loading) section.
* `index`: an index value denoting the index of entries to read.
* `source`: a flag denoting the source of cells to locate, possible values include:
    + 1: input cells.
    + `0x0100000000000001`: input cells with the same running script as current script
    + 2: output cells.
    + `0x0100000000000002`: output cells with the same running script as current script
    + 3: dep cells.

This syscall would locate a single cell in the current transaction based on `source` and `index` value, serialize the whole cell into the Molecule Encoding [1] format, then use the same step as documented in [Partial Loading](#partial-loading) section to feed the serialized value into VM.

This syscall might return the following errors:

* An invalid source value would immediately trigger an VM error and halt execution.
* The syscall would return with `1` as return value if the index value is out of bound.

In case of errors, `addr` and `index` will not contain meaningful data to use.

### Load Cell By Field
[load cell by field]: #load-cell-by-field

*Load Cell By Field* syscall has a signature like following:

```c
int ckb_load_cell_by_field(void* addr, uint64_t* len, size_t offset,
                           size_t index, size_t source, size_t field)
{
  return syscall(2081, addr, len, offset, index, source, field);
}
```

The arguments used here are:

* `addr`, `len` and `offset` follow the usage descripted in [Partial Loading](#partial-loading) section.
* `index`: an index value denoting the index of entries to read.
* `source`: a flag denoting the source of cells to locate, possible values include:
    + 1: input cells.
    + `0x0100000000000001`: input cells with the same running script as current script
    + 2: output cells.
    + `0x0100000000000002`: output cells with the same running script as current script
    + 3: dep cells.
* `field`: a flag denoting the field of the cell to read, possible values include:
    + 0: capacity in 64-bit unsigned little endian integer value.
    + 1: data hash.
    + 2: lock in the Molecule Encoding format.
    + 3: lock hash.
    + 4: type in the Molecule Encoding format.
    + 5: type hash.
    + 6: occupied capacity in 64-bit unsigned little endian integer value.

This syscall would locate a single cell in current transaction just like *Load Cell* syscall, and then fetches the data denoted by the `field` value. The data is then fed into VM memory space using the *partial loading* workflow.

This syscall might return the following errors:

* An invalid source value would immediately trigger an VM error and halt execution.
* The syscall would return with `1` as return value if the index value is out of bound.
* An invalid field value would immediately trigger an VM error and halt execution.
* In some cases certain values are missing(such as requesting type on a cell without type script), the syscall would return `2` as return value then.

In case of errors, `addr` and `index` will not contain meaningful data to use.

### Load Cell Data
[load cell Data]: #load-cell-data

*Load Cell Data* syscall has a signature like following:

```c
int ckb_load_cell_data(void* addr, uint64_t* len, size_t offset,
                       size_t index, size_t source)
{
  return syscall(2092, addr, len, offset, index, source, 0);
}
```

The arguments used here are:

* `addr`, `len` and `offset` follow the usage descripted in [Partial Loading](#partial-loading) section.
* `index`: an index value denoting the index of entries to read.
* `source`: a flag denoting the source of cells to locate, possible values include:
    + 1: input cells.
    + `0x0100000000000001`: input cells with the same running script as current script
    + 2: output cells.
    + `0x0100000000000002`: output cells with the same running script as current script
    + 3: dep cells.

This syscall would locale a single cell in the current transaction just like *Load Cell* syscall, then locates its cell data section. The cell data is then fed into VM memory space using the *partial loading* workflow.

This syscall might return the following errors:

* An invalid source value would immediately trigger an VM error and halt execution.
* The syscall would return with `1` as return value if the index value is out of bound.

In case of errors, `addr` and `index` will not contain meaningful data to use.

### Load Cell Data As Code
[load cell Data As Code]: #load-cell-data-as_code

*Load Cell Data* syscall has a signature like following:

```c
int ckb_load_cell_data_as_code(void* addr, size_t memory_size, size_t content_offset,
                               size_t content_size, size_t index, size_t source)
{
  return syscall(2091, addr, memory_size, content_offset, content_size, index, source);
}
```

The arguments used here are:

* `addr`: a pointer to a buffer in VM memory space used to hold loaded code, must be aligned on a 4KB boundary.
* `memory_size`: the size of memory buffer used to hold code, must be a multiple of 4KB.
* `content_offset`: start offset of code to load in cell data.
* `content_size`: size of code content to load in cell data.
* `index`: an index value denoting the index of entries to read.
* `source`: a flag denoting the source of cells to locate, possible values include:
    + 1: input cells.
    + `0x0100000000000001`: input cells with the same running script as current script
    + 2: output cells.
    + `0x0100000000000002`: output cells with the same running script as current script
    + 3: dep cells.

This syscall would locale a single cell in the current transaction just like *Load Cell* syscall, then locates its cell data section. But different from *Load Cell Data* syscall, this syscall would load the requested cell data content into VM memory, and marked the loaded memory page as executable. Later CKB VM can then jump to the loaded memory page to execute loaded code. This can be used to implement dynamic linking in CKB VM.

Notice this syscall does not implement *partial loading* workflow.

For now, memory pages marked as executable cannot be reverted to non-executable pages.

This syscall might return the following errors:

* An invalid source value would immediately trigger an VM error and halt execution.
* The syscall would return with `1` as return value if the index value is out of bound.
* An unaligned `addr` or `memory_size` would immediately trigger an VM error and halt execution.
* Out of bound`content_offset` or `content_size` values would immediately trigger an VM error and halt execution.
* `content_size` must not be larger than `memory_size`, otherwise it would immediately trigger an VM error and halt execution.

In case of errors, `addr` and `index` will not contain meaningful data to use.

For an example using this syscall, please refer to [this script](https://github.com/nervosnetwork/ckb-miscellaneous-scripts/blob/0759a656c20e652e9ad2711fde0ed96ce9f1130b/c/or.c).

### Load Input
[load input]: #load-input

*Load Input* syscall has a signature like following:

```c
int ckb_load_input(void* addr, uint64_t* len, size_t offset,
                   size_t index, size_t source)
{
  return syscall(2073, addr, len, offset, index, source, 0);
}
```

The arguments used here are:

* `addr`, `len` and `offset` follow the usage descripted in [Partial Loading](#partial-loading) section.
* `index`: an index value denoting the index of inputs to read.
* `source`: a flag denoting the source of inputs to locate, possible values include:
    + 1: input cells.
    + `0x0100000000000001`: input cells with the same running script as current script

This syscall would locate a single cell input in the current transaction based on `source` and `index` value, serialize the whole cell input into the Molecule Encoding [1] format, then use the same step as documented in [Partial Loading](#partial-loading) section to feed the serialized value into VM.

This syscall might return the following errors:
* An invalid source value would immediately trigger an VM error and halt execution.
* The syscall would return with `1` as return value if the index value is out of bound.
* When `output cells` or `dep cells` is used in `source` field, the syscall would return with `2` as return value, since cell input only exists for input cells.

In case of errors, `addr` and `index` will not contain meaningful data to use.

### Load Input By Field
[load input by field]: #load-input-by-field

*Load Input By Field* syscall has a signature like following:

```c
int ckb_load_input_by_field(void* addr, uint64_t* len, size_t offset,
                            size_t index, size_t source, size_t field)
{
  return syscall(2083, addr, len, offset, index, source, field);
}
```

The arguments used here are:

* `addr`, `len` and `offset` follow the usage descripted in [Partial Loading](#partial-loading) section.
* `index`: an index value denoting the index of inputs to read.
* `source`: a flag denoting the source of inputs to locate, possible values include:
    + 1: inputs.
    + `0x0100000000000001`: input cells with the same running script as current script
* `field`: a flag denoting the field of the input to read, possible values include:
    + 0: out_point in the Molecule Encoding format.
    + 1: since in 64-bit unsigned little endian integer value.

This syscall would locate a single cell input in current transaction just like *Load Cell* syscall, and then fetches the data denoted by the `field` value. The data is then fed into VM memory space using the *partial loading* workflow.

This syscall might return the following errors:
* An invalid source value would immediately trigger an VM error and halt execution.
* The syscall would return with `1` as return value if the index value is out of bound.
* When `output cells` or `dep cells` is used in `source` field, the syscall would return with `2` as return value, since cell input only exists for input cells.
* An invalid field value would immediately trigger an VM error and halt execution.

In case of errors, `addr` and `index` will not contain meaningful data to use.

### Load Header
[load header]: #load-header

*Load Header* syscall has a signature like following:

```c
int ckb_load_header(void* addr, uint64_t* len, size_t offset, size_t index, size_t source)
{
  return syscall(2072, addr, len, offset, index, source, 0);
}
```

The arguments used here are:

* `addr`, `len` and `offset` follow the usage descripted in [Partial Loading](#partial-loading) section.
* `index`: an index value denoting the index of entries to read.
* `source`: a flag denoting the source of cells to locate, possible values include:
    + 1: input cells.
    + `0x0100000000000001`: input cells with the same running script as current script
    + 3: dep cells.
    + 4: header deps.

This syscall would locate the header associated either with an input cell, a dep cell, or a header dep based on `source` and `index` value, serialize the whole header into Molecule Encoding [1] format, then use the same step as documented in [Partial Loading](#partial-loading) section to feed the serialized value into VM.

Note when you are loading the header associated with an input cell or a dep cell, the header hash should still be included in `header deps` section of current transaction.

This syscall might return the following errors:
* An invalid source value would immediately trigger an VM error and halt execution.
* The syscall would return with `1` as return value if the index value is out of bound.
* This syscall would return with `2` as return value if requesting a header for an input cell, but the `header deps` section is missing the header hash for the input cell.

In case of errors, `addr` and `index` will not contain meaningful data to use.

#### Loading Header Immature Rule
[loading header immature Rule]: #loading-header-immature-error

Attention that all the blocks referenced in header deps must be 4 epochs ago, otherwise the header is immature and the transaction must wait. For example, if the block is the first block in epoch 4, a transaction with its header as a header dep can only be included in the first block of epoch 8 and later blocks.

This rule will be removed since ckb2021 as proposed in [RFC36].

[RFC36]: ../0036-remove-header-deps-immature-rule/0036-remove-header-deps-immature-rule.md

### Load Header By Field
[load header by field]: #load-header-by-field

*Load Header By Field* syscall has a signature like following:

```c
int ckb_load_header_by_field(void* addr, uint64_t* len, size_t offset,
                             size_t index, size_t source, size_t field)
{
  return syscall(2082, addr, len, offset, index, source, field);
}
```

The arguments used here are:

* `addr`, `len` and `offset` follow the usage descripted in [Partial Loading](#partial-loading) section.
* `index`: an index value denoting the index of entries to read.
* `source`: a flag denoting the source of cells to locate, possible values include:
    + 1: input cells.
    + `0x0100000000000001`: input cells with the same running script as current script
    + 3: dep cells.
    + 4: header deps.
* `field`: a flag denoting the field of the header to read, possible values include:
    + 0: current epoch number in 64-bit unsigned little endian integer value.
    + 1: block number for the start of current epoch in 64-bit unsigned little endian integer value.
    + 2: epoch length in 64-bit unsigned little endian integer value.

This syscall would locate the header associated either with an input cell, a dep cell, or a header dep based on `source` and `index` value, and then fetches the data denoted by the `field` value. The data is then fed into VM memory space using the *partial loading* workflow.

Note when you are loading the header associated with an input cell or a dep cell, the header hash should still be included in `header deps` section of current transaction.

This syscall might return the following errors:
* An invalid source value would immediately trigger an VM error and halt execution.
* The syscall would return with `1` as return value if the index value is out of bound.
* This syscall would return with `2` as return value if requesting a header for an input cell, but the `header deps` section is missing the header hash for the input cell.
* An invalid field value would immediately trigger an VM error and halt execution.

In case of errors, `addr` and `index` will not contain meaningful data to use.

*Attention** that this syscall also follows [loading header immature rule][].

### Load Witness
[load witness]: #load-witness

*Load Witness* syscall has a signature like following:

```c
int ckb_load_witness(void* addr, uint64_t* len, size_t offset, size_t index, size_t source)
{
  return syscall(2074, addr, len, offset, index, source, 0);
}
```

The arguments used here are:

* `addr`, `len` and `offset` follow the usage descripted in [Partial Loading](#partial-loading) section.
* `index`: an index value denoting the index of entries to read.
* `source`: a flag denoting the source of cells to locate, possible values include:
    + 1: input cells.
    + `0x0100000000000001`: input cells with the same running script as current script
    + 2: output cells.
    + `0x0100000000000002`: output cells with the same running script as current script

This syscall locates a witness entry in current transaction based on `source` and `index` value, then use the same step as documented in [Partial Loading](#partial-loading) section to feed the serialized value into VM.

The `source` field here, is only used a hint helper for script side. As long as one provides a possible `source` listed above, the corresponding witness entry denoted by `index` will be returned.

This syscall might return the following errors:

* An invalid source value would immediately trigger an VM error and halt execution.
* The syscall would return with `1` as return value if the index value is out of bound.

In case of errors, `addr` and `index` will not contain meaningful data to use.

### Debug
[debug]: #debug

*Debug* syscall has a signature like following:

```c
void ckb_debug(const char* s)
{
  syscall(2177, s, 0, 0, 0, 0, 0);
}
```

This syscall accepts a null terminated string and prints it out as debug log in CKB. It can be used as a handy way to debug scripts in CKB. This syscall has no return value.

# Reference

* [1]: [Molecule Encoding][1]
* [2]: [VM Syscalls 2][2]

[1]: ../0008-serialization/0008-serialization.md
[2]: ../0034-vm-syscalls-2/0034-vm-syscalls-2.md


================================================
File: rfcs/0010-eaglesong/0010-eaglesong.md
================================================
---
Number: "0010"
Category: Standards Track
Status: Active
Author: Alan Szepieniec <alan.szepieniec@gmail.com>
Created: 2019-07-18
---

# Eaglesong (Proof-of-Work Function for Nervos CKB)

This document specifies the Eaglesong hash function as it is to be used in the context of Nervos CKB proof-of-work.

 * [Notation](#Notation)
 * [Design Strategies](#Desgin-Strategies)
   * [Sponge Construction](#Sponge-Construction)
   * [ARX](#ARX)
 * [Round Function](#Round-Function)
   * [Bit Matrix](#Bit-Matrix)
   * [Circulant Multiplication](#Circulant-Multiplication)
   * [Injection of Constants](#Injection-of-Constants)
   * [Addition-Rotation-Addition](#Addition-Rotation-Addition)
   * [Full Permutation](#Full-Permutation)
 * [Eaglesong Hash](#Eaglesong-Hash)
 * [Reference Implementations](#Reference-Implementations)


<a name="Notation"></a>
## Notation
In the pseudocode snippets below the following notation is used:

 - `//` -- denotes a comment
 - `||` -- denotes concatenation (of bit strings)
 - `length` -- returns the length (in bits) of the argument
 - `%` -- modulo operator, computes the remainder of left-hand-side argument after division by the right-hand-side
 - `zeros` -- produces a string of zero-bits of the specified length
 - `xor` -- denotes the exclusive-or operation of equal-length bit strings
 - `[]` -- python-style array indexing, with indexation starting (as it should) from zero; and when an upper-bound is included the range until but not including that upper bound is denoted; when not juxtaposed to an array, the expression `[a:b]` denotes the list of integers `{a, a+1, ..., b-1}`.

<a name="Design-Strategies"></a>
# Design Strategies

<a name="Sponge-Construction"></a>
## Sponge Construction

Eaglesong is a [sponge construction](https://en.wikipedia.org/wiki/Sponge_function). This means that what is specified is a permutation `F` along with three variables:

 - `r`, the rate : a positive integer;
 - `c`, the capacity : another positive integer;
 - `d`, the delimiter : a byte fixed to a nonzero value.

The permutation maps `r+c` bits to `r+c` bits. After appending the delimiter byte to variable-length input to the hash function, this input is chunked into pieces `chunk[i]`, each of `r` bits along with the last one padded with zeros as necessary. The sponge construction defines a state, which consists of `r+c` bits initialized to zeros. The absorbing phase iterates over all chunks and xors the current chunk into the top `r` bits of the state before applying the permutation. In the squeezing phase, the output buffer is initialized to the empty string. Until this buffer is of the right size, the permutation is applied to the state and the top `r` bits are read out and appended to the output buffer. If the output buffer is too long, it is truncated. The next pseudocode illustrates this operation.

```
function sponge( input, output_length ):
    // append delimiter and pad as necessary
    input = input || d
    while length(input) % r =/= 0 do:
        input = input || 0

    // split
    for i in [0, length(input) / r] do:
        chunks[i] = input[i*r : (i+1)*r]

    // initialize state
    state = zeros(r+c)

    // absorb
    for chunk in chunks do:
        state[0:r] = state[0:r] xor chunk
        state = F(state)

    // squeeze
    output = ""
    while length(output) <= output_length do:
        output = output || state[0:r]
        state = F(state)

    return output[0:output_length]
```

<a name="ARX"></a>
## ARX

Eaglesong is member of a family of ciphers that use only three operations: addition, rotation, and xor. The operands are always 32-bit integers. This motivates the following notational overload:

 - `+`: addition modulo 2^32;
 - `<<<`: left rotation, specifically `a <<< r` is equivalent to `((a << r) xor (a >> (32-r))) and 0xffffffff`, where `<<` and `>>` denote left and right shift, respectively.

<a name="Round-Function"></a>
# Round Function

The Eaglesong permutation F is obtained by iterating a round function f for `N = 43` times. The round function consists of four steps.

The state of the Eaglesong permutation consists of 16 integers.

<a name="Bit-Matrix"></a>
## Bit Matrix

The state vector is mapped to a new state vector determined via (row-)vector-matrix multiplication, where the coefficients of the matrix are either 0 or 1. Every row of this matrix represents an indication of which state elements to xor together.

The matrix is given below. A multiplication procedure follows.

```
bit_matrix = [
    [1 1 1 1 0 1 0 1 1 1 1 1 0 0 0 1]
    [0 1 1 1 1 0 1 0 1 1 1 1 1 0 0 1]
    [0 0 1 1 1 1 0 1 0 1 1 1 1 1 0 1]
    [0 0 0 1 1 1 1 0 1 0 1 1 1 1 1 1]
    [1 1 1 1 1 0 1 0 1 0 1 0 1 1 1 0]
    [1 0 0 0 1 0 0 0 1 0 1 0 0 1 1 1]
    [1 0 1 1 0 0 0 1 1 0 1 0 0 0 1 0]
    [1 0 1 0 1 1 0 1 0 0 1 0 0 0 0 1]
    [0 1 0 1 0 1 1 0 1 0 0 1 0 0 0 1]
    [0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1]
    [0 0 0 1 0 1 0 1 1 0 1 0 0 1 0 1]
    [0 0 0 0 1 0 1 0 1 1 0 1 0 0 1 1]
    [1 1 1 1 0 0 0 0 1 0 0 1 1 0 0 0]
    [0 1 1 1 1 0 0 0 0 1 0 0 1 1 0 0]
    [0 0 1 1 1 1 0 0 0 0 1 0 0 1 1 0]
    [1 1 1 0 1 0 1 1 1 1 1 0 0 0 1 1]
]

function ApplyBitMatrix( state_vector ):
    for j in range[0:16] do:
        output_vector[j] = 0
        for k in range[0:16] do:
            output_vector[j] = output_vector[j] xor (bit_matrix[j,k] * state_vector[k])
    return output_vector
```

Note that the matrix multiplication needs involve only ARX operations. Since the coefficients of the matrix are known, the loops can be unrolled and the entire matrix multiplication can be described in terms of the xor operation alone. The use of the multiplication operator `*` is merely a notational convenience to stress the link with matrix multiplication.

<a name="Circulant-Multiplication"></a>
## Circulant Multiplication

Every element of the state vector undergoes a map involving xor and rotation. Specifically, `state_vector[i] = state_vector[i] xor (state_vector[i] <<< a[i]) xor (state_vector[i] <<< b[i])`. The coefficients `a` and `b` are different for every component. They are given below, with a procedure for computing the entire step.

```
coefficients = [
    [0, 2, 4],
    [0, 13, 22],
    [0, 4, 19],
    [0, 3, 14],
    [0, 27, 31],
    [0, 3, 8],
    [0, 17, 26],
    [0, 3, 12],
    [0, 18, 22],
    [0, 12, 18],
    [0, 4, 7],
    [0, 4, 31],
    [0, 12, 27],
    [0, 7, 17],
    [0, 7, 8],
    [0, 1, 13]
]

function ApplyCirculantMultiplication( state_vector ):
    for i in [0:16] do:
        output_vector[i] = 0
        for k in [0:3] do:
            output_vector[i] = output_vector[i] xor (state_vector[i] <<< coefficients[i][k])
    return output_vector
```

<a name="Injection-of-Constants"></a>
## Injection of Constants

A given constant is xored into every component of the state vector. The coefficients differ per component and per round. They are determined from using Keccak to expand the ASCII string 

```
The various ways in which the knowledge on which people base their plan is communicated to them is the crucial problem for any theory explaining the economic process, and the problem of what is the best way to utilizing knowledge initially dispersed among all the people is at least one of the main problems of economic policy - or of designing an efficient economic system.
```

The constants are given below, along with a procedure to compute the entire step. Attached to this RFC is a [python script](constants.py) ([and one for keccak](CompactFIPS202.py)) to compute the constants.

```
injection_constants = [
0x6e9e40ae ,  0x71927c02 ,  0x9a13d3b1 ,  0xdaec32ad ,  0x3d8951cf ,  0xe1c9fe9a ,  0xb806b54c ,  0xacbbf417 ,  
0xd3622b3b ,  0xa082762a ,  0x9edcf1c0 ,  0xa9bada77 ,  0x7f91e46c ,  0xcb0f6e4f ,  0x265d9241 ,  0xb7bdeab0 ,  
0x6260c9e6 ,  0xff50dd2a ,  0x9036aa71 ,  0xce161879 ,  0xd1307cdf ,  0x89e456df ,  0xf83133e2 ,  0x65f55c3d ,  
0x94871b01 ,  0xb5d204cd ,  0x583a3264 ,  0x5e165957 ,  0x4cbda964 ,  0x675fca47 ,  0xf4a3033e ,  0x2a417322 ,  
0x3b61432f ,  0x7f5532f2 ,  0xb609973b ,  0x1a795239 ,  0x31b477c9 ,  0xd2949d28 ,  0x78969712 ,  0x0eb87b6e ,  
0x7e11d22d ,  0xccee88bd ,  0xeed07eb8 ,  0xe5563a81 ,  0xe7cb6bcf ,  0x25de953e ,  0x4d05653a ,  0x0b831557 ,  
0x94b9cd77 ,  0x13f01579 ,  0x794b4a4a ,  0x67e7c7dc ,  0xc456d8d4 ,  0x59689c9b ,  0x668456d7 ,  0x22d2a2e1 ,  
0x38b3a828 ,  0x0315ac3c ,  0x438d681e ,  0xab7109c5 ,  0x97ee19a8 ,  0xde062b2e ,  0x2c76c47b ,  0x0084456f ,  
0x908f0fd3 ,  0xa646551f ,  0x3e826725 ,  0xd521788e ,  0x9f01c2b0 ,  0x93180cdc ,  0x92ea1df8 ,  0x431a9aae ,  
0x7c2ea356 ,  0xda33ad03 ,  0x46926893 ,  0x66bde7d7 ,  0xb501cc75 ,  0x1f6e8a41 ,  0x685250f4 ,  0x3bb1f318 ,  
0xaf238c04 ,  0x974ed2ec ,  0x5b159e49 ,  0xd526f8bf ,  0x12085626 ,  0x3e2432a9 ,  0x6bd20c48 ,  0x1f1d59da ,  
0x18ab1068 ,  0x80f83cf8 ,  0x2c8c11c0 ,  0x7d548035 ,  0x0ff675c3 ,  0xfed160bf ,  0x74bbbb24 ,  0xd98e006b ,  
0xdeaa47eb ,  0x05f2179e ,  0x437b0b71 ,  0xa7c95f8f ,  0x00a99d3b ,  0x3fc3c444 ,  0x72686f8e ,  0x00fd01a9 ,  
0xdedc0787 ,  0xc6af7626 ,  0x7012fe76 ,  0xf2a5f7ce ,  0x9a7b2eda ,  0x5e57fcf2 ,  0x4da0d4ad ,  0x5c63b155 ,  
0x34117375 ,  0xd4134c11 ,  0x2ea77435 ,  0x5278b6de ,  0xab522c4c ,  0xbc8fc702 ,  0xc94a09e4 ,  0xebb93a9e ,  
0x91ecb65e ,  0x4c52ecc6 ,  0x8703bb52 ,  0xcb2d60aa ,  0x30a0538a ,  0x1514f10b ,  0x157f6329 ,  0x3429dc3d ,  
0x5db73eb2 ,  0xa7a1a969 ,  0x7286bd24 ,  0x0df6881e ,  0x3785ba5f ,  0xcd04623a ,  0x02758170 ,  0xd827f556 ,  
0x99d95191 ,  0x84457eb1 ,  0x58a7fb22 ,  0xd2967c5f ,  0x4f0c33f6 ,  0x4a02099a ,  0xe0904821 ,  0x94124036 ,  
0x496a031b ,  0x780b69c4 ,  0xcf1a4927 ,  0x87a119b8 ,  0xcdfaf4f8 ,  0x4cf9cd0f ,  0x27c96a84 ,  0x6d11117e ,  
0x7f8cf847 ,  0x74ceede5 ,  0xc88905e6 ,  0x60215841 ,  0x7172875a ,  0x736e993a ,  0x010aa53c ,  0x43d53c2b ,  
0xf0d91a93 ,  0x0d983b56 ,  0xf816663c ,  0xe5d13363 ,  0x0a61737c ,  0x09d51150 ,  0x83a5ac2f ,  0x3e884905 ,  
0x7b01aeb5 ,  0x600a6ea7 ,  0xb7678f7b ,  0x72b38977 ,  0x068018f2 ,  0xce6ae45b ,  0x29188aa8 ,  0xe5a0b1e9 ,  
0xc04c2b86 ,  0x8bd14d75 ,  0x648781f3 ,  0xdbae1e0a ,  0xddcdd8ae ,  0xab4d81a3 ,  0x446baaba ,  0x1cc0c19d ,  
0x17be4f90 ,  0x82c0e65d ,  0x676f9c95 ,  0x5c708db2 ,  0x6fd4c867 ,  0xa5106ef0 ,  0x19dde49d ,  0x78182f95 ,  
0xd089cd81 ,  0xa32e98fe ,  0xbe306c82 ,  0x6cd83d8c ,  0x037f1bde ,  0x0b15722d ,  0xeddc1e22 ,  0x93c76559 ,  
0x8a2f571b ,  0x92cc81b4 ,  0x021b7477 ,  0x67523904 ,  0xc95dbccc ,  0xac17ee9d ,  0x944e46bc ,  0x0781867e ,  
0xc854dd9d ,  0x26e2c30c ,  0x858c0416 ,  0x6d397708 ,  0xebe29c58 ,  0xc80ced86 ,  0xd496b4ab ,  0xbe45e6f5 ,  
0x10d24706 ,  0xacf8187a ,  0x96f523cb ,  0x2227e143 ,  0x78c36564 ,  0x4643adc2 ,  0x4729d97a ,  0xcff93e0d ,  
0x25484bbd ,  0x91c6798e ,  0x95f773f4 ,  0x44204675 ,  0x2eda57ba ,  0x06d313ef ,  0xeeaa4466 ,  0x2dfa7530 ,  
0xa8af0c9b ,  0x39f1535e ,  0x0cc2b7bd ,  0x38a76c0e ,  0x4f41071d ,  0xcdaf2475 ,  0x49a6eff8 ,  0x01621748 ,  
0x36ebacab ,  0xbd6d9a29 ,  0x44d1cd65 ,  0x40815dfd ,  0x55fa5a1a ,  0x87cce9e9 ,  0xae559b45 ,  0xd76b4c26 ,  
0x637d60ad ,  0xde29f5f9 ,  0x97491cbb ,  0xfb350040 ,  0xffe7f997 ,  0x201c9dcd ,  0xe61320e9 ,  0xa90987a3 ,  
0xe24afa83 ,  0x61c1e6fc ,  0xcc87ff62 ,  0xf1c9d8fa ,  0x4fd04546 ,  0x90ecc76e ,  0x46e456b9 ,  0x305dceb8 ,  
0xf627e68c ,  0x2d286815 ,  0xc705bbfd ,  0x101b6df3 ,  0x892dae62 ,  0xd5b7fb44 ,  0xea1d5c94 ,  0x5332e3cb ,  
0xf856f88a ,  0xb341b0e9 ,  0x28408d9d ,  0x5421bc17 ,  0xeb9af9bc ,  0x602371c5 ,  0x67985a91 ,  0xd774907f ,  
0x7c4d697d ,  0x9370b0b8 ,  0x6ff5cebb ,  0x7d465744 ,  0x674ceac0 ,  0xea9102fc ,  0x0de94784 ,  0xc793de69 ,  
0xfe599bb1 ,  0xc6ad952f ,  0x6d6ca9c3 ,  0x928c3f91 ,  0xf9022f05 ,  0x24a164dc ,  0xe5e98cd3 ,  0x7649efdb ,  
0x6df3bcdb ,  0x5d1e9ff1 ,  0x17f5d010 ,  0xe2686ea1 ,  0x6eac77fe ,  0x7bb5c585 ,  0x88d90cbb ,  0x18689163 ,  
0x67c9efa5 ,  0xc0b76d9b ,  0x960efbab ,  0xbd872807 ,  0x70f4c474 ,  0x56c29d20 ,  0xd1541d15 ,  0x88137033 ,  
0xe3f02b3e ,  0xb6d9b28d ,  0x53a077ba ,  0xeedcd29e ,  0xa50a6c1d ,  0x12c2801e ,  0x52ba335b ,  0x35984614 ,  
0xe2599aa8 ,  0xaf94ed1d ,  0xd90d4767 ,  0x202c7d07 ,  0x77bec4f4 ,  0xfa71bc80 ,  0xfc5c8b76 ,  0x8d0fbbfc ,  
0xda366dc6 ,  0x8b32a0c7 ,  0x1b36f7fc ,  0x6642dcbc ,  0x6fe7e724 ,  0x8b5fa782 ,  0xc4227404 ,  0x3a7d1da7 ,  
0x517ed658 ,  0x8a18df6d ,  0x3e5c9b23 ,  0x1fbd51ef ,  0x1470601d ,  0x3400389c ,  0x676b065d ,  0x8864ad80 ,  
0xea6f1a9c ,  0x2db484e1 ,  0x608785f0 ,  0x8dd384af ,  0x69d26699 ,  0x409c4e16 ,  0x77f9986a ,  0x7f491266 ,  
0x883ea6cf ,  0xeaa06072 ,  0xfa2e5db5 ,  0x352594b4 ,  0x9156bb89 ,  0xa2fbbbfb ,  0xac3989c7 ,  0x6e2422b1 ,  
0x581f3560 ,  0x1009a9b5 ,  0x7e5ad9cd ,  0xa9fc0a6e ,  0x43e5998e ,  0x7f8778f9 ,  0xf038f8e1 ,  0x5415c2e8 ,  
0x6499b731 ,  0xb82389ae ,  0x05d4d819 ,  0x0f06440e ,  0xf1735aa0 ,  0x986430ee ,  0x47ec952c ,  0xbf149cc5 ,  
0xb3cb2cb6 ,  0x3f41e8c2 ,  0x271ac51b ,  0x48ac5ded ,  0xf76a0469 ,  0x717bba4d ,  0x4f5c90d6 ,  0x3b74f756 ,  
0x1824110a ,  0xa4fd43e3 ,  0x1eb0507c ,  0xa9375c08 ,  0x157c59a7 ,  0x0cad8f51 ,  0xd66031a0 ,  0xabb5343f ,  
0xe533fa43 ,  0x1996e2bb ,  0xd7953a71 ,  0xd2529b94 ,  0x58f0fa07 ,  0x4c9b1877 ,  0x057e990d ,  0x8bfe19c4 ,  
0xa8e2c0c9 ,  0x99fcaada ,  0x69d2aaca ,  0xdc1c4642 ,  0xf4d22307 ,  0x7fe27e8c ,  0x1366aa07 ,  0x1594e637 ,  
0xce1066bf ,  0xdb922552 ,  0x9930b52a ,  0xaeaa9a3e ,  0x31ff7eb4 ,  0x5e1f945a ,  0x150ac49c ,  0x0ccdac2d ,  
0xd8a8a217 ,  0xb82ea6e5 ,  0xd6a74659 ,  0x67b7e3e6 ,  0x836eef4a ,  0xb6f90074 ,  0x7fa3ea4b ,  0xcb038123 ,  
0xbf069f55 ,  0x1fa83fc4 ,  0xd6ebdb23 ,  0x16f0a137 ,  0x19a7110d ,  0x5ff3b55f ,  0xfb633868 ,  0xb466f845 ,  
0xbce0c198 ,  0x88404296 ,  0xddbdd88b ,  0x7fc52546 ,  0x63a553f8 ,  0xa728405a ,  0x378a2bce ,  0x6862e570 ,  
0xefb77e7d ,  0xc611625e ,  0x32515c15 ,  0x6984b765 ,  0xe8405976 ,  0x9ba386fd ,  0xd4eed4d9 ,  0xf8fe0309 ,  
0x0ce54601 ,  0xbaf879c2 ,  0xd8524057 ,  0x1d8c1d7a ,  0x72c0a3a9 ,  0x5a1ffbde ,  0x82f33a45 ,  0x5143f446 ,  
0x29c7e182 ,  0xe536c32f ,  0x5a6f245b ,  0x44272adb ,  0xcb701d9c ,  0xf76137ec ,  0x0841f145 ,  0xe7042ecc ,  
0xf1277dd7 ,  0x745cf92c ,  0xa8fe65fe ,  0xd3e2d7cf ,  0x54c513ef ,  0x6079bc2d ,  0xb66336b0 ,  0x101e383b ,  
0xbcd75753 ,  0x25be238a ,  0x56a6f0be ,  0xeeffcc17 ,  0x5ea31f3d ,  0x0ae772f5 ,  0xf76de3de ,  0x1bbecdad ,  
0xc9107d43 ,  0xf7e38dce ,  0x618358cd ,  0x5c833f04 ,  0xf6975906 ,  0xde4177e5 ,  0x67d314dc ,  0xb4760f3e ,  
0x56ce5888 ,  0x0e8345a8 ,  0xbff6b1bf ,  0x78dfb112 ,  0xf1709c1e ,  0x7bb8ed8b ,  0x902402b9 ,  0xdaa64ae0 ,  
0x46b71d89 ,  0x7eee035f ,  0xbe376509 ,  0x99648f3a ,  0x0863ea1f ,  0x49ad8887 ,  0x79bdecc5 ,  0x3c10b568 ,  
0x5f2e4bae ,  0x04ef20ab ,  0x72f8ce7b ,  0x521e1ebe ,  0x14525535 ,  0x2e8af95b ,  0x9094ccfd ,  0xbcf36713 ,  
0xc73953ef ,  0xd4b91474 ,  0x6554ec2d ,  0xe3885c96 ,  0x03dc73b7 ,  0x931688a9 ,  0xcbbef182 ,  0x2b77cfc9 ,  
0x632a32bd ,  0xd2115dcc ,  0x1ae5533d ,  0x32684e13 ,  0x4cc5a004 ,  0x13321bde ,  0x62cbd38d ,  0x78383a3b ,  
0xd00686f1 ,  0x9f601ee7 ,  0x7eaf23de ,  0x3110c492 ,  0x9c351209 ,  0x7eb89d52 ,  0x6d566eac ,  0xc2efd226 ,  
0x32e9fac5 ,  0x52227274 ,  0x09f84725 ,  0xb8d0b605 ,  0x72291f02 ,  0x71b5c34b ,  0x3dbfcbb8 ,  0x04a02263 ,  
0x55ba597f ,  0xd4e4037d ,  0xc813e1be ,  0xffddeefa ,  0xc3c058f3 ,  0x87010f2e ,  0x1dfcf55f ,  0xc694eeeb ,  
0xa9c01a74 ,  0x98c2fc6b ,  0xe57e1428 ,  0xdd265a71 ,  0x836b956d ,  0x7e46ab1a ,  0x5835d541 ,  0x50b32505 ,  
0xe640913c ,  0xbb486079 ,  0xfe496263 ,  0x113c5b69 ,  0x93cd6620 ,  0x5efe823b ,  0x2d657b40 ,  0xb46dfc6c ,  
0x57710c69 ,  0xfe9fadeb ,  0xb5f8728a ,  0xe3224170 ,  0xca28b751 ,  0xfdabae56 ,  0x5ab12c3c ,  0xa697c457 ,  
0xd28fa2b7 ,  0x056579f2 ,  0x9fd9d810 ,  0xe3557478 ,  0xd88d89ab ,  0xa72a9422 ,  0x6d47abd0 ,  0x405bcbd9 ,  
0x6f83ebaf ,  0x13caec76 ,  0xfceb9ee2 ,  0x2e922df7 ,  0xce9856df ,  0xc05e9322 ,  0x2772c854 ,  0xb67f2a32 ,  
0x6d1af28d ,  0x3a78cf77 ,  0xdff411e4 ,  0x61c74ca9 ,  0xed8b842e ,  0x72880845 ,  0x6e857085 ,  0xc6404932 ,  
0xee37f6bc ,  0x27116f48 ,  0x5e9ec45a ,  0x8ea2a51f ,  0xa5573db7 ,  0xa746d036 ,  0x486b4768 ,  0x5b438f3b ,  
0x18c54a5c ,  0x64fcf08e ,  0xe993cdc1 ,  0x35c1ead3 ,  0x9de07de7 ,  0x321b841c ,  0x87423c5e ,  0x071aa0f6 ,  
0x962eb75b ,  0xbb06bdd2 ,  0xdcdb5363 ,  0x389752f2 ,  0x83d9cc88 ,  0xd014adc6 ,  0xc71121bb ,  0x2372f938 ,  
0xcaff2650 ,  0x62be8951 ,  0x56dccaff ,  0xac4084c0 ,  0x09712e95 ,  0x1d3c288f ,  0x1b085744 ,  0xe1d3cfef ,  
0x5c9a812e ,  0x6611fd59 ,  0x85e46044 ,  0x1981d885 ,  0x5a4c903f ,  0x43f30d4b ,  0x7d1d601b ,  0xdd3c3391 ,  
0x030ec65e ,  0xc12878cd ,  0x72e795fe ,  0xd0c76abd ,  0x1ec085db ,  0x7cbb61fa ,  0x93e8dd1e ,  0x8582eb06 ,  
0x73563144 ,  0x049d4e7e ,  0x5fd5aefe ,  0x7b842a00 ,  0x75ced665 ,  0xbb32d458 ,  0x4e83bba7 ,  0x8f15151f ,  
0x7795a125 ,  0xf0842455 ,  0x499af99d ,  0x565cc7fa ,  0xa3b1278d ,  0x3f27ce74 ,  0x96ca058e ,  0x8a497443 ,  
0xa6fb8cae ,  0xc115aa21 ,  0x17504923 ,  0xe4932402 ,  0xaea886c2 ,  0x8eb79af5 ,  0xebd5ea6b ,  0xc7980d3b ,  
0x71369315 ,  0x796e6a66 ,  0x3a7ec708 ,  0xb05175c8 ,  0xe02b74e7 ,  0xeb377ad3 ,  0x6c8c1f54 ,  0xb980c374 ,  
0x59aee281 ,  0x449cb799 ,  0xe01f5605 ,  0xed0e085e ,  0xc9a1a3b4 ,  0xaac481b1 ,  0xc935c39c ,  0xb7d8ce7f ]

function InjectConstants( state_vector, round_index ):
    for i in [0:16] do:
        output_vector[i] = state_vector[i] xor injection_constants[round_index*16 + i]
    return output_vector
```

<a name="Addition-Rotation-Addition"></a>
## Addition-Rotation-Addition

The addition-rotation-addition step is the only one that involves modular addition. The state vector is partitioned into 8 pairs, and every pair undergoes a sequence of three operations. First, the element on the right is added to the one on the left; then both words are rotated by 8 bits in opposite directions; then the left is added to the right. The next pseudocode makes this formal.

```
function ApplyARA( state_vector ):
    for i in [0:8] do:
        output_vector[2*i] = state_vector[2*i] + state_vector[2*i + 1]
        output_vector[2*i] = output_vector[2*i] <<< 8
        output_vector[2*i + 1] = state_vector[2*i + 1] <<< 24
        output_vector[2*i + 1] = output_vector[2*i + 1] + output_vector[2*i]
    return output_vector
```

<a name="Full-Permutation"></a>
## Full Permutation

For the sake of completeness, the following pseudocode computes the full permutation.

```
function F( input ):
    state = input
    for i in [0:43] do:
        state = ApplyBitMatrix( state )
        state = ApplyCirculantMultiplication( state )
        state = InjectConstants( state, i )
        state = ApplyARA( state )
    return state
```

<a name="Eaglesong-Hash"></a>
# Eaglesong Hash

The Eaglesong hash function is obtained by taking the sponge code and instantiating the parameters with:

 - `r = 256`
 - `c = 256`
 - `output_length = 256`
 - `d = 0x06`

<a name="Reference-Implementations"></a>
# Reference Implementations

This RFC is supplemented by two reference implementations, one in [C](eaglesong.c) and one in [python](eaglesong.py). Attached is also a wrapper ([C](hash.c) and [python](hash.py)) for computing the Eaglesong hash of a string read from stdin.

To compile and run the C program:
```
$> gcc -o hash eaglesong.c hash.c
$> ./hash
 > Hello, world!
 > [Ctrl-D]
 > 64867e2441d162615dc2430b6bcb4d3f4b95e4d0db529fca1eece73c077d72d6
```

To run the python program:
```
$> python hash.py
 > Hello, world!
 > [Ctrl-D]
 > 64867e2441d162615dc2430b6bcb4d3f4b95e4d0db529fca1eece73c077d72d6
```



================================================
File: rfcs/0010-eaglesong/CompactFIPS202.py
================================================
# -*- coding: utf-8 -*-
# Implementation by the Keccak, Keyak and Ketje Teams, namely, Guido Bertoni,
# Joan Daemen, Michaël Peeters, Gilles Van Assche and Ronny Van Keer, hereby
# denoted as "the implementer".
#
# For more information, feedback or questions, please refer to our websites:
# http://keccak.noekeon.org/
# http://keyak.noekeon.org/
# http://ketje.noekeon.org/
#
# To the extent possible under law, the implementer has waived all copyright
# and related or neighboring rights to the source code in this file.
# http://creativecommons.org/publicdomain/zero/1.0/

def ROL64(a, n):
    return ((a >> (64-(n%64))) + (a << (n%64))) % (1 << 64)

def KeccakF1600onLanes(lanes):
    R = 1
    for round in range(24):
        # θ
        C = [lanes[x][0] ^ lanes[x][1] ^ lanes[x][2] ^ lanes[x][3] ^ lanes[x][4] for x in range(5)]
        D = [C[(x+4)%5] ^ ROL64(C[(x+1)%5], 1) for x in range(5)]
        lanes = [[lanes[x][y]^D[x] for y in range(5)] for x in range(5)]
        # ρ and π
        (x, y) = (1, 0)
        current = lanes[x][y]
        for t in range(24):
            (x, y) = (y, (2*x+3*y)%5)
            (current, lanes[x][y]) = (lanes[x][y], ROL64(current, (t+1)*(t+2)//2))
        # χ
        for y in range(5):
            T = [lanes[x][y] for x in range(5)]
            for x in range(5):
                lanes[x][y] = T[x] ^((~T[(x+1)%5]) & T[(x+2)%5])
        # ι
        for j in range(7):
            R = ((R << 1) ^ ((R >> 7)*0x71)) % 256
            if (R & 2):
                lanes[0][0] = lanes[0][0] ^ (1 << ((1<<j)-1))
    return lanes

def load64(b):
    return sum((b[i] << (8*i)) for i in range(8))

def store64(a):
    return list((a >> (8*i)) % 256 for i in range(8))

def KeccakF1600(state):
    lanes = [[load64(state[8*(x+5*y):8*(x+5*y)+8]) for y in range(5)] for x in range(5)]
    lanes = KeccakF1600onLanes(lanes)
    state = bytearray(200)
    for x in range(5):
        for y in range(5):
            state[8*(x+5*y):8*(x+5*y)+8] = store64(lanes[x][y])
    return state

def Keccak(rate, capacity, inputBytes, delimitedSuffix, outputByteLen):
    outputBytes = bytearray()
    state = bytearray([0 for i in range(200)])
    rateInBytes = rate//8
    blockSize = 0
    if (((rate + capacity) != 1600) or ((rate % 8) != 0)):
        return
    inputOffset = 0
    # === Absorb all the input blocks ===
    while(inputOffset < len(inputBytes)):
        blockSize = min(len(inputBytes)-inputOffset, rateInBytes)
        for i in range(blockSize):
            state[i] = state[i] ^ inputBytes[i+inputOffset]
        inputOffset = inputOffset + blockSize
        if (blockSize == rateInBytes):
            state = KeccakF1600(state)
            blockSize = 0
    # === Do the padding and switch to the squeezing phase ===
    state[blockSize] = state[blockSize] ^ delimitedSuffix
    if (((delimitedSuffix & 0x80) != 0) and (blockSize == (rateInBytes-1))):
        state = KeccakF1600(state)
    state[rateInBytes-1] = state[rateInBytes-1] ^ 0x80
    state = KeccakF1600(state)
    # === Squeeze out all the output blocks ===
    while(outputByteLen > 0):
        blockSize = min(outputByteLen, rateInBytes)
        outputBytes = outputBytes + state[0:blockSize]
        outputByteLen = outputByteLen - blockSize
        if (outputByteLen > 0):
            state = KeccakF1600(state)
    return outputBytes

def SHAKE128(inputBytes, outputByteLen):
    return Keccak(1344, 256, inputBytes, 0x1F, outputByteLen)

def SHAKE256(inputBytes, outputByteLen):
    return Keccak(1088, 512, inputBytes, 0x1F, outputByteLen)

def SHA3_224(inputBytes):
    return Keccak(1152, 448, inputBytes, 0x06, 224//8)

def SHA3_256(inputBytes):
    return Keccak(1088, 512, inputBytes, 0x06, 256//8)

def SHA3_384(inputBytes):
    return Keccak(832, 768, inputBytes, 0x06, 384//8)

def SHA3_512(inputBytes):
    return Keccak(576, 1024, inputBytes, 0x06, 512//8)


================================================
File: rfcs/0010-eaglesong/constants.py
================================================
from CompactFIPS202 import SHAKE256

num_rounds = 43
num_constants = 16 * num_rounds
num_bytes = num_constants * 4

def padhex( integer, number_digits ):
    return "0x" + ("0" * (number_digits - len(hex(integer))+2)) + hex(integer)[2:]

#randomness = SHAKE256(bytearray("I have always been on the machines' side."), num_bytes)
randomness = SHAKE256(bytearray("The various ways in which the knowledge on which people base their plan is communicated to them is the crucial problem for any theory explaining the economic process, and the problem of what is the best way to utilizing knowledge initially dispersed among all the people is at least one of the main problems of economic policy - or of designing an efficient economic system."), num_bytes)

constants = []
for i in range(0, num_constants):
    integer = sum(256**j * randomness[i*4 + j] for j in range(0,4))
    constants.append(integer)

#print "constants = [", ", ".join(hex(c) for c in constants), "]"
print "injection_constants = [",
for i in range(0, num_constants):
    print padhex(constants[i], 8),
    if i != num_constants - 1:
        print ", ",
    if i%8 == 7 and i != num_constants - 1:
        print ""
print "]"



================================================
File: rfcs/0010-eaglesong/eaglesong.c
================================================
#include <stdint.h>
#include <stdio.h>

uint32_t bit_matrix[] = {1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
                         0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
                         0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
                         0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
                         1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
                         1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
                         1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
                         1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
                         0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
                         0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
                         0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
                         0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
                         1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
                         0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
                         0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
                         1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1};
int coefficients[] = {0, 2, 4, 0, 13, 22, 0, 4, 19, 0, 3, 14, 0, 27, 31, 0, 3, 8, 0, 17, 26, 0, 3, 12, 0, 18, 22, 0, 12, 18, 0, 4, 7, 0, 4, 31, 0, 12, 27, 0, 7, 17, 0, 7, 8, 0, 1, 13};
uint32_t injection_constants[] = { 0x6e9e40ae ,  0x71927c02 ,  0x9a13d3b1 ,  0xdaec32ad ,  0x3d8951cf ,  0xe1c9fe9a ,  0xb806b54c ,  0xacbbf417 ,
0xd3622b3b ,  0xa082762a ,  0x9edcf1c0 ,  0xa9bada77 ,  0x7f91e46c ,  0xcb0f6e4f ,  0x265d9241 ,  0xb7bdeab0 ,
0x6260c9e6 ,  0xff50dd2a ,  0x9036aa71 ,  0xce161879 ,  0xd1307cdf ,  0x89e456df ,  0xf83133e2 ,  0x65f55c3d ,
0x94871b01 ,  0xb5d204cd ,  0x583a3264 ,  0x5e165957 ,  0x4cbda964 ,  0x675fca47 ,  0xf4a3033e ,  0x2a417322 ,
0x3b61432f ,  0x7f5532f2 ,  0xb609973b ,  0x1a795239 ,  0x31b477c9 ,  0xd2949d28 ,  0x78969712 ,  0x0eb87b6e ,
0x7e11d22d ,  0xccee88bd ,  0xeed07eb8 ,  0xe5563a81 ,  0xe7cb6bcf ,  0x25de953e ,  0x4d05653a ,  0x0b831557 ,
0x94b9cd77 ,  0x13f01579 ,  0x794b4a4a ,  0x67e7c7dc ,  0xc456d8d4 ,  0x59689c9b ,  0x668456d7 ,  0x22d2a2e1 ,
0x38b3a828 ,  0x0315ac3c ,  0x438d681e ,  0xab7109c5 ,  0x97ee19a8 ,  0xde062b2e ,  0x2c76c47b ,  0x0084456f ,
0x908f0fd3 ,  0xa646551f ,  0x3e826725 ,  0xd521788e ,  0x9f01c2b0 ,  0x93180cdc ,  0x92ea1df8 ,  0x431a9aae ,
0x7c2ea356 ,  0xda33ad03 ,  0x46926893 ,  0x66bde7d7 ,  0xb501cc75 ,  0x1f6e8a41 ,  0x685250f4 ,  0x3bb1f318 ,
0xaf238c04 ,  0x974ed2ec ,  0x5b159e49 ,  0xd526f8bf ,  0x12085626 ,  0x3e2432a9 ,  0x6bd20c48 ,  0x1f1d59da ,
0x18ab1068 ,  0x80f83cf8 ,  0x2c8c11c0 ,  0x7d548035 ,  0x0ff675c3 ,  0xfed160bf ,  0x74bbbb24 ,  0xd98e006b ,
0xdeaa47eb ,  0x05f2179e ,  0x437b0b71 ,  0xa7c95f8f ,  0x00a99d3b ,  0x3fc3c444 ,  0x72686f8e ,  0x00fd01a9 ,
0xdedc0787 ,  0xc6af7626 ,  0x7012fe76 ,  0xf2a5f7ce ,  0x9a7b2eda ,  0x5e57fcf2 ,  0x4da0d4ad ,  0x5c63b155 ,
0x34117375 ,  0xd4134c11 ,  0x2ea77435 ,  0x5278b6de ,  0xab522c4c ,  0xbc8fc702 ,  0xc94a09e4 ,  0xebb93a9e ,
0x91ecb65e ,  0x4c52ecc6 ,  0x8703bb52 ,  0xcb2d60aa ,  0x30a0538a ,  0x1514f10b ,  0x157f6329 ,  0x3429dc3d ,
0x5db73eb2 ,  0xa7a1a969 ,  0x7286bd24 ,  0x0df6881e ,  0x3785ba5f ,  0xcd04623a ,  0x02758170 ,  0xd827f556 ,
0x99d95191 ,  0x84457eb1 ,  0x58a7fb22 ,  0xd2967c5f ,  0x4f0c33f6 ,  0x4a02099a ,  0xe0904821 ,  0x94124036 ,
0x496a031b ,  0x780b69c4 ,  0xcf1a4927 ,  0x87a119b8 ,  0xcdfaf4f8 ,  0x4cf9cd0f ,  0x27c96a84 ,  0x6d11117e ,
0x7f8cf847 ,  0x74ceede5 ,  0xc88905e6 ,  0x60215841 ,  0x7172875a ,  0x736e993a ,  0x010aa53c ,  0x43d53c2b ,
0xf0d91a93 ,  0x0d983b56 ,  0xf816663c ,  0xe5d13363 ,  0x0a61737c ,  0x09d51150 ,  0x83a5ac2f ,  0x3e884905 ,
0x7b01aeb5 ,  0x600a6ea7 ,  0xb7678f7b ,  0x72b38977 ,  0x068018f2 ,  0xce6ae45b ,  0x29188aa8 ,  0xe5a0b1e9 ,
0xc04c2b86 ,  0x8bd14d75 ,  0x648781f3 ,  0xdbae1e0a ,  0xddcdd8ae ,  0xab4d81a3 ,  0x446baaba ,  0x1cc0c19d ,
0x17be4f90 ,  0x82c0e65d ,  0x676f9c95 ,  0x5c708db2 ,  0x6fd4c867 ,  0xa5106ef0 ,  0x19dde49d ,  0x78182f95 ,
0xd089cd81 ,  0xa32e98fe ,  0xbe306c82 ,  0x6cd83d8c ,  0x037f1bde ,  0x0b15722d ,  0xeddc1e22 ,  0x93c76559 ,
0x8a2f571b ,  0x92cc81b4 ,  0x021b7477 ,  0x67523904 ,  0xc95dbccc ,  0xac17ee9d ,  0x944e46bc ,  0x0781867e ,
0xc854dd9d ,  0x26e2c30c ,  0x858c0416 ,  0x6d397708 ,  0xebe29c58 ,  0xc80ced86 ,  0xd496b4ab ,  0xbe45e6f5 ,
0x10d24706 ,  0xacf8187a ,  0x96f523cb ,  0x2227e143 ,  0x78c36564 ,  0x4643adc2 ,  0x4729d97a ,  0xcff93e0d ,
0x25484bbd ,  0x91c6798e ,  0x95f773f4 ,  0x44204675 ,  0x2eda57ba ,  0x06d313ef ,  0xeeaa4466 ,  0x2dfa7530 ,
0xa8af0c9b ,  0x39f1535e ,  0x0cc2b7bd ,  0x38a76c0e ,  0x4f41071d ,  0xcdaf2475 ,  0x49a6eff8 ,  0x01621748 ,
0x36ebacab ,  0xbd6d9a29 ,  0x44d1cd65 ,  0x40815dfd ,  0x55fa5a1a ,  0x87cce9e9 ,  0xae559b45 ,  0xd76b4c26 ,
0x637d60ad ,  0xde29f5f9 ,  0x97491cbb ,  0xfb350040 ,  0xffe7f997 ,  0x201c9dcd ,  0xe61320e9 ,  0xa90987a3 ,
0xe24afa83 ,  0x61c1e6fc ,  0xcc87ff62 ,  0xf1c9d8fa ,  0x4fd04546 ,  0x90ecc76e ,  0x46e456b9 ,  0x305dceb8 ,
0xf627e68c ,  0x2d286815 ,  0xc705bbfd ,  0x101b6df3 ,  0x892dae62 ,  0xd5b7fb44 ,  0xea1d5c94 ,  0x5332e3cb ,
0xf856f88a ,  0xb341b0e9 ,  0x28408d9d ,  0x5421bc17 ,  0xeb9af9bc ,  0x602371c5 ,  0x67985a91 ,  0xd774907f ,
0x7c4d697d ,  0x9370b0b8 ,  0x6ff5cebb ,  0x7d465744 ,  0x674ceac0 ,  0xea9102fc ,  0x0de94784 ,  0xc793de69 ,
0xfe599bb1 ,  0xc6ad952f ,  0x6d6ca9c3 ,  0x928c3f91 ,  0xf9022f05 ,  0x24a164dc ,  0xe5e98cd3 ,  0x7649efdb ,
0x6df3bcdb ,  0x5d1e9ff1 ,  0x17f5d010 ,  0xe2686ea1 ,  0x6eac77fe ,  0x7bb5c585 ,  0x88d90cbb ,  0x18689163 ,
0x67c9efa5 ,  0xc0b76d9b ,  0x960efbab ,  0xbd872807 ,  0x70f4c474 ,  0x56c29d20 ,  0xd1541d15 ,  0x88137033 ,
0xe3f02b3e ,  0xb6d9b28d ,  0x53a077ba ,  0xeedcd29e ,  0xa50a6c1d ,  0x12c2801e ,  0x52ba335b ,  0x35984614 ,
0xe2599aa8 ,  0xaf94ed1d ,  0xd90d4767 ,  0x202c7d07 ,  0x77bec4f4 ,  0xfa71bc80 ,  0xfc5c8b76 ,  0x8d0fbbfc ,
0xda366dc6 ,  0x8b32a0c7 ,  0x1b36f7fc ,  0x6642dcbc ,  0x6fe7e724 ,  0x8b5fa782 ,  0xc4227404 ,  0x3a7d1da7 ,
0x517ed658 ,  0x8a18df6d ,  0x3e5c9b23 ,  0x1fbd51ef ,  0x1470601d ,  0x3400389c ,  0x676b065d ,  0x8864ad80 ,
0xea6f1a9c ,  0x2db484e1 ,  0x608785f0 ,  0x8dd384af ,  0x69d26699 ,  0x409c4e16 ,  0x77f9986a ,  0x7f491266 ,
0x883ea6cf ,  0xeaa06072 ,  0xfa2e5db5 ,  0x352594b4 ,  0x9156bb89 ,  0xa2fbbbfb ,  0xac3989c7 ,  0x6e2422b1 ,
0x581f3560 ,  0x1009a9b5 ,  0x7e5ad9cd ,  0xa9fc0a6e ,  0x43e5998e ,  0x7f8778f9 ,  0xf038f8e1 ,  0x5415c2e8 ,
0x6499b731 ,  0xb82389ae ,  0x05d4d819 ,  0x0f06440e ,  0xf1735aa0 ,  0x986430ee ,  0x47ec952c ,  0xbf149cc5 ,
0xb3cb2cb6 ,  0x3f41e8c2 ,  0x271ac51b ,  0x48ac5ded ,  0xf76a0469 ,  0x717bba4d ,  0x4f5c90d6 ,  0x3b74f756 ,
0x1824110a ,  0xa4fd43e3 ,  0x1eb0507c ,  0xa9375c08 ,  0x157c59a7 ,  0x0cad8f51 ,  0xd66031a0 ,  0xabb5343f ,
0xe533fa43 ,  0x1996e2bb ,  0xd7953a71 ,  0xd2529b94 ,  0x58f0fa07 ,  0x4c9b1877 ,  0x057e990d ,  0x8bfe19c4 ,
0xa8e2c0c9 ,  0x99fcaada ,  0x69d2aaca ,  0xdc1c4642 ,  0xf4d22307 ,  0x7fe27e8c ,  0x1366aa07 ,  0x1594e637 ,
0xce1066bf ,  0xdb922552 ,  0x9930b52a ,  0xaeaa9a3e ,  0x31ff7eb4 ,  0x5e1f945a ,  0x150ac49c ,  0x0ccdac2d ,
0xd8a8a217 ,  0xb82ea6e5 ,  0xd6a74659 ,  0x67b7e3e6 ,  0x836eef4a ,  0xb6f90074 ,  0x7fa3ea4b ,  0xcb038123 ,
0xbf069f55 ,  0x1fa83fc4 ,  0xd6ebdb23 ,  0x16f0a137 ,  0x19a7110d ,  0x5ff3b55f ,  0xfb633868 ,  0xb466f845 ,
0xbce0c198 ,  0x88404296 ,  0xddbdd88b ,  0x7fc52546 ,  0x63a553f8 ,  0xa728405a ,  0x378a2bce ,  0x6862e570 ,
0xefb77e7d ,  0xc611625e ,  0x32515c15 ,  0x6984b765 ,  0xe8405976 ,  0x9ba386fd ,  0xd4eed4d9 ,  0xf8fe0309 ,
0x0ce54601 ,  0xbaf879c2 ,  0xd8524057 ,  0x1d8c1d7a ,  0x72c0a3a9 ,  0x5a1ffbde ,  0x82f33a45 ,  0x5143f446 ,
0x29c7e182 ,  0xe536c32f ,  0x5a6f245b ,  0x44272adb ,  0xcb701d9c ,  0xf76137ec ,  0x0841f145 ,  0xe7042ecc ,
0xf1277dd7 ,  0x745cf92c ,  0xa8fe65fe ,  0xd3e2d7cf ,  0x54c513ef ,  0x6079bc2d ,  0xb66336b0 ,  0x101e383b ,
0xbcd75753 ,  0x25be238a ,  0x56a6f0be ,  0xeeffcc17 ,  0x5ea31f3d ,  0x0ae772f5 ,  0xf76de3de ,  0x1bbecdad ,
0xc9107d43 ,  0xf7e38dce ,  0x618358cd ,  0x5c833f04 ,  0xf6975906 ,  0xde4177e5 ,  0x67d314dc ,  0xb4760f3e ,
0x56ce5888 ,  0x0e8345a8 ,  0xbff6b1bf ,  0x78dfb112 ,  0xf1709c1e ,  0x7bb8ed8b ,  0x902402b9 ,  0xdaa64ae0 ,
0x46b71d89 ,  0x7eee035f ,  0xbe376509 ,  0x99648f3a ,  0x0863ea1f ,  0x49ad8887 ,  0x79bdecc5 ,  0x3c10b568 ,
0x5f2e4bae ,  0x04ef20ab ,  0x72f8ce7b ,  0x521e1ebe ,  0x14525535 ,  0x2e8af95b ,  0x9094ccfd ,  0xbcf36713 ,
0xc73953ef ,  0xd4b91474 ,  0x6554ec2d ,  0xe3885c96 ,  0x03dc73b7 ,  0x931688a9 ,  0xcbbef182 ,  0x2b77cfc9 ,
0x632a32bd ,  0xd2115dcc ,  0x1ae5533d ,  0x32684e13 ,  0x4cc5a004 ,  0x13321bde ,  0x62cbd38d ,  0x78383a3b ,
0xd00686f1 ,  0x9f601ee7 ,  0x7eaf23de ,  0x3110c492 ,  0x9c351209 ,  0x7eb89d52 ,  0x6d566eac ,  0xc2efd226 ,
0x32e9fac5 ,  0x52227274 ,  0x09f84725 ,  0xb8d0b605 ,  0x72291f02 ,  0x71b5c34b ,  0x3dbfcbb8 ,  0x04a02263 ,
0x55ba597f ,  0xd4e4037d ,  0xc813e1be ,  0xffddeefa ,  0xc3c058f3 ,  0x87010f2e ,  0x1dfcf55f ,  0xc694eeeb ,
0xa9c01a74 ,  0x98c2fc6b ,  0xe57e1428 ,  0xdd265a71 ,  0x836b956d ,  0x7e46ab1a ,  0x5835d541 ,  0x50b32505 ,
0xe640913c ,  0xbb486079 ,  0xfe496263 ,  0x113c5b69 ,  0x93cd6620 ,  0x5efe823b ,  0x2d657b40 ,  0xb46dfc6c ,
0x57710c69 ,  0xfe9fadeb ,  0xb5f8728a ,  0xe3224170 ,  0xca28b751 ,  0xfdabae56 ,  0x5ab12c3c ,  0xa697c457 ,
0xd28fa2b7 ,  0x056579f2 ,  0x9fd9d810 ,  0xe3557478 ,  0xd88d89ab ,  0xa72a9422 ,  0x6d47abd0 ,  0x405bcbd9 ,
0x6f83ebaf ,  0x13caec76 ,  0xfceb9ee2 ,  0x2e922df7 ,  0xce9856df ,  0xc05e9322 ,  0x2772c854 ,  0xb67f2a32 ,
0x6d1af28d ,  0x3a78cf77 ,  0xdff411e4 ,  0x61c74ca9 ,  0xed8b842e ,  0x72880845 ,  0x6e857085 ,  0xc6404932 ,
0xee37f6bc ,  0x27116f48 ,  0x5e9ec45a ,  0x8ea2a51f ,  0xa5573db7 ,  0xa746d036 ,  0x486b4768 ,  0x5b438f3b ,
0x18c54a5c ,  0x64fcf08e ,  0xe993cdc1 ,  0x35c1ead3 ,  0x9de07de7 ,  0x321b841c ,  0x87423c5e ,  0x071aa0f6 ,
0x962eb75b ,  0xbb06bdd2 ,  0xdcdb5363 ,  0x389752f2 ,  0x83d9cc88 ,  0xd014adc6 ,  0xc71121bb ,  0x2372f938 ,
0xcaff2650 ,  0x62be8951 ,  0x56dccaff ,  0xac4084c0 ,  0x09712e95 ,  0x1d3c288f ,  0x1b085744 ,  0xe1d3cfef ,
0x5c9a812e ,  0x6611fd59 ,  0x85e46044 ,  0x1981d885 ,  0x5a4c903f ,  0x43f30d4b ,  0x7d1d601b ,  0xdd3c3391 ,
0x030ec65e ,  0xc12878cd ,  0x72e795fe ,  0xd0c76abd ,  0x1ec085db ,  0x7cbb61fa ,  0x93e8dd1e ,  0x8582eb06 ,
0x73563144 ,  0x049d4e7e ,  0x5fd5aefe ,  0x7b842a00 ,  0x75ced665 ,  0xbb32d458 ,  0x4e83bba7 ,  0x8f15151f ,
0x7795a125 ,  0xf0842455 ,  0x499af99d ,  0x565cc7fa ,  0xa3b1278d ,  0x3f27ce74 ,  0x96ca058e ,  0x8a497443 ,
0xa6fb8cae ,  0xc115aa21 ,  0x17504923 ,  0xe4932402 ,  0xaea886c2 ,  0x8eb79af5 ,  0xebd5ea6b ,  0xc7980d3b ,
0x71369315 ,  0x796e6a66 ,  0x3a7ec708 ,  0xb05175c8 ,  0xe02b74e7 ,  0xeb377ad3 ,  0x6c8c1f54 ,  0xb980c374 ,
0x59aee281 ,  0x449cb799 ,  0xe01f5605 ,  0xed0e085e ,  0xc9a1a3b4 ,  0xaac481b1 ,  0xc935c39c ,  0xb7d8ce7f };

int num_rounds = 43;
int capacity = 256;
int rate = 256;

void PrintState( uint32_t * state ) {
    int i;
    for( i = 0 ; i < 16 ; ++i ) {
        printf("0x%02x%02x%02x%02x ", (state[i] >> (3*8)) & 0xff, (state[i] >> (2*8)) & 0xff, (state[i] >> (1*8)) & 0xff, (state[i] >> (0*8)) & 0xff);
    }

    printf("\n");
}

void EaglesongPermutation( uint32_t * state ) {
    uint32_t new[16];
    int i, j, k;

    //PrintState(state);

    for( i = 0 ; i < num_rounds ; ++i ) {
        // bit matrix
        for( j = 0 ; j < 16 ; ++j ) {
            new[j] = 0;
            for( k = 0 ; k < 16 ; ++k ) {
                new[j] = new[j] ^ (bit_matrix[k*16 + j] * state[k]);
            }
        }
        for( j = 0 ; j < 16 ; ++j ) {
            state[j] = new[j];
        }

        // circulant multiplication
        for( j = 0 ; j < 16 ; ++j ) {
            state[j] = state[j]  ^  (state[j] << coefficients[3*j+1]) ^ (state[j] >> (32-coefficients[3*j+1]))  ^  (state[j] << coefficients[3*j+2]) ^ (state[j] >> (32-coefficients[3*j+2]));
        }

        // constants injection
        for( j = 0 ; j < 16 ; ++j ) {
            state[j] = state[j] ^ injection_constants[i*16+j];
        }

        // addition / rotation / addition
        for( j = 0 ; j < 16 ; j = j + 2 ) {
            state[j] = state[j] + state[j+1];
            state[j] = (state[j] << 8) ^ (state[j] >> 24);
            state[j+1] = (state[j+1] << 24) ^ (state[j+1] >> 8);
            state[j+1] = state[j] + state[j+1];
        }
    }
}

void EaglesongSponge( unsigned char * output, unsigned int output_length, const unsigned char * input, unsigned int input_length, unsigned char delimiter ) {
    uint32_t state[16];
    int i, j, k;
    uint32_t integer;

    // initialize to zero
    for( i = 0 ; i < 16 ; ++i ) {
        state[i] = 0;
    }

    // absorbing
    for( i = 0 ; i < ((input_length+1)*8+rate-1) / rate ; ++i ) {
        for( j = 0 ; j < rate/32 ; ++j ) {
            integer = 0;
            for( k = 0 ; k < 4 ; ++k ) {
                if( i*rate/8 + j*4 + k < input_length ) {
                    integer = (integer << 8) ^ input[i*rate/8 + j*4 + k];
                }
                else if( i*rate/8 + j*4 + k == input_length ) {
                    integer = (integer << 8) ^ delimiter;
                }
            }
            state[j] = state[j] ^ integer;
        }
        EaglesongPermutation(state);
    }

    // squeezing
    for( i = 0 ; i < output_length/(rate/8) ; ++i ) {
        for( j = 0 ; j < rate/32 ; ++j ) {
            for( k = 0 ; k < 4 ; ++k ) {
                output[i*rate/8 + j*4 + k] = (state[j] >> (8*k)) & 0xff;
            }
        }
        EaglesongPermutation(state);
    }
}

void EaglesongHash( unsigned char * output, const unsigned char * input, unsigned int input_length ) {
    EaglesongSponge(output, 32, input, input_length, 0x06);
}


================================================
File: rfcs/0010-eaglesong/eaglesong.py
================================================
def PrintState( state ):
    s = ""
    for i in range(0, 16):
        s += "0x%08x" % state[i]
        s += " "
    print(s)

def EaglesongPermutation( state ):
    N = 43
    #PrintState(state)
    for i in range(0, N):
        state = EaglesongRound(state, i)
    return state

def EaglesongRound( state, index ):
    # constants
    bitmatrix = [[1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1],
                 [0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1],
                 [0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1],
                 [0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1],
                 [1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0],
                 [1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1],
                 [1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0],
                 [1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1],
                 [0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1],
                 [0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1],
                 [0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1],
                 [0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1],
                 [1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0],
                 [0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0],
                 [0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0],
                 [1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1]]
    coefficients = [[0, 2, 4], [0, 13, 22], [0, 4, 19], [0, 3, 14], [0, 27, 31], [0, 3, 8], [0, 17, 26], [0, 3, 12], [0, 18, 22], [0, 12, 18], [0, 4, 7], [0, 4, 31], [0, 12, 27], [0, 7, 17], [0, 7, 8], [0, 1, 13]]
    injection_constants = [ 0x6e9e40ae ,  0x71927c02 ,  0x9a13d3b1 ,  0xdaec32ad ,  0x3d8951cf ,  0xe1c9fe9a ,  0xb806b54c ,  0xacbbf417 ,
    0xd3622b3b ,  0xa082762a ,  0x9edcf1c0 ,  0xa9bada77 ,  0x7f91e46c ,  0xcb0f6e4f ,  0x265d9241 ,  0xb7bdeab0 ,
    0x6260c9e6 ,  0xff50dd2a ,  0x9036aa71 ,  0xce161879 ,  0xd1307cdf ,  0x89e456df ,  0xf83133e2 ,  0x65f55c3d ,
    0x94871b01 ,  0xb5d204cd ,  0x583a3264 ,  0x5e165957 ,  0x4cbda964 ,  0x675fca47 ,  0xf4a3033e ,  0x2a417322 ,
    0x3b61432f ,  0x7f5532f2 ,  0xb609973b ,  0x1a795239 ,  0x31b477c9 ,  0xd2949d28 ,  0x78969712 ,  0x0eb87b6e ,
    0x7e11d22d ,  0xccee88bd ,  0xeed07eb8 ,  0xe5563a81 ,  0xe7cb6bcf ,  0x25de953e ,  0x4d05653a ,  0x0b831557 ,
    0x94b9cd77 ,  0x13f01579 ,  0x794b4a4a ,  0x67e7c7dc ,  0xc456d8d4 ,  0x59689c9b ,  0x668456d7 ,  0x22d2a2e1 ,
    0x38b3a828 ,  0x0315ac3c ,  0x438d681e ,  0xab7109c5 ,  0x97ee19a8 ,  0xde062b2e ,  0x2c76c47b ,  0x0084456f ,
    0x908f0fd3 ,  0xa646551f ,  0x3e826725 ,  0xd521788e ,  0x9f01c2b0 ,  0x93180cdc ,  0x92ea1df8 ,  0x431a9aae ,
    0x7c2ea356 ,  0xda33ad03 ,  0x46926893 ,  0x66bde7d7 ,  0xb501cc75 ,  0x1f6e8a41 ,  0x685250f4 ,  0x3bb1f318 ,
    0xaf238c04 ,  0x974ed2ec ,  0x5b159e49 ,  0xd526f8bf ,  0x12085626 ,  0x3e2432a9 ,  0x6bd20c48 ,  0x1f1d59da ,
    0x18ab1068 ,  0x80f83cf8 ,  0x2c8c11c0 ,  0x7d548035 ,  0x0ff675c3 ,  0xfed160bf ,  0x74bbbb24 ,  0xd98e006b ,
    0xdeaa47eb ,  0x05f2179e ,  0x437b0b71 ,  0xa7c95f8f ,  0x00a99d3b ,  0x3fc3c444 ,  0x72686f8e ,  0x00fd01a9 ,
    0xdedc0787 ,  0xc6af7626 ,  0x7012fe76 ,  0xf2a5f7ce ,  0x9a7b2eda ,  0x5e57fcf2 ,  0x4da0d4ad ,  0x5c63b155 ,
    0x34117375 ,  0xd4134c11 ,  0x2ea77435 ,  0x5278b6de ,  0xab522c4c ,  0xbc8fc702 ,  0xc94a09e4 ,  0xebb93a9e ,
    0x91ecb65e ,  0x4c52ecc6 ,  0x8703bb52 ,  0xcb2d60aa ,  0x30a0538a ,  0x1514f10b ,  0x157f6329 ,  0x3429dc3d ,
    0x5db73eb2 ,  0xa7a1a969 ,  0x7286bd24 ,  0x0df6881e ,  0x3785ba5f ,  0xcd04623a ,  0x02758170 ,  0xd827f556 ,
    0x99d95191 ,  0x84457eb1 ,  0x58a7fb22 ,  0xd2967c5f ,  0x4f0c33f6 ,  0x4a02099a ,  0xe0904821 ,  0x94124036 ,
    0x496a031b ,  0x780b69c4 ,  0xcf1a4927 ,  0x87a119b8 ,  0xcdfaf4f8 ,  0x4cf9cd0f ,  0x27c96a84 ,  0x6d11117e ,
    0x7f8cf847 ,  0x74ceede5 ,  0xc88905e6 ,  0x60215841 ,  0x7172875a ,  0x736e993a ,  0x010aa53c ,  0x43d53c2b ,
    0xf0d91a93 ,  0x0d983b56 ,  0xf816663c ,  0xe5d13363 ,  0x0a61737c ,  0x09d51150 ,  0x83a5ac2f ,  0x3e884905 ,
    0x7b01aeb5 ,  0x600a6ea7 ,  0xb7678f7b ,  0x72b38977 ,  0x068018f2 ,  0xce6ae45b ,  0x29188aa8 ,  0xe5a0b1e9 ,
    0xc04c2b86 ,  0x8bd14d75 ,  0x648781f3 ,  0xdbae1e0a ,  0xddcdd8ae ,  0xab4d81a3 ,  0x446baaba ,  0x1cc0c19d ,
    0x17be4f90 ,  0x82c0e65d ,  0x676f9c95 ,  0x5c708db2 ,  0x6fd4c867 ,  0xa5106ef0 ,  0x19dde49d ,  0x78182f95 ,
    0xd089cd81 ,  0xa32e98fe ,  0xbe306c82 ,  0x6cd83d8c ,  0x037f1bde ,  0x0b15722d ,  0xeddc1e22 ,  0x93c76559 ,
    0x8a2f571b ,  0x92cc81b4 ,  0x021b7477 ,  0x67523904 ,  0xc95dbccc ,  0xac17ee9d ,  0x944e46bc ,  0x0781867e ,
    0xc854dd9d ,  0x26e2c30c ,  0x858c0416 ,  0x6d397708 ,  0xebe29c58 ,  0xc80ced86 ,  0xd496b4ab ,  0xbe45e6f5 ,
    0x10d24706 ,  0xacf8187a ,  0x96f523cb ,  0x2227e143 ,  0x78c36564 ,  0x4643adc2 ,  0x4729d97a ,  0xcff93e0d ,
    0x25484bbd ,  0x91c6798e ,  0x95f773f4 ,  0x44204675 ,  0x2eda57ba ,  0x06d313ef ,  0xeeaa4466 ,  0x2dfa7530 ,
    0xa8af0c9b ,  0x39f1535e ,  0x0cc2b7bd ,  0x38a76c0e ,  0x4f41071d ,  0xcdaf2475 ,  0x49a6eff8 ,  0x01621748 ,
    0x36ebacab ,  0xbd6d9a29 ,  0x44d1cd65 ,  0x40815dfd ,  0x55fa5a1a ,  0x87cce9e9 ,  0xae559b45 ,  0xd76b4c26 ,
    0x637d60ad ,  0xde29f5f9 ,  0x97491cbb ,  0xfb350040 ,  0xffe7f997 ,  0x201c9dcd ,  0xe61320e9 ,  0xa90987a3 ,
    0xe24afa83 ,  0x61c1e6fc ,  0xcc87ff62 ,  0xf1c9d8fa ,  0x4fd04546 ,  0x90ecc76e ,  0x46e456b9 ,  0x305dceb8 ,
    0xf627e68c ,  0x2d286815 ,  0xc705bbfd ,  0x101b6df3 ,  0x892dae62 ,  0xd5b7fb44 ,  0xea1d5c94 ,  0x5332e3cb ,
    0xf856f88a ,  0xb341b0e9 ,  0x28408d9d ,  0x5421bc17 ,  0xeb9af9bc ,  0x602371c5 ,  0x67985a91 ,  0xd774907f ,
    0x7c4d697d ,  0x9370b0b8 ,  0x6ff5cebb ,  0x7d465744 ,  0x674ceac0 ,  0xea9102fc ,  0x0de94784 ,  0xc793de69 ,
    0xfe599bb1 ,  0xc6ad952f ,  0x6d6ca9c3 ,  0x928c3f91 ,  0xf9022f05 ,  0x24a164dc ,  0xe5e98cd3 ,  0x7649efdb ,
    0x6df3bcdb ,  0x5d1e9ff1 ,  0x17f5d010 ,  0xe2686ea1 ,  0x6eac77fe ,  0x7bb5c585 ,  0x88d90cbb ,  0x18689163 ,
    0x67c9efa5 ,  0xc0b76d9b ,  0x960efbab ,  0xbd872807 ,  0x70f4c474 ,  0x56c29d20 ,  0xd1541d15 ,  0x88137033 ,
    0xe3f02b3e ,  0xb6d9b28d ,  0x53a077ba ,  0xeedcd29e ,  0xa50a6c1d ,  0x12c2801e ,  0x52ba335b ,  0x35984614 ,
    0xe2599aa8 ,  0xaf94ed1d ,  0xd90d4767 ,  0x202c7d07 ,  0x77bec4f4 ,  0xfa71bc80 ,  0xfc5c8b76 ,  0x8d0fbbfc ,
    0xda366dc6 ,  0x8b32a0c7 ,  0x1b36f7fc ,  0x6642dcbc ,  0x6fe7e724 ,  0x8b5fa782 ,  0xc4227404 ,  0x3a7d1da7 ,
    0x517ed658 ,  0x8a18df6d ,  0x3e5c9b23 ,  0x1fbd51ef ,  0x1470601d ,  0x3400389c ,  0x676b065d ,  0x8864ad80 ,
    0xea6f1a9c ,  0x2db484e1 ,  0x608785f0 ,  0x8dd384af ,  0x69d26699 ,  0x409c4e16 ,  0x77f9986a ,  0x7f491266 ,
    0x883ea6cf ,  0xeaa06072 ,  0xfa2e5db5 ,  0x352594b4 ,  0x9156bb89 ,  0xa2fbbbfb ,  0xac3989c7 ,  0x6e2422b1 ,
    0x581f3560 ,  0x1009a9b5 ,  0x7e5ad9cd ,  0xa9fc0a6e ,  0x43e5998e ,  0x7f8778f9 ,  0xf038f8e1 ,  0x5415c2e8 ,
    0x6499b731 ,  0xb82389ae ,  0x05d4d819 ,  0x0f06440e ,  0xf1735aa0 ,  0x986430ee ,  0x47ec952c ,  0xbf149cc5 ,
    0xb3cb2cb6 ,  0x3f41e8c2 ,  0x271ac51b ,  0x48ac5ded ,  0xf76a0469 ,  0x717bba4d ,  0x4f5c90d6 ,  0x3b74f756 ,
    0x1824110a ,  0xa4fd43e3 ,  0x1eb0507c ,  0xa9375c08 ,  0x157c59a7 ,  0x0cad8f51 ,  0xd66031a0 ,  0xabb5343f ,
    0xe533fa43 ,  0x1996e2bb ,  0xd7953a71 ,  0xd2529b94 ,  0x58f0fa07 ,  0x4c9b1877 ,  0x057e990d ,  0x8bfe19c4 ,
    0xa8e2c0c9 ,  0x99fcaada ,  0x69d2aaca ,  0xdc1c4642 ,  0xf4d22307 ,  0x7fe27e8c ,  0x1366aa07 ,  0x1594e637 ,
    0xce1066bf ,  0xdb922552 ,  0x9930b52a ,  0xaeaa9a3e ,  0x31ff7eb4 ,  0x5e1f945a ,  0x150ac49c ,  0x0ccdac2d ,
    0xd8a8a217 ,  0xb82ea6e5 ,  0xd6a74659 ,  0x67b7e3e6 ,  0x836eef4a ,  0xb6f90074 ,  0x7fa3ea4b ,  0xcb038123 ,
    0xbf069f55 ,  0x1fa83fc4 ,  0xd6ebdb23 ,  0x16f0a137 ,  0x19a7110d ,  0x5ff3b55f ,  0xfb633868 ,  0xb466f845 ,
    0xbce0c198 ,  0x88404296 ,  0xddbdd88b ,  0x7fc52546 ,  0x63a553f8 ,  0xa728405a ,  0x378a2bce ,  0x6862e570 ,
    0xefb77e7d ,  0xc611625e ,  0x32515c15 ,  0x6984b765 ,  0xe8405976 ,  0x9ba386fd ,  0xd4eed4d9 ,  0xf8fe0309 ,
    0x0ce54601 ,  0xbaf879c2 ,  0xd8524057 ,  0x1d8c1d7a ,  0x72c0a3a9 ,  0x5a1ffbde ,  0x82f33a45 ,  0x5143f446 ,
    0x29c7e182 ,  0xe536c32f ,  0x5a6f245b ,  0x44272adb ,  0xcb701d9c ,  0xf76137ec ,  0x0841f145 ,  0xe7042ecc ,
    0xf1277dd7 ,  0x745cf92c ,  0xa8fe65fe ,  0xd3e2d7cf ,  0x54c513ef ,  0x6079bc2d ,  0xb66336b0 ,  0x101e383b ,
    0xbcd75753 ,  0x25be238a ,  0x56a6f0be ,  0xeeffcc17 ,  0x5ea31f3d ,  0x0ae772f5 ,  0xf76de3de ,  0x1bbecdad ,
    0xc9107d43 ,  0xf7e38dce ,  0x618358cd ,  0x5c833f04 ,  0xf6975906 ,  0xde4177e5 ,  0x67d314dc ,  0xb4760f3e ,
    0x56ce5888 ,  0x0e8345a8 ,  0xbff6b1bf ,  0x78dfb112 ,  0xf1709c1e ,  0x7bb8ed8b ,  0x902402b9 ,  0xdaa64ae0 ,
    0x46b71d89 ,  0x7eee035f ,  0xbe376509 ,  0x99648f3a ,  0x0863ea1f ,  0x49ad8887 ,  0x79bdecc5 ,  0x3c10b568 ,
    0x5f2e4bae ,  0x04ef20ab ,  0x72f8ce7b ,  0x521e1ebe ,  0x14525535 ,  0x2e8af95b ,  0x9094ccfd ,  0xbcf36713 ,
    0xc73953ef ,  0xd4b91474 ,  0x6554ec2d ,  0xe3885c96 ,  0x03dc73b7 ,  0x931688a9 ,  0xcbbef182 ,  0x2b77cfc9 ,
    0x632a32bd ,  0xd2115dcc ,  0x1ae5533d ,  0x32684e13 ,  0x4cc5a004 ,  0x13321bde ,  0x62cbd38d ,  0x78383a3b ,
    0xd00686f1 ,  0x9f601ee7 ,  0x7eaf23de ,  0x3110c492 ,  0x9c351209 ,  0x7eb89d52 ,  0x6d566eac ,  0xc2efd226 ,
    0x32e9fac5 ,  0x52227274 ,  0x09f84725 ,  0xb8d0b605 ,  0x72291f02 ,  0x71b5c34b ,  0x3dbfcbb8 ,  0x04a02263 ,
    0x55ba597f ,  0xd4e4037d ,  0xc813e1be ,  0xffddeefa ,  0xc3c058f3 ,  0x87010f2e ,  0x1dfcf55f ,  0xc694eeeb ,
    0xa9c01a74 ,  0x98c2fc6b ,  0xe57e1428 ,  0xdd265a71 ,  0x836b956d ,  0x7e46ab1a ,  0x5835d541 ,  0x50b32505 ,
    0xe640913c ,  0xbb486079 ,  0xfe496263 ,  0x113c5b69 ,  0x93cd6620 ,  0x5efe823b ,  0x2d657b40 ,  0xb46dfc6c ,
    0x57710c69 ,  0xfe9fadeb ,  0xb5f8728a ,  0xe3224170 ,  0xca28b751 ,  0xfdabae56 ,  0x5ab12c3c ,  0xa697c457 ,
    0xd28fa2b7 ,  0x056579f2 ,  0x9fd9d810 ,  0xe3557478 ,  0xd88d89ab ,  0xa72a9422 ,  0x6d47abd0 ,  0x405bcbd9 ,
    0x6f83ebaf ,  0x13caec76 ,  0xfceb9ee2 ,  0x2e922df7 ,  0xce9856df ,  0xc05e9322 ,  0x2772c854 ,  0xb67f2a32 ,
    0x6d1af28d ,  0x3a78cf77 ,  0xdff411e4 ,  0x61c74ca9 ,  0xed8b842e ,  0x72880845 ,  0x6e857085 ,  0xc6404932 ,
    0xee37f6bc ,  0x27116f48 ,  0x5e9ec45a ,  0x8ea2a51f ,  0xa5573db7 ,  0xa746d036 ,  0x486b4768 ,  0x5b438f3b ,
    0x18c54a5c ,  0x64fcf08e ,  0xe993cdc1 ,  0x35c1ead3 ,  0x9de07de7 ,  0x321b841c ,  0x87423c5e ,  0x071aa0f6 ,
    0x962eb75b ,  0xbb06bdd2 ,  0xdcdb5363 ,  0x389752f2 ,  0x83d9cc88 ,  0xd014adc6 ,  0xc71121bb ,  0x2372f938 ,
    0xcaff2650 ,  0x62be8951 ,  0x56dccaff ,  0xac4084c0 ,  0x09712e95 ,  0x1d3c288f ,  0x1b085744 ,  0xe1d3cfef ,
    0x5c9a812e ,  0x6611fd59 ,  0x85e46044 ,  0x1981d885 ,  0x5a4c903f ,  0x43f30d4b ,  0x7d1d601b ,  0xdd3c3391 ,
    0x030ec65e ,  0xc12878cd ,  0x72e795fe ,  0xd0c76abd ,  0x1ec085db ,  0x7cbb61fa ,  0x93e8dd1e ,  0x8582eb06 ,
    0x73563144 ,  0x049d4e7e ,  0x5fd5aefe ,  0x7b842a00 ,  0x75ced665 ,  0xbb32d458 ,  0x4e83bba7 ,  0x8f15151f ,
    0x7795a125 ,  0xf0842455 ,  0x499af99d ,  0x565cc7fa ,  0xa3b1278d ,  0x3f27ce74 ,  0x96ca058e ,  0x8a497443 ,
    0xa6fb8cae ,  0xc115aa21 ,  0x17504923 ,  0xe4932402 ,  0xaea886c2 ,  0x8eb79af5 ,  0xebd5ea6b ,  0xc7980d3b ,
    0x71369315 ,  0x796e6a66 ,  0x3a7ec708 ,  0xb05175c8 ,  0xe02b74e7 ,  0xeb377ad3 ,  0x6c8c1f54 ,  0xb980c374 ,
    0x59aee281 ,  0x449cb799 ,  0xe01f5605 ,  0xed0e085e ,  0xc9a1a3b4 ,  0xaac481b1 ,  0xc935c39c ,  0xb7d8ce7f ]

    # bit matrix
    new = [0 for i in range(0,16)]
    for j in range(0, 16):
        for k in range(0, 16):
            new[j] = new[j] ^ (state[k] * bitmatrix[k][j])
        new[j] = new[j] & 0xffffffff # truncate to 32 bits, if necessary
    state = new

    # circulant multiplication
    for i in range(0, 16):
        acc = 0
        for j in range(0, 3):
            acc = acc ^ (state[i] << coefficients[i][j]) ^ (state[i] >> (32-coefficients[i][j]))
        state[i] = acc & 0xffffffff # truncate to 32 bits, if necessary

    # constants injection
    for i in range(0, 16):
        state[i] = state[i] ^ injection_constants[index*16 + i]

    # add / rotate / add
    for i in range(0, 8):
        state[2*i] = (state[2*i] + state[2*i+1]) & 0xffffffff # truncate to 32 bits, if necessary
        state[2*i] = (state[2*i] >> 24) ^ ((state[2*i] << 8) & 0xffffffff) # shift bytes
        state[2*i+1] = (state[2*i+1] >> 8) ^ ((state[2*i+1] << 24) & 0xffffffff) # shift bytes
        state[2*i+1] = (state[2*i] + state[2*i+1]) & 0xffffffff # truncate to 32 bits, if necessary

    return state

def EaglesongSponge( input_bytes, num_output_bytes, delimiter ):
    # parameters
    capacity = 256 # must be multiple of 32
    rate = 256 # must be multiple of 32

    state = [0 for i in range(0, 16)]

    # absorbing
    for i in range(0, ((len(input_bytes)+1)*8+rate-1) // rate):
        for j in range(0, rate//32):
            integer = 0
            for k in range(0, 4):
                if i*rate//8 + j*4 + k < len(input_bytes):
                    integer = (integer << 8) ^ input_bytes[i*rate//8 + j*4 + k]
                elif i*rate//8 + j*4 + k == len(input_bytes):
                    integer = (integer << 8) ^ delimiter
            state[j] = state[j] ^ integer

        state = EaglesongPermutation(state)

    # squeezing
    output_bytes = [0] * num_output_bytes
    for i in range(0, num_output_bytes//(rate//8)):
        for j in range(0, rate//32):
            for k in range(0, 4):
                output_bytes[i*rate//8 + j*4 + k] = (state[j] >> (8*k)) & 0xff

        state = EaglesongPermutation(state)

    return output_bytes

def EaglesongHash( input_bytes ):
    # just run the sponge (with delimiter 0x06 -- hashing mode) and truncate to 32 bytes == 256 bits
    return EaglesongSponge(bytearray(input_bytes), 32, 0x06)



================================================
File: rfcs/0010-eaglesong/hash.c
================================================
#include <stdio.h>
#include <stdint.h>

void EaglesongHash( unsigned char * output, const unsigned char * input, int input_length );

void main( int argc, char ** argv ) {
    unsigned char input[10000];
    unsigned char output[32];
    int c;
    int i;

    i = 0;
    while( 1 ) {
        c = getchar();
        if( c == EOF ) {
            break;
        }
        input[i] = c;
        i = i + 1;
    }

    EaglesongHash(output, input, i);

    for( i = 0 ; i < 32 ; ++i ) {
        printf("%02x", output[i]);
    }

    printf("\n");
}


================================================
File: rfcs/0010-eaglesong/hash.py
================================================
from eaglesong import EaglesongHash
import sys
from binascii import hexlify

lines = sys.stdin.readlines()
input_bytes = "\n".join(lines)
input_bytes = bytearray(input_bytes, "utf8")
output_bytes = EaglesongHash(input_bytes)
print(hexlify(bytearray(output_bytes)))





================================================
File: rfcs/0011-transaction-filter-protocol/0011-transaction-filter-protocol.md
================================================
---
Number: "0011"
Category: Standards Track
Status: Withdrawn
Author: Quake Wang <quake.wang@gmail.com>
Created: 2018-12-11
---

# Transaction Filter Protocol

## Abstract

Transaction filter protocol allows peers to reduce the amount of transaction data they send. Peer which wants to retrieve transactions of interest, has the option of setting filters on each connection. A filter is defined as a [Bloom filter](http://en.wikipedia.org/wiki/Bloom_filter) on data derived from transactions.

## Motivation

The purpose of transaction filter protocol is to allow low-capacity peers (smartphones, browser extensions, embedded devices, etc) to maintain a high-security assurance about the up to date state of some particular transactions of the chain or verify the execution of transactions.

These peers do not attempt to fully verify the block chain, instead just checking that [block headers connect](../0004-ckb-block-sync/0004-ckb-block-sync.md#connecting-header) together correctly and trusting that the transactions in the block of highest difficulty are in fact valid.

Without this protocol, peers have to download the entire blocks and accept all broadcast transactions, then throw away majority of the transactions. This slows down the synchronization process, wastes users bandwidth and increases memory usage.

## Messages

*Message serialization format is [Molecule](../0008-serialization/0008-serialization.md)*

### SetFilter

Upon receiving a `SetFilter` message, the remote peer will immediately restrict the transactions that it broadcasts to the ones matching the filter, where the [matching algorithm](#filter-matching-algorithm) is specified as below.

```
table SetFilter {
    filter: [uint8];
    num_hashes: uint8;
    hash_seed: uint32;
}
```

`filter`: A bit field of arbitrary byte-aligned size. The maximum size is 36,000 bytes.

`num_hashes`: The number of hash functions to use in this filter. The maximum value allowed in this field is 20. This maximum value and `filter` maximum size allow to store ~10,000 items and the false positive rate is 0.0001%.

`hash_seed`: We use [Kirsch-Mitzenmacher-Optimization](https://www.eecs.harvard.edu/~michaelm/postscripts/tr-02-05.pdf) hash function in this protocol, `hash_seed` is a random offset, `h1` is low uint32 of hash value, `h2` is high uint32 of hash value, and the nth hash value is `(hash_seed + h1 + n * h2) mod filter_size`.

### AddFilter

Upon receiving a `AddFilter` message, the given bit data will be added to the existing filter via bitwise OR operator. A filter must have been previously provided using `SetFilter`. This messsage is useful if a new filter is added to a peer whilst it has connections to the network open, alsp avoids the need to re-calculate and send an entirely new filter to every peer.

```
table AddFilter {
    filter: [uint8];
}
```

`filter`: A bit field of arbitrary byte-aligned size. The data size must be litter than or equal to previously provided filter size.

### ClearFilter

The `ClearFilter` message tells the receiving peer to remove a previously-set bloom filter.

```
table ClearFilter {
}
```

The `ClearFilter` message has no arguments at all.


### FilteredBlock

After a filter has been set, peers don't merely stop announcing non-matching transactions, they can also serve filtered blocks. This message is a replacement for `Block` message of sync protocol and `CompactBlock` message of relay protocol.

```
table FilteredBlock {
    header: Header;
    transactions: [IndexTransaction];
    hashes: [H256];
}

table IndexTransaction {
    index:                      uint32;
    transaction:                Transaction;
}
```

`header`: Standard block header struct.

`transactions`: Standard transaction struct plus transaction index.

`hashes`: Partial [Merkle](../0006-merkle-tree/0006-merkle-tree.md#merkle-proof) branch proof.

## Filter matching algorithm

The filter can be tested against all broadcast transactions, to determine if a transaction matches the filter, the following algorithm is used. Once a match is found the algorithm aborts.

1. Test the hash of the transaction itself.
2. For each CellInput, test the hash of `previous_output`.
3. For each CellOutput, test the `lock hash` and `type hash` of script.
4. Otherwise there is no match.


================================================
File: rfcs/0012-node-discovery/0012-node-discovery.md
================================================
---
Number: "0012"
Category: Standards Track
Status: Active
Author: Linfeng Qian <@thewawar>, JinYang Jiang <@jjyr>
Created: 2018-11-28
---

# CKB Node Discovery Protocol

CKB Node Discovery Protocol mainly refers to [Satoshi Client Node Discovery][0]. The differences between them are summarized below:

* The node version number is included in the `GetNodes` message.
* The `Nodes` message is used to periodically broadcast all nodes currently connected.
* We use `multiaddr` as the format of node addresses (It MUST NOT include `/p2p/` segment otherwise it's considered as *misbehavior* and a low score SHOULD be given.)

Every time client startup, if PeerStore's address list is empty, it SHOULD try to issue DNS requests to initialize address list. If DNS requests don't work it SHOULD fallback to the hard-coded address list.

## Discovery Methods
### DNS Addresses
At the first time startup (bootstrap stage), if the discovery service is needed, the local node SHOULD issues DNS requests to learn about the addresses of other peer nodes. The client includes a list of seed hostnames for DNS services.

### Hard-Coded "Seed" Addresses
The client contains some hard-coded "seed" IP addresses that represent CKB nodes. Those addresses are used only if all DNS requests fail. Once the local node has enough addresses (presumably learned from the seed nodes), the client SHOULD close seed node connections to avoid overloading those nodes.

"Seed" nodes are nodes that generally have a high uptime and have had many connections to many other nodes.

### Protocol Message
#### `GetNodes` Message
When all the following conditions are met, the local node will send a `GetNodes` message:

  1. It's an outbound connection (for resisting [fingerprinting attack][3]).
  2. The other node's version must bigger than a preset value.
  3. The number of addresses currently stored is less than `ADDRESSES_THRESHOLD` (default 1000).


#### `Nodes` Message
When the client receives a `GetNodes` request, it SHOULD return a `Nodes` message if this kind of reception is the first time and the connection is an inbound connection, the `announce` field is set to `false`. At regular intervals, local node SHOULD broadcast all connected `Node` information in `Nodes` message to all connected nodes, the `announce` field is set to `true`. When local node received a `Nodes` message and it's `announce` field is `true`, local node SHOULD relay those node addresses that are [routable][1].

The `announce` field here is to distinguish a `Nodes` as a response of `GetNodes` or a broadcast message, so it's convenient to apply different rules for punishing misbehaviors. The main rules:

* A node can only send one `Nodes` message (announce=false) as a response of `GetNodes` message.
* Among a node's broadcast messages only the first `Nodes` message (announce=true) can include more than `ANNOUNCE_THRESHOLD` (default 10) node information, in case other peers send too many node information.

The number of `addresses` field of each `Node` in all `Nodes` messages cannot exceed `MAX_NODE_ADDRESSES` (default 3).

## Resist Typical Attacks
### Fingerprinting Attack
[Related paper][3]

`GetNodes` can only send to an outbound connection.

## Data Structures
We use [Molecule][2] as serialize/deserialize format, the *schema*:

```
array Bool [byte; 1];
array Uint16 [byte; 2];
array Uint32 [byte; 4];
option PortOpt (Uint16);
vector NodeVec <Node>;
vector BytesVec <Bytes>;

table DiscoveryMessage {
    payload: DiscoveryPayload,
}

union DiscoveryPayload {
    GetNodes,
    Nodes,
}

table GetNodes {
    version: Uint32,
    count: Uint32,
    listen_port: PortOpt,
}

table Nodes {
    announce: Bool,
    items: NodeVec,
}

table Node {
    addresses: BytesVec,
}
```

## Flow Diagram
### Node Bootstrap
![](images/bootstrap.png)
### Send `GetNodes` Message
![](images/get-nodes.png)
### Announce Connected Nodes
![](images/announce-nodes.png)

[0]: https://en.bitcoin.it/wiki/Satoshi_Client_Node_Discovery
[1]: https://www.iana.org/assignments/iana-ipv4-special-registry/iana-ipv4-special-registry.xhtml
[2]: ../0008-serialization/0008-serialization.md
[3]: https://arxiv.org/pdf/1410.6079.pdf


================================================
File: rfcs/0012-node-discovery/0012-node-discovery.zh.md
================================================
---
Number: "0012"
Category: Standards Track
Status: Active
Author: Linfeng Qian <@thewawar>, JinYang Jiang <@jjyr>
Created: 2018-11-28
---

# CKB 节点发现协议

CKB 节点发现协议主要参考了[比特币的协议][0]。主要不同点如下:
* 节点版本号包含在 `GetNodes` 消息中
* 通过 `Nodes` 消息来定时广播当前连接的所有节点
* 我们使用 `multiaddr` 作为节点地址的格式 (不允许出现 `/p2p/` 段，如果违反会被认为是*不良*行为并被打低分)

每次客户端启动时，如果 PeerStore 中的地址列表为空就会尝试通过 DNS 的方式获取初始地址，如果 DNS 的方式失败了就使用硬编码的种子地址来初始化地址列表。

## 节点发现的手段
### DNS 获取地址
第一次启动的时候(引导阶段)，如果需要节点发现服务，客户端会尝试向内置的 DNS 服务器发送 DNS 请求来获取种子服务器地址。

### 硬编码的「种子」地址
客户端会硬编码一些「种子」节点地址，这些地址只有在 DNS 获取地址失败的时候被使用。当通过这些种子节点获取了足够多的地址后需要断开这些连接，防止它们过载。这些「种子」地址的时间戳被设置为 0 所以不会加入到 `GetNodes` 请求的返回值中。

「种子」节点是那些在线时间较长而且和很多其它节点互连的节点。

### 协议消息

#### `GetNodes` 消息
当满足所有以下条件时，节点会发送一个 `GetNodes` 请求：

  1. 这个连接是自己主动发起的 (防御[指纹攻击][3])
  2. 对方的版本号大于一个预设的值
  3. 当前存储的地址数量小于 `ADDRESSES_THRESHOLD` (默认 1000) 个

#### `Nodes` 消息

当客户端收到一个 `GetNodes` 请求时，如果是第一次收到 `GetNodes` 消息而且这个连接是对方主动发起的就会返回一个 `Nodes` 消息，该 `Nodes` 消息的 `announce` 字段为 `false`。每隔一定时间当前节点会将当前连接的节点信息以及本节点信息以 `Nodes` 消息广播给当前连接的所有节点，`announce` 字段为 `true`。当前收到 `announce` 字段为 `true` 的 `Nodes` 消息时会对地址[可路由][1]的那些节点地址进行转发。

这里 `announce` 字段的目的是为了区分 `Nodes` 消息是作为 `GetNodes` 消息的返回值还是广播消息，可以方便应用不同的规则来对节点的恶意行为做相应的处罚。涉及到的规则主要有:

* 一个节点只能有一个 `Nodes` 消息 (announce=false) 作为 `GetNodes` 消息的返回值。
* 一个节点的广播消息中只能第一个 `Nodes` 消息 (announce=true) 包含的节点信息数量超过 `ANNOUNCE_THRESHOLD` (默认 10) 个，这是为了防止其它节点发送过多的 `Node` 信息。

所有 `Nodes` 消息中的每个 `Node` 中的 `addresses` 的数量不能超过 `MAX_NODE_ADDRESSES` (默认 3) 个。

## 对主要攻击方式的处理
### 指纹攻击 (fingerprinting attack)
[相关论文][3]

`GetNodes` 消息只能通过 outbound 连接发送出去。

## 相关数据结构
我们使用 [Molecule][2] 作为数据序列化格式，以下为相关数据结构的 schema:

```
array Bool [byte; 1];
array Uint16 [byte; 2];
array Uint32 [byte; 4];
option PortOpt (Uint16);
vector NodeVec <Node>;
vector BytesVec <Bytes>;

table DiscoveryMessage {
    payload: DiscoveryPayload,
}

union DiscoveryPayload {
    GetNodes,
    Nodes,
}

table GetNodes {
    version: Uint32,
    count: Uint32,
    listen_port: PortOpt,
}

table Nodes {
    announce: Bool,
    items: NodeVec,
}

table Node {
    addresses: BytesVec,
}
```

## 流程图
### 节点 Bootstrap
![](images/bootstrap.png)
### 发送 `GetNodes` 消息
![](images/get-nodes.png)
### 广播当前连接的节点信息
![](images/announce-nodes.png)

[0]: https://en.bitcoin.it/wiki/Satoshi_Client_Node_Discovery
[1]: https://www.iana.org/assignments/iana-ipv4-special-registry/iana-ipv4-special-registry.xhtml
[2]: ../0008-serialization/0008-serialization.md
[3]: https://arxiv.org/pdf/1410.6079.pdf


================================================
File: rfcs/0013-get-block-template/0013-get-block-template.md
================================================
---
Number: "0013"
Category: Standards Track
Status: Active
Author: Dingwei Zhang <zhangsoledad@gmail.com>
Created: 2019-01-02
---

# get_block_template

## Abstract

This RFC describes the decentralized CKB mining protocol.


## Motivation

The original `get_work` [[btc][1] [eth][2]] mining protocol simply issues block headers for a miner to solve, the miner is kept in the dark, and has no influence over block creation. `get_block_template` moves block creation to the miner, the entire block structure is sent, and left to the miner to (optionally) customize and assemble, miner are enabled to audit and possibly modify the block before hashing it, this improves the security of the CKB network by making blocks decentralized.

## Specification

### Block Template Request

A JSON-RPC method is defined, called `get_block_template`. It accepts exactly three argument:

| Key          | Required | Type   | Description                                         |
| ------------ | -------- | ------ | --------------------------------------------------- |
| cycles_limit | No       | Number | maximum number of cycles to include in template     |
| bytes_limit  | No       | Number | maximum number of bytes to use for the entire block |
| max_version  | No       | Number | highest block version number supported              |

For `cycles_limit`, `bytes_limit` and `max_version`, if omitted, the default limit (consensus level) is used.
Servers SHOULD respect these desired maximums (if those maximums exceed consensus level limit, Servers SHOULD instead return the consensus level limit), but are NOT required to, clients SHOULD check that the returned template satisfies their requirements appropriately.

`get_block_template` MUST return a JSON Object containing the following keys:

| Key                   | Required | Type             | Description                                                                  |
| --------------------- | -------- | ---------------- | ---------------------------------------------------------------------------- |
| version               | Yes      | Number           | block version                                                                |
| difficulty            | Yes      | String           | difficulty in hex-encoded string                                             |
| current_time          | Yes      | Number           | the current time as seen by the server (recommended for block time)          |
| number                | Yes      | Number           | the number of the block we are looking for                                   |
| parent_hash           | Yes      | String           | the hash of the parent block, in hex-encoded string                          |
| cycles_limit          | No       | Number           | maximum number of cycles allowed in blocks                                   |
| bytes_limit           | No       | Number           | maximum number of bytes allowed in blocks                                    |
| commit_transactions   | Should   | Array of Objects | objects containing information for CKB transactions (excluding cellbase)     |
| proposal_transactions | Should   | Array of String  | array of hex-encoded transaction proposal_short_id                           |
| cellbase              | Yes      | Object           | information for cellbase transaction                                         |
| work_id               | No       | String           | if provided, this value must be returned with results (see Block Submission) |

#### Transaction Object

The Objects listed in the response's "commit_transactions" key contains these keys:

| Key      | Required | Type             | Description                                                                                                                                                                                                                       |
| -------- | -------- | ---------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| hash     | Yes      | String           | the hash of the transaction                                                                                                                                                                                                       |
| required | No       | Boolean          | if provided and true, this transaction must be in the final block                                                                                                                                                                 |
| cycles   | No       | Number           | total number of cycles, if key is not present, cycles is unknown and clients MUST NOT assume there aren't any                                                                                                                     |
| depends  | No       | Array of Numbers | other transactions before this one (by 1-based index in "transactions" list) that must be present in the final block if this one is; if key is not present, dependencies are unknown and clients MUST NOT assume there aren't any |
| data     | Yes      | String           | transaction [Molecule][3] bytes in  hex-encoded string                                                                                                                                                                            |

### Block Submission

A JSON-RPC method is defined, called `submit_block`. to submit potential blocks (or shares). It accepts two arguments: the first is always a String of the hex-encoded block [Molecule][3] bytes to submit; the second is String of work_id.

| Key     | Required | Type   | Description                                                           |
| ------- | -------- | ------ | --------------------------------------------------------------------- |
| data    | Yes      | String | block [Molecule][3] bytes in  hex-encoded string                      |
| work_id | No       | String | if the server provided a workid, it MUST be included with submissions |

### References

* bitcoin Getwork, https://en.bitcoin.it/wiki/Getwork
* ethereum Getwork, https://github.com/ethereum/wiki/wiki/JSON-RPC#eth_getwork
* [Molecule Encoding][3]

[1]: https://en.bitcoin.it/wiki/Getwork
[2]: https://github.com/ethereum/wiki/wiki/JSON-RPC#eth_getwork
[3]: ../0008-serialization/0008-serialization.md


================================================
File: rfcs/0014-vm-cycle-limits/0014-vm-cycle-limits.md
================================================
---
Number: "0014"
Category: Standards Track
Status: Active
Author: Xuejie Xiao <xxuejie@gmail.com>
Created: 2019-01-04
---

# VM Cycle Limits

## Introduction

This RFC describes cycle limits used to regulate VM scripts.

CKB VM is a flexible VM that is free to implement many control flow constructs, such as loops or branches. As a result, we will need to enforce certain rules in CKB VM to prevent malicious scripts, such as a script with infinite loops.

We introduce a concept called `cycles`, each VM instruction or syscall will consume some amount of cycles. At consensus level, a scalar `max_block_cycles` field is defined so that the sum of cycles consumed by all scripts in a block cannot exceed this value. Otherwise, the block will be rejected. This way we can guarantee all scripts running in CKB VM will halt, or result in error state.

## Consensus Change

As mentioned above, a new scalar `max_block_cycles` field is added to chain spec as a consensus rule, it puts a hard limit on how many cycles a block's scripts can consume. No block can consume cycles larger than `max_block_cycles`.

Note there's no limit on the cycles for an individual transaction or a script. As long as the whole block consumes cycles less than `max_block_cycles`, a transaction or a script in that block are free to consume how many cycles they want.

## Cycle Measures

Here we will specify the cycles needed by each CKB VM instructions or syscalls. Note right now in the RFC, we define hard rules for each instruction or syscall here, in future this might be moved into consensus rules so we can change them more easily.

The cycles consumed for each operation are determined based on the following rules:

1. Cycles for RISC-V instructions are determined based on real hardware that implement RISC-V ISA.
2. Cycles for syscalls are measured based on real runtime performance metrics obtained while benchmarking current CKB implementation.

### Initial Loading Cycles

For each byte loaded into CKB VM in the initial ELF loading phase, 0.25 cycles will be charged. This is to encourage dapp developers to ship smaller smart contracts as well as preventing DDoS attacks using large binaries. Notice fractions will be rounded up here, so 30.25 cycles will become 31 cycles.

### Instruction Cycles

All CKB VM instructions consume 1 cycle except the following ones:

| Instruction | Cycles               |
|-------------|----------------------|
| JALR        | 3                    |
| JAL         | 3                    |
| J           | 3                    |
| JR          | 3                    |
| BEQ         | 3                    |
| BNE         | 3                    |
| BLT         | 3                    |
| BGE         | 3                    |
| BLTU        | 3                    |
| BGEU        | 3                    |
| BEQZ        | 3                    |
| BNEZ        | 3                    |
| LD          | 2                    |
| SD          | 2                    |
| LDSP        | 2                    |
| SDSP        | 2                    |
| LW          | 3                    |
| LH          | 3                    |
| LB          | 3                    |
| LWU         | 3                    |
| LHU         | 3                    |
| LBU         | 3                    |
| SW          | 3                    |
| SH          | 3                    |
| SB          | 3                    |
| LWSP        | 3                    |
| SWSP        | 3                    |
| MUL         | 5                    |
| MULW        | 5                    |
| MULH        | 5                    |
| MULHU       | 5                    |
| MULHSU      | 5                    |
| DIV         | 32                   |
| DIVW        | 32                   |
| DIVU        | 32                   |
| DIVUW       | 32                   |
| REM         | 32                   |
| REMW        | 32                   |
| REMU        | 32                   |
| REMUW       | 32                   |
| ECALL       | 500 (see note below) |
| EBREAK      | 500 (see note below) |

### Syscall Cycles

As shown in the above chart, each syscall will have 500 initial cycle consumptions. This is based on real performance metrics gathered benchmarking CKB implementation, certain bookkeeping logics are required for each syscall here.

In addition, for each byte loaded into CKB VM in the syscalls, 0.25 cycles will be charged. Notice fractions will also be rounded up here, so 30.25 cycles will become 31 cycles.

## Guidelines

In general, the cycle consumption rules above follow certain guidelines:

* Branches are more expensive than normal instructions.
* Memory accesses are more expensive than normal instructions. Since CKB VM is a 64-bit system, loading 64-bit value directly will cost less cycle than loading smaller values.
* Multiplication and divisions are much more expensive than normal instructions.
* Syscalls include 2 parts: the bookkeeping part at first, and a plain memcpy phase. The first bookkeeping part includes quite complex logic, which should consume much more cycles. The memcpy part is quite cheap on modern hardware, hence less cycles will be charged.

Looking into the literature, the cycle consumption rules here resemble a lot like the performance metrics one can find in modern computer archtecture.


================================================
File: rfcs/0015-ckb-cryptoeconomics/0015-ckb-cryptoeconomics.md
================================================
---
Number: "0015"
Category: Informational
Status: Final
Author: Kevin Wang <@knwang>, Jan Xie <jan@cryptape.com>, Jiasun Li <@mysteryfigure>, David Zou <chwzou@gmail.com>
Created: 2019-03-08
---
# Crypto-Economics of the Nervos Common Knowledge Base

## 1. The Objectives of a Token Economics Design

Public permission-less blockchains are open and distributed systems with diverse groups of participants. A well-designed crypto-economics model is to provide incentives so that participants' pursuit of own economic interests leads to desired emergent behaviors in alignment with the protocol, to contribute to the blockchain network's success.

More specifically, the design of a crypto-economic system must provide answers to the following questions:

- How can the economic model ensure the security of the protocol?
- How can the economic model ensure long term sustainability of the protocol?
- How can the economic model align the objectives of different actors to grow the value of the protocol network?

## 2. The Crypto-economics Design of Bitcoin

The Bitcoin protocol uses its native currency to incentivize miners to validate and produce blocks. The Nakamoto Consensus considers the longest chain as the valid chain, which encourages block producing miners to propagate new blocks as soon as they produce them and validate blocks as soon as they receive them. This ensures that the whole network achieves consensus on the global state.

The native tokens of the Bitcoin network function both as a utility token and an asset. When bitcoins function as a utility, they represent a "Medium of Exchange" (MoE) and can be used to pay transaction fees; when they function as an asset, they represent a "Store of Value" (SoV) and can be used to preserve value over time. The two use cases are not mutually exclusive. They are both important for the network to function. However, it's important to study the economic motives of the users of both use cases as a guide to analyze the sustainability of the Bitcoin network.

The Bitcoin protocol constrains the network's transaction throughput by using a fixed block size limit. Users bid with fees on the limited throughput to have their transactions processed. With this auction like mechanism, transaction fees are determined by the transaction demand - the more demand there is on the network, the higher the transaction fee a user has to pay to beat the competition and have their transaction included in the block.

### Bitcoin as a Medium of Exchange Network

The Medium of Exchange use case views the Bitcoin network primarily as a peer to peer value transfer network. MoE users don't have to hold bitcoins to benefit from the network - it's the transactions in themselves that provide value. In fact, there are specialized Bitcoin payment services to provide access to liquidity and allow senders and receivers to acquire and dispose of Bitcoins just in time to perform the value transfer, without having to hold the cryptocurrency. MoE users are not concerned with price or the movement of price but care about the fiat equivalent cost of the transaction fees.

It's challenging for Bitcoin to become a dominant MoE network. If the protocol calibrates its block time and the block size limit, thereby fixing the supply of transactions, the success of the network will necessarily increase the cost of transactions and reduce its competitiveness among other similar purposed blockchains as well as its own forks; If the protocol aims to keep the transaction cost low and increase the supply of transactions with faster block time or bigger blocks, it could compromise both security and decentralization through higher fork rate and increased cost of consensus participation.


### Bitcoin as a Store of Value Network

Store of Value users view the Bitcoin network as a protocol to provide security to its native cryptocurrency as an asset that can preserve value over time. They see the Medium of Exchange use case as the necessary function to go in and out of this asset. A store of value user, especially the ones who hold the cryptocurrency for a long time, doesn't care much about the transaction cost, as they can amortize it over time. They do care about the value of a Bitcoin, which depends on the network's security and decentralization - if the network becomes less secure and can be attacked easily, it'll stop being perceived as a store of value and the tokens will lose value; if the network becomes centralized, Bitcoin as an asset no longer has independent value, but has to assume counter-party risk.

For Bitcoin to succeed as an SoV network, it must continue to keep its monetary policy stable and its network secure and decentralized. However, Bitcoin's monetary policy has a hard cap, and after all the coins are mined, the network can only pay for the miners with transaction fees. It's still an open question whether this model could be sustainable, especially considering Store of Value networks themselves tend not to produce many transactions.

### Who Compensates the Miners Over the Long Run?


Security and decentralization are two essential properties of a blockchain network, and they come with a high cost that must be paid to the operators of the network. Bitcoin's current model has network security entirely paid with transaction fees after all the coins are mined. However, the MoE users have very limited time exposure to the network's security risk, therefore won't be willing to pay for it;  the SoV users have prolonged exposure to the network's security risk and are willing to pay for it, but they produce nearly no transactions.

Bitcoin's consensus mechanism incentivizes miners to recognize the longest chain as the network's canonical state. Miner's ongoing supply of hashing power doesn't only provide security for the current block, but the immutability of all the blocks before it on the canonical chain. Relying on the SoV users to make one time payments for the ongoing security protection they receive from miners is not sustainable.

In an SoV network, relying on inflation to fund network security is more incentive compatible with the users. An inflation based block reward mechanism represents indirect payments from the beneficiaries of the network's ongoing security to the providers of such security, in proportion to the duration that they enjoy the service.

## 3. Preservational and Transactional Smart Contract Platforms

Smart contract platforms like Ethereum come with Turing-complete programmability and can support a much wider variety of use cases. The native tokens are typically used to price and pay for the cost of decentralized computation. Like the Bitcoin network, smart contract platforms also have the dual functions of preserving value and performing transactions. They differ from the payment networks in that the value they preserve is not only their own native tokens but also the internal states of decentralized applications, for example, crypto-assets ownership in ERC20 smart contracts.

Another significant difference is that transactions on smart contract platforms are much more "portable". It's much easier to take advantage of the more advanced scripting capability of smart contract platforms to develop interoperability protocols to move transactions to a more cost-effective transactional blockchain and then securely settle back to the main "system of record" blockchains.

The economic models of smart contract platforms face similar polarization tendency of payment networks. With their superior interoperable capabilities, smart contract platforms are going to be even more specialized into transactional platforms and preservation platforms.  Economically, this bifurcation comes from the fact that the two use cases have different ways of utilizing system resources - transactions consume instantaneous but renewable computation and bandwidth resources, and preservation requires long term occupation of the global state. An economic model optimized for one is unlikely to be optimal for the other.

Competitive transactional platforms need to prioritize for low transaction cost. Transactional users are willing to accept less-optimal security, because of their only moment-in-time, limited exposure to security risk. They're willing to accept the possibility of censored transactions, as long as there are options to take their transactions elsewhere. A transactional platform that invests in either security or censorship resistance will have higher cost of transactions, reflected either with higher transaction fees or high capital cost for stakes in a "stake for access" model, making the network less competitive. This is especially true when a well-designed inter-blockchain protocol can allow trust-less state transfers and fraud repudiation of transactions. We already see examples of transactional users prioritizing cost over security in centralized crypto-asset exchanges and not-so-decentralized blockchains - despite their flaws, they're still popular because of their transactional efficiency.

Competitive preservation platforms need to be sustainably secure and censorship-resistant. It requires an economic model designed not around transactions that happen moment-in-time, but around the ongoing occupation of the global state, and have users pay for the network infrastructure metered in their consumption of this critical resource.

## 4. Store of Assets

One of the most important use cases for smart contract platforms is to issue tokens to represent ownership of assets. These crypto-assets can have their own communities and markets, and their values are independent of the value of their platform tokens. On the other hand, these assets depend on the platform to process transactions and provide security. Payment networks like Bitcoin can be seen as single asset platforms, where smart contract platforms are multi-asset platforms. Similar to the concept of "Store of Value" in the context of Bitcoin, we call the utility that smart contract platforms preserve the value of its crypto-assets "Store of Assets".

Preservation focused smart contract platforms must have a Store of Assets token economics design. The level of platform security has to grow along with the asset value it preserves. Otherwise, as asset value grows, it will be increasingly profitable to "double-spend" assets by attacking the consensus process of the platform.

None of the current smart contract platforms are designed as Store of Assets platforms. Their token economics are designed either to facilitate transactions (for example, Ethereum's native tokens are to pay for the decentralized computation) or to fulfill staking requirements. In either case, the growth in asset value doesn't necessarily raise miner's income to provide more security.

Every multi-asset platform is an ecosystem of independent projects. The security of the platform can be seen as "public goods" that benefit all projects. To make the ecosystem sustainable from a security point of view, there has to be a clear mechanism that the platform captures the economic success of the ecosystem to raise its own level of security. In other words, a Store of Assets platform has to be able to translate the demand of crypto-assets to the revenue of its miners, often through raising the value of the native tokens with which the miners are compensated. Otherwise, the platform's level of security becomes the ceiling of assets' value. When the value of an asset rises such that typical transactions can no longer be sufficiently protected by the platform, the liquidity would dry up and the demand of the asset would fade.

Decentralized multi-assets smart contract platforms have to be Store of Assets to be sustainable.

## 5. Decentralization and the Need for Bounded State

Like other long term store of value systems, a Store of Assets platform has to be neutral and free of risks of censorship and confiscation. These are the properties that made gold the world's favorite the store of value for thousands of years. For open, permission-less blockchain networks, censorship resistance comes down to having the broadest consensus scope with a low barrier for consensus and full node participation. Compared to payment networks, running a full node for a smart contract system is more resource intensive. Therefore a Store of Assets platform must take measures to protect the operating cost of full nodes to keep the network sufficiently decentralized.

Both Bitcoin and Ethereum throttle transaction throughput to ensure participation is not limited to only "super computers" - Bitcoin throttles on bandwidth and Ethereum throttles on computation. However, they haven't taken effective measures to contain the ever growing global state necessary for consensus participation and independent transaction validation.  This is especially a centralization force for high throughput smart contract platforms, where the global state grows even faster.

In Bitcoin, the global state is the UTXO set, and its growth rate is effectively capped with the block size limit. Users are encouraged to create UTXOs efficiently, since every new UTXO adds overhead to the transaction where it's created, making the transaction more expensive. However, once a UTXO is created, it doesn't cost anything to have it occupy the global state forever.

In Ethereum, the global state is represented with the EVM's state trie, the data structure that contains the balances and internal states of all accounts. When new accounts or new contract values are created, the size of the global state expands. Ethereum charges fixed amounts of Gas for inserting new values into its state storage and offers fixed amounts of Gas as transaction refund when values are removed. Ethereum's approach is a step in the right direction, but still has several issues:


- Neither the size nor the growth rate of the global state is bounded, this gives very little certainty in the cost of full node participation.
- The system raises one-time revenue for expanding the state storage, but miners and full nodes have to bear the cost of storage over time.
- There's no obvious reason why the cost of expanding storage should be priced in fixed Gas amounts, which is designed as measurement to price units of computation.
- The "pay once, occupy forever" state storage model gives very little incentive for users to voluntarily clear state, and do so sooner than later.

The Ethereum community is actively working on this problem, and the leading solution is to charge smart contract "state rent" - contracts have to periodically pay fees based on the size of its state. If the rent is not paid, the contract goes to "hibernation" and is not accessible before the payment is current again. We see several difficult-to-solve problems with this approach:


- Many contracts, especially popular ERC20 contracts, represent decentralized communities and express asset ownership of many users. It's a difficult problem to coordinate all the users to pay for state rent in a fair and efficient way.
- Even if a contract is current on its rent payment, it still may not be fully functional because some of its dependent contracts may be behind on their payments.
- The user experience for contracts with state rent is sub-optimal

We believe a well-designed mechanism to regulate the state storage has to be able to achieve the following goals:


- The growth of the global state has to be bounded to give predictability for full node participation. Ideally, the cost is well within the range of non-professional participants to keep the network maximally decentralized. Keeping this barrier low allows participants of the decentralized network to verify history and state independently, without having to trust a third party or service. This is fundamentally the reason why public blockchains are valuable.
- With bounded growth of the global state, the price for expanding it and the rewards for reducing it should be determined by the market. In particular, it's desirable to have the cost of expanding state storage higher when it's mostly full, and lower when it's mostly empty.
- The system has to be able to continuously raise revenue from its state users to pay miners for providing this resource. This serves both purposes of balancing miner's economics and providing incentives for users to clear unnecessary states sooner than later.

Just like how Bitcoin throttles and forces pricing on bandwidth and Ethereum throttles and forces pricing on computation, to keep a blockchain network long term decentralized and sustainable, we have to come up with a way to constrain and price the global state. This is especially important for preservation focused, Store of Assets networks, where usage of the network is not about transactions that mostly happen off-chain, but ongoing occupation of the global state.

## 6. The Economic Model of the Nervos Common Knowledge Base

The Nervos Common Knowledge Base (Nervos CKB for short) is a preservation focused, "Store of Assets" blockchain. Architecturally, it's designed to best support on-chain state and off-chain computation; economically, it's designed to provide sustainable security and decentralization.  Nervos CKB is the base layer of the overall Nervos Network.


### Native Tokens

The native token for the Nervos CKB is the "Common Knowledge Byte", or "CK Byte" for short. The CK Bytes represent cell capacity in bytes, and they give owners the ability to occupy a piece of the blockchain's overall global state. For example, if Alice owns 1000 CK Bytes, she can create a cell with 1000 bytes in capacity, or multiple cells that add up to 1000 bytes in capacity. She can use the 1000 bytes to store assets, application state, or other types of common knowledge.

A cell's occupied capacity could be equal to or less than its specified capacity. For example, for a 1000 byte cell, 4 bytes would be used to specify its own capacity, 64 bytes for the lock script and 128 bytes for storing state. Then the cell's current occupied capacity is 196 bytes, but with room to grow up to 1000 bytes.

The smallest unit of the native token is "CK Shannon": `1 CK Byte = 100_000_000 CK Shannons`.
"CK Shannon" is the indivisible unit.
"CK Shannon" is designed for the scenes that people want to transfer value less than one "CK Byte".

### Token Issuance

There are two types of native token issuance. The "base issuance" has a finite total supply with a Bitcoin like issuance schedule - the number of base issuance halves approximately every 4 years until all the base issuance tokens are mined out. All base issuance tokens are rewarded to the miners as incentives to protect the network.

The "secondary issuance" is designed to collect state rent, and has issuance amount that is constant over time. After base issuance stops, there will only be secondary issuance.


### Collecting State Rent with Secondary Issuance and the NervosDAO

Since the native tokens represent right to expand the global state, the issuance policy of the native tokens bounds the state growth. As state storage is bounded and becomes a scarce resource like bandwidth in Bitcoin and computation throughput in Ethereum, they can be market priced and traded. State rent adds the necessary time dimension to the fee structure of state storage occupation. Instead of mandating periodic rent payments, we use a two-step approach as a "targeted inflation" scheme to collect this rent:


- On top of the base issuance, we add the secondary issuance which can be seen as "inflation tax" to all existing token holders. For users who use their CK Bytes to store state, this recurring inflation tax is how they pay state rent to the miners.
- However, we would have also collected rent from the CK Bytes that are not used to store state, and we need to return to them what we collected. We allow those users to deposit and lock their native tokens into a special contract called the NervosDAO. The NervosDAO receives part of the "secondary issuance" to make up for the otherwise unfair dilution.

Let's suppose at the time of a secondary issuance event, 60% of all CK Bytes are used to store state, 35% of all CK Bytes are deposited and locked in the NervosDAO, and 5% of all CK Bytes are kept liquid. Then 60% of the secondary issuance goes to the miners, 35% of the issuance goes to the NervosDAO to be distributed to the locked tokens proportionally. The use of the rest of the secondary issuance - in this example, 5% of the that issuance - is determined by the community through the governance mechanism. Before the community can reach agreement, this part of the secondary issuance is going to be burned.

For long term token holders, as long as they lock their tokens in the NervosDAO, the inflationary effect of secondary issuance is only nominal. For them it's as if the secondary issuance doesn't exist, and they're holding hard-capped tokens like Bitcoin.


### Miner Compensation

Miners are compensated with both block rewards and transaction fees. They receive all the base issuance, and part of the secondary issuance. In the long term when base issuance stops, miners still receive state rent income that's independent of transactions but tied to the adoption of the common knowledge base.


### Paying for Transaction Fees

A decentralized blockchain network's transaction capacity is always limited. Transaction fees serve the dual purposes of establishing a market for the limited transaction capacity and as protection against spams. In Bitcoin, transaction fees are expressed with the difference between the outputs and inputs; In Ethereum, the user specify the per computation unit price they're willing to pay with `gasprice`, and use `gaslimit` to establish a budget for the entire transaction.

To ensure decentralization, the Nervos CKB restricts both computation and bandwidth throughput, effectively making it an auction for users to use those system resources. When submitting a transaction, the user can leave the total input cell capacities exceeding the total output cell capacities, leaving the difference as transaction fees expressed in the native tokens, payable to the miner that creates the block containing the transaction.

The number of units of computation (called "cycles") are added to the peer-to-peer messages between the full nodes. When producing blocks, miners order transactions based on both transaction fees and the number of computation cycles necessary for transaction validation, maximizing its per-computation-cycle income within the computation and bandwidth throughput restrictions.

In the Nervos CKB, the transaction fees can be paid with the native tokens, user defined tokens or a combination of both.

### Paying for Transaction Fees with User Defined Tokens

Users are also free to use other tokens (for example, stable coins) to pay transactions fees, a concept known as "Economic Abstraction". Note that even without explicit protocol support, it's always possible to have users make arrangements with miners to pay transaction fees in other tokens outside of the protocol. This is often seen as a threat for many platforms - if the platform's native tokens are purely to facilitate transactions, this would take away its intrinsic value and cause a collapse.

With the Nervos CKB, economic abstraction is possible because the payment methods are not hard-coded in transactions. We embrace economic abstraction and the benefits it brings. Since the intrinsic value of the native tokens is based not on transaction payment, economic abstraction doesn't pose a threat to the stability of our economic model. We do expect, however, the native tokens themselves are going to be the payment method of choice for vast majority of users and use cases - the native tokens are going to be the most widely held tokens in the Nervos ecosystem, and everyone who owns assets necessarily owns the Nervos natives tokens as state storage capacity that the assets occupy.

For more a more detailed analysis on transaction payments, please see Appendix 1.

## 7. An Economic Model Designed for Preservation

The economic model of the Nervos CKB is designed specifically to preserve assets and other types of common knowledge. Let's bring back the 3 high level design goals and examine our design in this context:


- How can the economic model ensure the security of the protocol?
- How can the economic model ensure long term sustainability of the protocol?
- How can the economic model align the objectives of different actors to grow the value of the protocol network?


### Security and Sustainability of the Protocol

The main design choices we made to ensure security of the Nervos CKB as a "Store of Assets" protocol are:


- Our native tokens represent claim to capacity in the state storage. This means the demand to holding assets on the platform directly puts demand on owning the native tokens. This creates an effective value capture mechanism into the native tokens from the assets they preserve. We claim that this is the only sustainable way that a "Store of Assets" platform can grow its security budget over time, instead of entirely basing it on speculation and altruism.
- The secondary issuance makes sure miner compensation is predictable and based on preservation demand instead of transactional demand. It also eliminates potential incentive incompatibility of the Nakamoto Consensus nodes after block reward stops. This is also important in a future when most transactions move to the layer 2, leaving a starved layer 1.
- The NervosDAO serves as the counter-force to the inflationary effects of secondary issuance, to ensure long term token holders are not diluted by this issuance.

For a purpose of keeping the network decentralized and censorship resistant, we believe it's important to limit the resource requirements of consensus and full nodes. We protect the operating cost of nodes by regulating the throughput of computation and bandwidth, similar to how it's accomplished with Bitcoin and Ethereum. We regulate the state storage with a combination of a "cap and trade" pricing scheme and opportunity cost based cost model for storage users.


### Aligning the Interests of Network Participants

In a typical smart contract platform, participants of the network have different interests - users want cheaper transactions, developers want adoption of their applications, miners want higher income, and holders want appreciation of their tokens. Those interests are not well aligned, and oftentimes in conflict - for example, more adoption won't give cheaper transactions (they'll be more expensive as more demand is put on the blockchain); cheaper transactions won't give more income to the miners; higher token price won't help with transaction cost (the opposite could happen if users don't adjust their local transaction fee setting). Decentralized computation platforms provide value through processing transactions. The price of their tokens doesn't materially change the intrinsic value of the network. For example, Ether's price doubling doesn't increase or decrease Ethereum's intrinsic value as a decentralized computation platform, because the introduction of Gas in the first place is to de-couple the price of computations from the price actions of Ether the cryptocurrency. This makes token holders of Ethereum only take the role of a speculator, instead of active contributors that can increase the value of the network.

In the Nervos CKB, Store of Assets users want security of their assets; developers want more adoption, reflected in more assets preserved; miners want higher income and token holders want price appreciation of their tokens. Higher token price supports everyone's objective - the network would be more secure, miners get higher income, and token holders get better return.

Aligning all participants' incentives allows the network to best harness network effects to grow its intrinsic value. It also produces a more cohesive community and makes the system less prone to governance challenges.

### Bootstrapping Network Effect and Network Growth

As the network grows to secure more assets and common knowledge, more native tokens of the Nervos CKB are going to become occupied. This accrues value to the native tokens by reducing circulating supply and providing positive support to the market price of the tokens. The higher price and increased share of secondary issuance motivate miners to expand operations and make the network more secure, increasing the intrinsic value of the network and the native tokens, attracting more and higher value preservation usage.

The pro-cyclical loop of the network's adoption and network's intrinsic value provides a powerful growth engine for the network. Combined with how the network's value accrues to the native tokens and gets captured by long term holders, it makes the network's native token an excellent candidate for store of value. Compared to Bitcoin as a monetary store of value, the Nervos CKB is similarly designed to be secure and long term decentralized. We believe Nervos CKB has a more balanced and sustainable economic model than Bitcoin, and also comes with the intrinsic utility of securing crypto-assets and common knowledge.


### Developer's Cost in a "First Class Asset" Platform

In Ethereum, the top-level abstraction is its accounts. Assets are expressed as state owned by smart contract accounts. In the Nervos CKB, assets are the first class abstraction with cells, where ownership is expressed with the lock script of a transaction output, a concept known as "[First Class Assets](https://medium.com/nervosnetwork/first-class-asset-ff4feaf370c4)". In other words, just like Bitcoin, assets in the Common Knowledge Base are owned by users directly instead of being kept custody in a smart contract.

The "First Class Asset" design allows the state storage cost of owning assets put not on developers, but on individual users. For example, a developer could create a User Defined Token with 400 bytes of code as validation rules, and every record of asset ownership would take 64 bytes. Even if the assets were to have 10,000 owners, the developer would still only need to use 400 CK Bytes.

For developers, we expect the capital cost of building projects on the CKB is moderate even in a scenario that the price of the native tokens were to go up degrees of magnitude higher. For users, the cost of the 64 CK Bytes to own an asset on the Nervos CKB would also be trivial for a long time even in the most aggressive adoption assumption of the platform.

In the future where those cost were to become meaningfully expensive, it's always possible for developers to rely on lending to bootstrap their projects and for users to move their assets off the Common Knowledge Base on to other transaction blockchains in the Nervos Network if they're willing to take the corresponding trade-offs. Please see the "Nervos Network" section for more details.

### Lending

Nervos CKB will support native token lending to improve the liquidity of the CK Bytes thanks to the programming ability provided by CKB-VM and the Cell model. Since the utility of the native token is realized through possession instead of transactions, it's possible to have risk-free un-collateralized lending for CK Bytes locked for known duration of time.  Entrepreneurs can borrow the CK Bytes they need with much lower capital cost for a period such as 6 months to work on prototypes and prove their business model. Long term users can lend out their tokens to earn extra income.

The effective interest rate of lending is determined by the market supply and demand, but the current state of token utilization also plays a big role. Higher utilization of the available global state means fewer tokens can be made available for lending. This makes the lending interest higher and makes it more attractive to release state and lock tokens in the NervosDAO to earn income. It serves the purpose to help reduce the global state: lower utilization of the available state means more tokens can be lent out. It makes the lending interest rate lower to encourage adoption.


### Nervos Network

The Nervos CKB is the base layer of the Nervos Network with the highest security, decentralization, transaction cost and state storage cost. Just like how Bitcoin and Ethereum could scale off-chain with lightning network and plasma solutions, Nervos CKB also embraces off-chain scaling solutions and allow users to preserve and transact assets off-chain. When using off-chain solutions, users and developers can choose their own trade-offs between cost, security, latency and liveness properties.

Owning and transacting assets on the Nervos CKB come with the highest capital and transaction cost, but is also the most secure. It's best suited for high value assets and long term asset preservation; Layer 2 solutions can provide scaling for both transaction throughput and state storage, but they would come with either weakened security assumptions or mandate extra steps of repudiation. They also often require participants to be online within a time window. If both are acceptable (likely for owning and transacting low value assets for short duration), the Nervos CKB can be used as security anchor to other transaction blockchains, to effectively magnify both its transaction and state storage capacities.

If operators of transaction blockchains don't want to introduce extra security assumptions, they can mandate that high value assets be issued on the CKB and low value assets be issued on transactional blockchains. Then they can use CK Bytes on the CKB to store periodic block commits, challenges and proofs from the transactional blockchains - critical common knowledge for secure off-chain transaction repudiation. If a transaction chain doesn't mind introducing an extra layer of security assumption with a committee-based consensus protocol, they could also have their validators bond CK Bytes on the CKB to explicitly adjust security parameters.

## 8. Applications of the Token Economics Model

The economic model of the Nervos CKB provides building blocks that application developers can use directly as part of their own economic model. We'll list subscriptions and liquidity income as two such possible building blocks.

### Subscriptions

Recurring payment or subscription is a typical economic model for services offered on the blockchain that span over some duration of time. One such example is the off-chain transaction monitoring service that's often needed for layer 2 solutions. On the Nervos CKB, duration based services can ask their users to lock certain amount of native tokens in the NervosDAO and designate the service providers as the beneficiaries of the generated interest income in a subscription based model. Users can stop using the services by withdrawing their tokens from the NervosDAO.

In fact, Store of Assets users that occupy global state can be seen as paying an ongoing subscription metered by the size of their state, and the beneficiaries are the miners that provide the security service.

### Liquidity Income

In a Plasma like layer 2 solution, a typical pattern is that users would deposit native tokens in a smart contract on the layer 1 blockchain in exchange for transaction tokens on the layer 2. A layer 2 operator with sufficient reputation can have users commit to fixed duration deposits, and then use such deposits to provide liquidity to the lending market and earn income. This gives operators of layer 2 solutions an additional revenue stream on top of the fees collected on layer 2.

## Appendix 1: Transaction Cost Analysis

Nervos CKB uses Proof of Work based Nakamoto consensus, similar to what's used in Bitcoin - for more details, please see the "Nervos Consensus Paper"

The economics of the consensus process is designed to incentivize nodes to participate in the consensus process and provide measurements that nodes can use to prioritize transactions.  At the core, it's designed to help consensus nodes answer the question: "Is this transaction worth to be included in the next block if I had the opportunity to produce the block?"

A block producing node can do a cost/benefit analysis to answer this question. The benefit of including a transaction is to be able to collect its transaction fee, and the cost of including a transaction in a block has three parts:


- Fee Estimation Cost (![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/01.png) ): this is the cost to estimate the maximum possible income if a node where to include a transaction
- Transaction Verification Cost (![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/02.png) ): blocks containing invalid transactions will be rejected by the consensus process, therefore block producing nodes have to verify transactions before including them in a new block.
- State Transition Cost (![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/03.png)）: after a block is produced, the block producing node has to perform local state transitions defined by state machines of the transactions in the block.

In particular, transaction verification, ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/04.png)  has two possible steps:

- ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/05.png): Authorization Verification Cost

- ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/06.png): State Transition Verification Cost

We use CPC and EVC to represent Complete Processing Cost and Estimation and Verification Cost:

- CPC: Complete Processing Cost
  - ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/07.png)
- EVC: Estimation and Verification Cost;
  - ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/08.png)

### Bitcoin's Transaction Cost Analysis

Bitcoin allows flexible authorization verification with the Bitcoin Script. Users can script the authorization rules and build smart contracts through ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/09.png) when creating transactions. Bitcoin has a fixed state transition semantic, which is to spend and create new UTXOs. In Bitcoin, the result of the state transitions are already included in transactions, therefore the State Transition Cost (STC) is 0.

Bitcoin uses the amount difference of the inputs and outputs to express transaction fees. Therefore, the cost of estimating transaction fees scales to ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/10.png) where ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/11.png) is the total number of inputs and outputs.

Authorization verification in Bitcoin requires running scripts of all inputs. Because the Bitcoin Script prohibits JUMP/looping, the computation complexity can roughly scale to the length of the input scripts, as![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/12.png), where ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/13.png) is the number of inputs and ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/14.png) is the average script length of an input. Therefore, the total cost of ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/15.png) roughly scales to the size of total transaction.

Bitcoin's state transition rules are simple, and nodes only have to verify the total input amount is the same as the total output amount. Therefore, the ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/16.png) in Bitcoin is the same as ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/17.png), also scaling to ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/18.png).

In total, Bitcoin's cost of processing a transaction roughly scales to the size of the transaction:
![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/19.png)

### Ethereum's Transaction Cost Analysis

Ethereum comes with Turing-complete scriptability, and gives users more flexibility to customize state transition rules with smart contracts. Ethereum transactions include *gaslimit* and *gasprice*, and the transaction fees are calculated using the product of their multiplication. Therefore, ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/20.png) is ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/21.png).

Unlike Bitcoin, Ethereum's transactions only include the computation commands of state transitions, instead of the results of the state transitions. Therefore, Ethereum's transaction verification is limited to authorization verification, and doesn't have state transition verification. The rules of authorization verification in Ethereum are:


- Verify the validility of the Secp256k1 signatures, with computation complexity of ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/22.png)
- Verify the nonce match of the transaction and the account that starts the transaction, with computation complexity of ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/23.png)
- Verify the account that starts transaction has enough ether to pay for the transaction fees and the amount transferred. This requires access to the account's current balance. Ignoring the global state size's impact on account access, we can assume the complexity of this step is also ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/23.png).

Based on the above, the overall authorization verification complexity in Ethereum is ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/24.png).

Since every byte of the transaction data comes with cost ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/25.png), the larger ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/26.png) is, the more gas it needs, up to the *gaslimit* ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/27.png)specified. Therefore,

![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/54.png)

Ethereum comes with a Turing complete VM, and the computation of the result state could include logic of any complexity. Ethereum transaction's ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/28.png) caps the upper bound of computation, therefore ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/29.png)。To summarize all the above:

![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/30.png)

![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/31.png)

![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/32.png)

Different from Bitcoin, ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/33.png) for the Ethereum nodes is less than ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/34.png). This is because Ethereum nodes only compute the result state after transactions are included in the block. This is also the reason that transaction results on Ethereum could be invalid, (e.g. exceptions in contract invocation or the gas limit is exceeded),  but the Bitcoin blockchain only has successfully executed transactions and valid results.

### Nervos CKB's Transaction Cost Analysis

Nervos CKB's transactions are structured with inputs and outputs, similar to Bitcoin's. Therefore, the ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/35.png) and ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/36.png) for the Nervos CKB are the same as those of Bitcoin's:

![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/37.png)

![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/38.png)

Because CKB transactions include the result of the transactions as outputs, therefore:

![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/39.png)

### Cycles as Measurement Units of Computation Complexity

We introduce "cycle" as a unit of measurement for computation complexity in the CKB, similar to the "gas" concept in Ethereum. Nervos CKB's VM is a RISC-V CPU simulator, therefore cycles here refer to real CPU computation cycles in the VM. The cycle number for an instruction represents the relative computation cost of that instruction. Transactions in the Nervos CKB require the sender to specify the number of cycles required for its verification. Nodes can opt to set an acceptable cycle upper bound *cyclemax*, and only process transactions with fewer cycles. We'll also introduce *cycles* to a block, with its value equal to the sum of all specified transaction cycles.  The value of *cycles* in a block can't exceed the value *blockcyclesmax*, which are set and can be automatically adjusted by the system.

Nodes can set their *cyclemax* to different values. *cyclemax* only impacts how a block producing node accepts new transactions, not how a node accepts transactions in a new block. Therefore, it's not going to cause inconsistency in the validation of blocks. A valid block needs valid proof of work, and this cost discourages a block producing node to include an invalid transaction with high *cycles* value.

The following table shows the runtime differences in Bitcoin, Ethereum and the Nervos CKB.

|          | Authorization (![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/40.png)） | State Validation (![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/41.png)) | State Transition（![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/42.png)） |
| -------- | ---------------------------- | ----------------------------- | ------------------------ |
| Bitcoin  | Generalized                  | Fixed                         | None                     |
| Ethereum | Fixed                        | None                          | Generalized              |
| CKB      | Generalized                  | Generalized                   | None                     |


Here's a summary of the computational complexity of different parts of the consensus process for Bitcoin, Ethereum and Nervos CKB (![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/55.png) means cycle limit)

|          |![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/43.png)     |![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/44.png)         |![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/45.png)          |![](https://raw.githubusercontent.com/Jack0814/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/46.png)          | ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/47.png)         |
| -------- | ------------- | ---------------- | ---------------- | ---------------- | ---------------- |
| Bitcoin  |![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/48.png)| ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/49.png)    | ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/50.png)         | ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/49.png)   | ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/49.png)  |
| Ethereum |![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/51.png)     |![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/49.png)    | ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/52.png) | ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/49.png)     | ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/52.png) |
| CKB      | ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/48.png)  | ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/53.png) | ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/50.png)           | ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/53.png)  | ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/53.png) |


================================================
File: rfcs/0017-tx-valid-since/0017-tx-valid-since.md
================================================
---
Number: "0017"
Category: Standards Track
Status: Active
Author: Jinyang Jiang <@jjyr>, Ian Yang <@doitian>, Jordan Mack <@jordanmack>
Created: 2019-03-11
---

# Transaction Since Precondition

<!-- Diagrams are created in Lucid: https://lucid.app/documents/view/d756089a-2388-4ea4-b61a-3943cbe2620a -->

## Abstract

This RFC describes a consensus rule used to prevent a transaction input from being committed before a specified time in the future. 

## Summary

An optional `since` value can be added to any input within a transaction that specifies a time in the future when it can be committed. The current time must be equal to or greater than the time specified in the `since` value before a CKB node will consider this precondition fulfilled and allow the transaction to be committed.

A transaction may be composed of multiple inputs from different parties that are batched together. Therefore a `since` field is located on each input within the transaction to allow every party to set their `since` value individually. If the `since` precondition is not met on any input within the transaction, the entire transaction will be immediately rejected by the CKB node on submission.

Three metrics can be used to specify how the `since` time is expressed:

1. Block Number
2. Epoch Number (with fraction)
3. Timestamp (median of previous 37 blocks)

A developer can choose exactly one of these three metrics to use, and this will be used to indicate the format of `since` value. The metric used also used to determine how the threshold value, and the target value will be calculated. The threshold value is the block number, epoch number, or timestamp that must be reached to fulfill the precondition and allow the commit to occur. The target value can be thought of as the current block number, epoch number, or timestamp.

Note: Each one of these metrics will always increase in value over time with each new block added to the chain, meaning it is safe to assume these values will never decrease over time.

The relative flag must be set to either `absolute` or `relative` with the specified metric. This indicates how the threshold value should be calculated. 

When `absolute` is specified, the threshold value is set to the specified `since` value, which is interpreted by the metric selected.

When `relative` is specified, the threshold value is calculated by retrieving the base value from the commitment block and adding the `since` value. The base value is the block number, epoch number, or timestamp depending on the metric selected. The commitment block is the block in which the input was committed.

After the threshold value and target value have been calculated they can be compared. The precondition is fulfilled only if the target value is equal to or greater than the threshold value.

## Specification

### How to Specify the Since Precondition

Each transaction input has a `since` field. The field itself is an unsigned 64-bit integer (u64) with special encoding for different values.[^1] A u64 value of `0` is used to indicate that the `since` precondition is disabled and will be ignored. If the field value is not `0`, then the highest 8 bits of the `since` field represent configuration `flags` and the remaining `56` bits represent the `value`.

[^1]: See [RFC22](../0022-transaction-structure/0022-transaction-structure.md) for the full transaction structure.

![Since Encoding](since-encoding.jpg)

* The highest bit is used to specify if the `value` is `absolute` or `relative`.

    * `0`: The `value` is `absolute`.
    * `1`: The `value` is `relative`.

* The next two bits specify the metric which will be used to interpret `value` and to calculate the treshold value and target value.

    * `00`: Use the block number.
    * `01`: Use the epoch number with fraction.
    * `10`: Use the median timestamp of the previous 37 block headers.
    * `11`: Invalid. The metric flag should never be set to `11`.

* The next 5 bits are reserved for future extension. They must all be set to zero for now.

Interpretation of `value` is dependent on the metric which was specified.

When the metric flag is set to block number (`00`) or timestamp median (`10`), then `value` is a 56-bit unsigned integer stored in little-endian.

When the metric flag is set to epoch number with fraction (`01`), `value` represents an epoch (E), epoch index (I), and epoch length (L). These three values are encoded into `value` as follows:

* `E` has 3 bytes, from the lowest bit 0 to 23.
* `I` has 2 bytes, from bit 24 to 39.
* `L` has 2 bytes, from bit 40 to 55.

Note: The bit ranges for `E`, `I`, and `L` are using [LSB 0 Bit Numbering](https://en.wikipedia.org/wiki/Bit_numbering#LSB_0_bit_numbering) which counts from right to left. All three values are stored in little-endian.

The following diagram illustrates how the `E`, `I`, and `L` bit ranges are positioned within the 56-bit `value` portion of the `since` field.

![EIL Encoding](e-i-l-encoding.png)

The following table shows how to decode different values contained within `since` using bit operations right shift (`>>`), left shift (`<<`), and bit and (`&`).

| Name | Bit Operation |
| ---- | ------------- |
| relative flag | `since >> 63` |
| metric flag   | `(since >> 61) & 3` |
| value         | `since & ((1 << 56) - 1)` |
| `E` in value  | `since & ((1 << 24) - 1)` |
| `I` in value  | `(since >> 24) & ((1 << 16) - 1)` |
| `L` in value  | `(since >> 40) & ((1 << 16) - 1)` |

### How to Verify the Since Precondition

There are three major steps to verify the since precondition:

1. Decode Values: Extract the necessary values which are encoded within the `since` field and verify the format is valid.
2. Compute Threshold Value: Determine the threshold value that will be used for comparison.
3. Derive Target Value and Compare: Derive the target value from the block that is going to commit the transaction. Compare it with the threshold value to check whether the precondition is fulfilled.

The following flowchart illustrates the verification process.

![Since Verification](since-verification.jpg)

#### Step 1. Decode Values

> Extract the necessary values which are encoded within the `since` field and verify the format is valid.

If the `since` field value is zero, then this indicates that since precondition is not used and verification can be skipped.

If the `since` field value is not zero, verify that the format is valid:

1. The metric flag should not be `11`.
2. The reserved flags must be all zeros.
3. When the metric flag is epoch number with fraction (`01`), `I` must be less than `L`, or they must both be zeros. In the latter case, the `I` and `L` values will be set to `0` and `1` respectively.

If the format is valid, the process moves on to the next step.

#### Step 2. Compute Threshold Value

> Determine the threshold value that will be used for comparison.

The relative flag specifies if `absolute` or `relative` should be used to determine the threshold value.

When `absolute` is specified, the threshold value is set to the 56-bit `value` portion of the `since` field. This is a simple process that copies the value directly without any conversion, and no further steps need to be taken.

When `relative` is specified, the threshold value is set to the base value derived from the commitment block plus the (56-bit) `value` portion of the `since` field.

##### Commitment Block

A two-step transaction confirmation protocol is used by CKB, and it is defined in [RFC20][]. A transaction is only considered committed when it is included in the commitment zone of a block. The commitment block of a transaction is the block that committed the transaction.

[RFC20]: ../0020-ckb-consensus-protocol/0020-ckb-consensus-protocol.md#two-step-transaction-confirmation

A transaction input is a reference to an output of another transaction. The commitment block of a transaction input is the same as the commitment block of the transaction producing the referenced output.

In the diagram below, `Block B` is the commitment block of `Input 0` of `Transaction X`.

![Commitment Block](commitment-block.jpg)

##### Base Value

The base value is derived from the input commitment block according to the metric that is specified.

1. If the metric flag is `00` (block number), the base value is the block number of the commitment block.
2. If the metric flag is `01` (epoch number with fraction), the base value is the epoch field in the commitment block header. This shares the same encoding as the aforementioned epoch field which contains the individual values for epoch (E), epoch index (I), and epoch length (L).
3. If the metric flag is `10` (timestamp), the base value is the timestamp field in the commitment block header.

The threshold value of an `absolute` precondition equals the 56-bit `value` portion of the `since` field. The threshold value of a `relative` precondition equals the base value plus the `value` portion of the `since` field.

The diagram below visualizes the threshold value computation process.

![Threshold Value](threshold-value.jpg)

#### Step 3. Derive Target Value and Compare

> Derive the target value from the block that is going to commit the transaction. Compare it with the threshold to check whether the precondition is fulfilled.

We will use the term "target block" to refer to the block that commits a transaction. The target block of a committed transaction is the mined block which included the transaction. This is the same as the commitment block. The target block of a pending transaction in the mempool or in transit is the next block that will be mined, but has not been mined yet.

The target value is determined by the target block depending on which metric was selected. These are calculated as follows:

1. If the metric flag is `00` (block number), the target value is the block number of the last mined block, plus 1.
2. If the metric flag is `01` (epoch number with fraction), the target value is the next index of the last mined block. If the last mined block is `500 10/20` (E = 500, I = 10, L = 20), then the target value is `500 11/20` (E = 500, I = 11, L = 20).
3. If the metric flag is `10` (timestamp), the target value is the median of the timestamp field of the 37 blocks preceding the target block.

![Target Value](target-value.jpg)

The final step is to compare the target value to the threshold value. The precondition is fulfilled if the target value is equal to or greater than the threshold value.


================================================
File: rfcs/0019-data-structures/0019-data-structures.md
================================================
---
Number: "0019"
Category: Informational
Status: Withdrawn
Author: Xuejie Xiao <xxuejie@gmail.com>
Created: 2019-03-26
---

# Data Structures of Nervos CKB

This documents explains all the basic data structures used in CKB.

* [Cell](#Cell)
* [Script](#Script)
* [Transaction](#Transaction)
* [Block](#Block)



## Cell

### Example

```json
{
  "capacity": "0x19995d0ccf",
  "lock": {
    "code_hash": "0x9bd7e06f3ecf4be0f2fcd2188b23f1b9fcc88e5d4b65a8637b17723bbda3cce8",
    "args": "0x0a486fb8f6fe60f76f001d6372da41be91172259",
    "hash_type": "type"
  },
  "type": null
}
```

## Description

| Name       | Type       | Description                                                  |
| :--------- | :--------- | :----------------------------------------------------------- |
| `capacity` | uint64     | **The size of the cell (in shannons).** When a new cell is generated (via transaction), one of the verification rule is `capacity_in_bytes >= len(capacity) + len(data) + len(type) + len(lock)`. This value also represents the balance of CKB coin, just like the `nValue` field in the Bitcoin's CTxOut. (E.g. Alice owns 100 CKB coins means she can unlock a group of cells that has 100 amount of `bytes` (which is 10_000_000_000 amount of `shannons`) in total.). The actual value is returned in hex string format. |
| `type`     | `Script`   | **A Script that defines the type of the cell.** It limits how the `data` field of the new cells can be changed from old cells. `type` is required to has a data structure of `script`. **This field is optional.** |
| `lock`     | `Script`   | **A Script that defines the ownership of the cell**, just like the `scriptPubKey` field in the Bitcoin's CTxOut. Whoever can provide unlock arguments that makes the execution of this script success can consume this cell as input in an transaction (i.e. has the ownership of this cell). |



More information about Cell can be found in the [whitepaper](https://github.com/nervosnetwork/rfcs/blob/master/rfcs/0002-ckb/0002-ckb.md#42-cell).



## Script

### Example

```json
{
  "code_hash": "0x9bd7e06f3ecf4be0f2fcd2188b23f1b9fcc88e5d4b65a8637b17723bbda3cce8",
  "args": "0x0a486fb8f6fe60f76f001d6372da41be91172259",
  "hash_type": "type"
}
```



### Description

| Name          | Type                                 | Description                                                  |
| :------------ | :----------------------------------- | :----------------------------------------------------------- |
| `code_hash`   | H256(hash)                           | **The hash of ELF formatted RISC-V binary that contains a CKB script.** For space efficiency consideration, the actual script is attached to current transaction as a dep cell. Depending on the value of `hash_type`, the hash specified here should either match the hash of cell data part in the dep cell, or the hash of type script in the dep cell. The actual binary is loaded into an CKB-VM instance when they are specified upon the transaction verification. |
| `args`        | Bytes                                | **The argument as the script input.** The argument here is imported into the CKB-VM instance as the input argument for the scripts. |
| `hash_type`   | String, could be `type` or `data`    | **The interpretation of code hash when looking for matched dep cells.** If this is `data`, `code_hash` should match the blake2b hash of data in a dep cell; if this is `type`, `code_hash` should instead match the type script hash of a dep cell. |



When a script is validated, CKB will run it in a RISC-V VM, `args` must be loaded via special CKB syscalls. UNIX standard `argc`/`argv` convention is not used in CKB. For more information on the CKB VM please refer to [CKB VM RFC](../0003-ckb-vm/0003-ckb-vm.md).

For more information regarding how `Script` structure is implemented please refer to the [CKB repo](https://github.com/nervosnetwork/ckb).



## Transaction

### Example

```json
{
  "version": "0x0",
  "cell_deps": [
    {
      "out_point": {
        "tx_hash": "0xbd864a269201d7052d4eb3f753f49f7c68b8edc386afc8bb6ef3e15a05facca2",
        "index": "0x0"
      },
      "dep_type": "dep_group"
    }
  ],
  "header_deps": [
    "0xaa1124da6a230435298d83a12dd6c13f7d58caf7853f39cea8aad992ef88a422"
  ],
  "inputs": [
    {
      "previous_output": {
        "tx_hash": "0x8389eba3ae414fb6a3019aa47583e9be36d096c55ab2e00ec49bdb012c24844d",
        "index": "0x1"
      },
      "since": "0x0"
    }
  ],
  "outputs": [
    {
      "capacity": "0x746a528800",
      "lock": {
        "code_hash": "0x9bd7e06f3ecf4be0f2fcd2188b23f1b9fcc88e5d4b65a8637b17723bbda3cce8",
        "args": "0x56008385085341a6ed68decfabb3ba1f3eea7b68",
        "hash_type": "type"
      },
      "type": null
    },
    {
      "capacity": "0x1561d9307e88",
      "lock": {
        "code_hash": "0x9bd7e06f3ecf4be0f2fcd2188b23f1b9fcc88e5d4b65a8637b17723bbda3cce8",
        "args": "0x886d23a7858f12ebf924baaacd774a5e2cf81132",
        "hash_type": "type"
      },
      "type": null
    }
  ],
  "outputs_data": [
    "0x",
    "0x"
  ],
  "witnesses": [
    "0x55000000100000005500000055000000410000004a975e08ff99fa000142ff3b86a836b43884b5b46f91b149f7cc5300e8607e633b7a29c94dc01c6616a12f62e74a1415f57fcc5a00e41ac2d7034e90edf4fdf800"
  ]
}
```

### Description

#### Transaction

| Name              | Type                             | Description                                                  |
| ----------------- | -------------------------------- | ------------------------------------------------------------ |
| `version`         | uint32                           | **The version of the transaction.** It‘s used to distinguish transactions when there's a fork happened to the blockchain system. |
| `cell_deps`       | [`CellDep`]                      | **An array of `outpoint` pointing to the cells that are dependencies of this transaction.** Only live cells can be listed here. The cells listed are read-only. |
| `header_deps`     | [`H256(hash)`]                   | **An array of `H256` hashes pointing to block headers that are dependencies of this transaction.**   |
| `inputs`          | [`CellInput`]                    | **An array of referenced cell inputs.** See below for explanations of underlying data structure |
| `outputs`         | [`Cells`], see above for details | **An array of cells that are used as outputs**, i.e. the newly generated cells. These are the cells may be used as inputs for other transactions. Each of the Cell has the same structure to [the Cell section](#cell) above. |
| `outputs_data`    | [`Bytes`]                        | **An array of cell data for each cell output.** The actual data are kept separated from outputs for the ease of CKB script handling and for the possibility of future optimizations. |
| `witnesses`       | [`Bytes`]                        | **Witnesses provided by transaction creator to make the execution of corresponding lock script success**. One example here, is that signatures might be include here to make sure a signature verification lock script passes. |


#### CellDep


| Name        | Type                                 | Description                                                  |
| ----------- | ------------------------------------ | ------------------------------------------------------------ |
| `out_point` | `OutPoint`                           | **A cell outpoint that point to the cells used as deps.** Dep cells are dependencies of a transaction, it could be used to include code that are loaded into CKB VM, or data that could be used in script execution. |
| `dep_type`  | String, either `code` or `dep_group` | **The way to interpret referenced cell deps.** A cell dep could be referenced in 2 ways: for a cell dep with `code` as `dep_type`, the dep cell is directly included in the transaction. If a cell dep `dep_type` uses `dep_group`, however, CKB would first load this dep cell, assume the content of this cell contains a list of cell deps, then use the extracted list of cell deps to replace current cell dep, and include them in current transaction. This provides a quicker and smaller(in terms of transaction size) to include multiple commonly used dep cells in one CellDep construct. |


#### CellInput


| Name              | Type       | Description                                                  |
| ----------------- | ---------- | ------------------------------------------------------------ |
| `previous_output` | `OutPoint` | **A cell outpoint that point to the cells used as inputs.** Input cells are in fact the output of previous transactions, hence they are noted as `previous_output` here. These cells are referred through  `outpoint`, which contains the transaction `hash` of the previous transaction, as well as this cell's `index` in its transaction's output list. |
| `since`           | uint64     | **Since value guarding current referenced inputs.** Please refer to the [Since RFC](../0017-tx-valid-since/0017-tx-valid-since.md) for details on this field. |


#### OutPoint


| Name      | Type               | Description                                                  |
| ----------| ------------------ | ------------------------------------------------------------ |
| `tx_hash` | H256(hash)         | **The hash of the transaction that this cell belongs to.**   |
| `index`   | uint32             | **The index of the cell in its transaction's output list.**  |





More information about the Transaction of Nervos CKB can be found in [whitepaper](../0002-ckb/0002-ckb.md#44-transaction).



## Block

### Example

```json
{
  "uncles": [
    {
      "proposals": [

      ],
      "header": {
        "compact_target": "0x1a9c7b1a",
        "hash": "0x87764caf4a0e99302f1382421da1fe2f18382a49eac2d611220056b0854868e3",
        "number": "0x129d3",
        "parent_hash": "0x815ecf2140169b9d283332c7550ce8b6405a120d5c21a7aa99d8a75eb9e77ead",
        "nonce": "0x78b105de64fc38a200000004139b0200",
        "timestamp": "0x16e62df76ed",
        "transactions_root": "0x66ab0046436f97aefefe0549772bf36d96502d14ad736f7f4b1be8274420ca0f",
        "proposals_hash": "0x0000000000000000000000000000000000000000000000000000000000000000",
        "uncles_hash": "0x0000000000000000000000000000000000000000000000000000000000000000",
        "version": "0x0",
        "epoch": "0x7080291000049",
        "dao": "0x7088b3ee3e738900a9c257048aa129002cd43cd745100e000066ac8bd8850d00"
      }
    }
  ],
  "proposals": [
    "0x5b2c8121455362cf70ff"
  ],
  "transactions": [
    {
      "version": "0x0",
      "cell_deps": [

      ],
      "header_deps": [

      ],
      "inputs": [
        {
          "previous_output": {
            "tx_hash": "0x0000000000000000000000000000000000000000000000000000000000000000",
            "index": "0xffffffff"
          },
          "since": "0x129d5"
        }
      ],
      "outputs": [
        {
          "capacity": "0x1996822511",
          "lock": {
            "code_hash": "0x9bd7e06f3ecf4be0f2fcd2188b23f1b9fcc88e5d4b65a8637b17723bbda3cce8",
            "args": "0x2ec3a5fb4098b14f4887555fe58d966cab2c6a63",
            "hash_type": "type"
          },
          "type": null
        }
      ],
      "outputs_data": [
        "0x"
      ],
      "witnesses": [
        "0x590000000c00000055000000490000001000000030000000310000009bd7e06f3ecf4be0f2fcd2188b23f1b9fcc88e5d4b65a8637b17723bbda3cce801140000002ec3a5fb4098b14f4887555fe58d966cab2c6a6300000000"
      ],
      "hash": "0x84395bf085f48de9f8813df8181e33d5a43ab9d92df5c0e77d711e1d47e4746d"
    }
  ],
  "header": {
    "compact_target": "0x1a9c7b1a",
    "hash": "0xf355b7bbb50627aa26839b9f4d65e83648b80c0a65354d78a782744ee7b0d12d",
    "number": "0x129d5",
    "parent_hash": "0x4dd7ae439977f1b01a8c9af7cd4be2d7bccce19fcc65b47559fe34b8f32917bf",
    "nonce": "0x91c4b4746ffb69fe000000809a170200",
    "timestamp": "0x16e62dfdb19",
    "transactions_root": "0x03c72b4c2138309eb46342d4ab7b882271ac4a9a12d2dcd7238095c2d131caa6",
    "proposals_hash": "0x0000000000000000000000000000000000000000000000000000000000000000",
    "uncles_hash": "0x90eb89b87b4af4c391f3f25d0d9f59b8ef946d9627b7e86283c68476fee7328b",
    "version": "0x0",
    "epoch": "0x7080293000049",
    "dao": "0xae6c356c8073890051f05bd38ea12900939dbc2754100e0000a0d962db850d00"
  }
}
```

### Description

#### Block

| Name                    | Type            | Description                                                  |
| ----------------------- | --------------- | ------------------------------------------------------------ |
| `header`                | `Header`        | **The block header of the block.** This part contains some metadata of the block. See [the Header section](#header) below for the details of this part. |
| `transactions`           | [`Transaction`] | **An array of committed transactions contained in the block.** Each element of this array has the same structure as [the Transaction structure](#transaction) above. |
| `proposals`             | [string]        | **An array of hex-encoded short transaction ID of the proposed transactions.** |
| `uncles`                | [`UncleBlock`]  | **An array of uncle blocks of the block.** See [the UncleBlock section](#uncleblock) below for the details of this part. |

#### Header

(`header` is a sub-structure of `block` and `UncleBlock`.)

| Name                | Type       | Description                                                  |
| ------------------- | ---------- | ------------------------------------------------------------ |
| `compact_target`    | uint32     | **The difficulty of the PoW puzzle represented in compact target format.** |
| `number`            | uint64     | **The block height.**                                        |
| `parent_hash`       | H256(hash) | **The hash of the parent block.**                            |
| `nonce`             | uint128    | **The nonce.** Similar to [the nonce in Bitcoin](https://en.bitcoin.it/wiki/Nonce). Represent the solution of the PoW puzzle |
| `timestamp`         | uint64     | **A [Unix time](http://en.wikipedia.org/wiki/Unix_time) timestamp in milliseconds.** |
| `transactions_root` | H256(hash) | **The hash of concatenated transaction hashes CBMT root and transaction witness hashes CBMT root.** |
| `proposals_hash`    | H256(hash) | **The hash of concatenated proposal ids.** (all zeros when proposals is empty) |
| `uncles_hash`       | H256(hash) | **The hash of concatenated hashes of uncle block headers.** （all zeros when uncles is empty) |
| `version`           | uint32     | **The version of the block**. This is for solving the compatibility issues might be occurred after a fork. |
| `epoch`             | uint64     | **Current epoch information.** Assume `number` represents the current epoch number, `index` represents the index of the block in the current epoch(start at 0), `length` represents the length of current epoch. The value store here will then be `(number & 0xFFFFFF) \| ((index & 0xFFFF) << 24) \| ((length & 0xFFFF) << 40)` |
| `dao`               | Bytes      | **Data containing DAO related information.** Please refer to Nervos DAO RFC for details on this field. |

#### UncleBlock

(`UncleBlock` is a sub-structure of `Block`.)

| Name                    | Type          | Description                                                  |
| ----------------------- | ------------- | ------------------------------------------------------------ |
| `header`                | `Header`      | **The block header of the uncle block.** The inner structure of this part is same as [the Header structure](#header) above. |
| `proposals`             | [`string`]    | **An array of short transaction IDs of the proposed transactions in the uncle block.** |


================================================
File: rfcs/0020-ckb-consensus-protocol/0020-ckb-consensus-protocol.md
================================================
---
Number: "0020"
Category: Informational
Status: Draft
Author: Ren Zhang <@nirenzang>
Created: 2019-6-19
---
# CKB Consensus Protocol

* [Abstract](#Abstract)
* [Motivation](#Motivation)
* [Technical Overview](#Technical-Overview)
  * [Eliminating the Bottleneck in Block Propagation](#Eliminating-the-Bottleneck-in-Block-Propagation)
  * [Utilizing the Shortened Latency for Higher Throughput](#Utilizing-the-Shortened-Latency-for-Higher-Throughput)
  * [Mitigating Selfish Mining Attacks](#Mitigating-Selfish-Mining-Attacks)
* [Specification](#Specification)
  * [Two-Step Transaction Confirmation](#Two-Step-Transaction-Confirmation)
  * [Dynamic Difficulty Adjustment Mechanism](#Dynamic-Difficulty-Adjustment-Mechanism)
  * [Protocol Parameters](#Protocol-Parameters)

<a name="Abstract"></a>
## Abstract

Bitcoin's Nakamoto Consensus (NC) is well-received due to its simplicity and low communication overhead. However, NC suffers from two kinds of drawback: first, its transaction processing throughput is far from satisfactory; second, it is vulnerable to a selfish mining attack, where attackers can gain more block rewards by deviating from the protocol's prescribed behavior.

The CKB consensus protocol is a variant of NC that raises its performance limit and selfish mining resistance while keeping its merits. By identifying and eliminating the bottleneck in NC's block propagation latency, our protocol supports very short block interval without sacrificing security. The shortened block interval not only raises the throughput, but also lowers the transaction confirmation latency. By incorporating all valid blocks in the difficulty adjustment, selfish mining is no longer profitable for a large range of parameters in our protocol.

We provide only the gist of our design in this document. The detailed background and supporting evidence---experiments, performance evaluation, and security proof---can be found in two academic publications:

> Ren Zhang, Dingwei Zhang, Quake Wang, Shichen Wu, Jan Xie, Bart Preneel. NC-Max: Breaking the Security-Performance Tradeoff in Nakamoto Consensus. In *the Network and Distributed System Security Symposium (NDSS) 2022*.

Paper on [Eprint](https://eprint.iacr.org/2020/1101). Short talk (15 min) on [Youtube](https://www.youtube.com/watch?v=mYS-A1CK6zc). Full talk (23 min) on [Youtube](https://www.youtube.com/watch?v=WwD9ZvuI9J8), [Bilibili](https://www.bilibili.com/video/BV1XP411n7qV/).

> A paper in submission by Roozbeh Sarenche, Ren Zhang, Svetla Nikova, Bart Preneel.

We will publish it as soon as the paper is accepted.

<a name="Motivation"></a>
## Motivation

Although a number of non-NC consensus mechanisms have been proposed, NC has the following threefold advantage comparing with its alternatives. First, its security is carefully scrutinized and well-understood [[1](https://www.cs.cornell.edu/~ie53/publications/btcProcFC.pdf), [2](https://eprint.iacr.org/2014/765.pdf), [3](https://fc16.ifca.ai/preproceedings/30_Sapirshtein.pdf), [4](https://eprint.iacr.org/2016/454.pdf), [5](https://eprint.iacr.org/2016/1048.pdf), [6](https://eprint.iacr.org/2018/800.pdf), [7](https://eprint.iacr.org/2018/129.pdf), [8](https://arxiv.org/abs/1607.02420)], whereas alternative protocols often open new attack vectors, either unintentionally [[1](http://fc19.ifca.ai/preproceedings/180-preproceedings.pdf), [2](https://www.esat.kuleuven.be/cosic/publications/article-3005.pdf)] or by relying on security assumptions that are difficult to realize in practice [[1](https://arxiv.org/abs/1711.03936), [2](https://arxiv.org/abs/1809.06528)]. Second, NC minimizes the consensus protocol's communication overhead. In the best-case scenario, propagating a 1 MB block in Bitcoin is equivalent to broadcasting a compact block message of roughly 13 KB [[1](https://github.com/bitcoin/bips/blob/master/bip-0152.mediawiki), [2](https://www.youtube.com/watch?v=EHIuuKCm53o)]; valid blocks are immediately accepted by all honest nodes. In contrast, alternative protocols often demand a non-negligible communication overhead to certify that certain nodes witness a block. For example, [Algorand](https://algorandcom.cdn.prismic.io/algorandcom%2Fa26acb80-b80c-46ff-a1ab-a8121f74f3a3_p51-gilad.pdf) demands that each block be accompanied by 300 KB of block certificate. Third, NC's chain-based topology ensures that a transaction global order is determined at block generation, which is compatible with all smart contract programming models. Protocols adopting other topologies either [abandon the global order](https://allquantor.at/blockchainbib/pdf/sompolinsky2016spectre.pdf) or establish it after a long confirmation delay [[1](https://eprint.iacr.org/2018/104.pdf), [2](https://eprint.iacr.org/2017/300.pdf)], limiting their efficiency or functionality.

Despite NC's merits, a scalability barrier hinders it from processing more than a few transactions per second. Two parameters collectively cap the system's throughput: the maximum block size and the expected block interval. For example, Bitcoin enforces a roughly 1 MB block size upper bound and targets a 10-minute block interval and  with its **difficulty adjustment mechanism**, translating to roughly ten transactions per second (TPS). Increasing the block size or reducing the block interval leads to longer block propagation latency or more frequent block generation events, respectively; both approaches raise the fraction of blocks generated during other blocks' propagation, thus raising the fraction of competing blocks. As at most one block among the competing ones contributes to transaction confirmation, the nodes' bandwidth on propagating other **orphaned blocks** is wasted, limiting the system's effective throughput. Moreover, raising the orphan rate downgrades the protocol's security by lowering the difficulty of double-spending attacks [[1](<https://fc15.ifca.ai/preproceedings/paper_30.pdf>), [2](<https://fc15.ifca.ai/preproceedings/paper_101.pdf>)].

Moreover, the security of NC is undermined by a [**selfish mining attack**](https://www.cs.cornell.edu/~ie53/publications/btcProcFC.pdf), which allows attackers to gain unfair block rewards by deliberately orphaning blocks mined by other miners. Researchers observe that the unfair profit roots in NC's difficulty adjustment mechanism, which neglects orphaned blocks when estimating the network's computing power. Through this mechanism, the increased orphan rate caused by selfish mining leads to lower mining difficulty, enabling the attacker's higher time-averaged block reward [[1](https://eprint.iacr.org/2016/555.pdf), [2](https://github.com/ethereum/EIPs/blob/master/EIPS/eip-100.md), [3](https://arxiv.org/abs/1805.08281)].

In this RFC, we present the CKB consensus protocol, a consensus protocol that raises NC's performance limit and selfish mining resistance while keeping all NC's merits. Our protocol supports very short block interval by reducing the block propagation latency. The shortened block interval not only raises the blockchain's throughput, but also minimizes the transaction confirmation latency without decreasing the level of confidence, as the orphan rate remains low. Selfish mining is no longer profitable for a large range of parameters as we incorporate all blocks, including uncles, in the difficulty adjustment when estimating the network's computing power, so that the new difficulty is independent of the orphan rate.

<a name="Technical-Overview"></a>
## Technical Overview

Our consensus protocol makes three changes to NC.

<a name="#Eliminating-the-Bottleneck-in-Block-Propagation"></a>
### Eliminating the Bottleneck in Block Propagation

[Bitcoin's developers identify](https://www.youtube.com/watch?v=EHIuuKCm53o) that when the block interval decreases, the bottleneck in block propagation latency is transferring **fresh transactions**, which are newly broadcast transactions that have not finished propagating to the network when embedded in the latest block. Nodes that have not received these transactions must request them before forwarding the block to their neighbors. The resulted delay not only limits the blockchain's performance, but can also be exploited in a **de facto selfish mining attack**, where attackers deliberately embed fresh transactions in their blocks, hoping that the longer propagation latency gives them an advantage in finding the next block to gain more rewards.

Departing from this observation, our protocol eliminates the bottleneck by decoupling NC's transaction confirmation into two separate steps: **propose** and **commit**. A transaction is proposed if its truncated hash, named `txpid`, is embedded in the **proposal zone** of a blockchain block or its **uncles**---orphaned blocks that are referred to by the blockchain block. Newly proposed transactions affect neither the block validity nor the block propagation, as a node can start transferring the block to its neighbors before receiving these transactions. The transaction is committed if it appears in the **commitment zone** in a window starting several blocks after its proposal. This two-step confirmation rule eliminates the block propagation bottleneck, as committed transactions in a new block are already received and verified by all nodes when they are proposed. The new rule also effectively mitigates de facto selfish mining by limiting the attack time window.

<a name="Utilizing-the-Shortened-Latency-for-Higher-Throughput"></a>
### Utilizing the Shortened Latency for Higher Throughput

Our protocol prescribes that blockchain blocks refer to all orphaned blocks as uncles. This information allows us to estimate the current block propagation latency and dynamically adjust the expected block interval, increasing the throughput when the latency improves. Accordingly, our difficulty adjustment targets a fixed orphan rate to utilize the shortened latency without compromising security. The protocol hard-codes the upper and lower bounds of the interval to defend against DoS attacks and avoid overloading the nodes. In addition, the block reward is adjusted proportionally to the expected block interval within an epoch, so that the expected time-averaged reward is independent of the block interval.

<a name="Mitigating-Selfish-Mining-Attacks"></a>
### Mitigating Selfish Mining Attacks

Our protocol incorporate all blocks, including uncles, in the difficulty adjustment when estimating the network's computing power, so that the new difficulty is independent of the orphan rate, following the suggestion of [Vitalik](https://github.com/ethereum/EIPs/blob/master/EIPS/eip-100.md), [Grunspan and Perez-Marco](https://arxiv.org/abs/1805.08281).

In addition, we prove that selfish mining is no longer profitable for a large range of parameters in our protocol. This prove is non-trivial in two aspects. First, Vitalik, Grunspan and Perez-Marco's informal arguments do not rule out the possibility that the attacker adapts to the modified mechanism and still gets unfair block reward. For example, the attacker may temporarily turn off some mining gears in the first epoch, causing the modified difficulty adjustment algorithm to underestimate the network's computing power, and starts selfish mining in the second epoch for a higher overall time-averaged reward. We prove that in our defense works regardless of how the attacker divides its mining power among honest mining, selfish mining and idle, and how many epochs the attack involves. Second, it is always possible that the attacker may invalidate the last few honest blocks in an epoch, preventing these blocks from being incorporated in the difficulty adjustment. We quantify the upper bound of the damage caused by this "orphan exclusion attack" in our proof, which will be released later.

<a name="Specification"></a>
## Specification

<a name="Two-Step-Transaction-Confirmation"></a>
### Two-Step Transaction Confirmation

In our protocol, we use a two-step transaction confirmation to eliminate the aforementioned block propagation bottleneck, regardless of how short the block interval is. We start by defining the two steps and the block structure, and then introduce the new block propagation protocol. 

#### Definitions

> **Definition 1:** A transaction’s proposal id `txpid` is defined as the first *p_len* bits of the transaction hash `txid`.

In our protocol, `txpid` does not need to be as globally unique as `txid`, as a `txpid` is used to identify a transaction among several neighboring blocks. Since we embed `txpid`s in both blocks and compact blocks, sending only the truncated `txid`s could reduce the bandwidth consumption. 

When multiple transactions share the same `txpid`s, all of them are considered proposed. In practice, we can set *p_len* to be large enough so that the computational effort of finding a collision is non-trivial.

> **Definition 2:** A block *B*<sub>1</sub> is considered to be the *uncle* of another block *B*<sub>2</sub> if all of the following conditions are met:
>​	(1) *B*<sub>1</sub> and *B*<sub>2</sub> are in the same epoch, sharing the same difficulty;
>​	(2) height(*B*<sub>2</sub>) > height(*B*<sub>1</sub>);
>​	(3) *B*<sub>2</sub> is the first block in its chain to refer to *B*<sub>1</sub>. 

Our uncle definition is different from [that of Ethereum](https://github.com/ethereum/wiki/wiki/White-Paper#modified-ghost-implementation), in that we do not consider how far away the two blocks' first common ancestor is, as long as the two blocks are in the same epoch.

> **Definition 3:** A transaction is *proposed* at height *h*<sub>p</sub> if its `txpid` is in the proposal zone of the main chain block with height *h*<sub>p</sub> and this block’s uncles. 

It is possible that a proposed transaction is previously proposed, in conflict with other transactions, or even malformed. These incidents do not affect the block’s validity, as the proposal zone is used to facilitate transaction synchronization.

> **Definition 4:** A non-coinbase transaction is *committed* at height *h*<sub>c</sub> if all of the following conditions are met: 
> ​	(1) the transaction is proposed at height *h*<sub>p</sub> of the same chain, and *w<sub>close</sub>  ≤  h<sub>c</sub> − h*<sub>p</sub>  ≤  *w<sub>far</sub>*
> ​	(2) the transaction is in the commitment zone of the main chain block with height *h*<sub>c</sub>; 
> ​	(3) the transaction is not in conflict with any previously-committed transactions in the main chain. 
> The coinbase transaction is committed at height *h*<sub>c</sub> if it satisfies (2).

*w<sub>close</sub>* and *w<sub>far</sub>* define the closest and farthest on-chain distance between a transaction’s proposal and commitment. We require *w<sub>close</sub>*  to be large enough so that *w<sub>close</sub>* block intervals are long enough for a transaction to be propagated to the network. 

These two parameters are also set according to the maximum number of transactions in the proposed transaction pool of a node’s memory. As the total number of proposed transactions is limited, they can be stored in the memory so that there is no need to fetch a newly committed transaction from the hard disk in most occasions. 

A transaction is considered embedded in the blockchain when it is committed. Therefore, a receiver that requires σ confirmations needs to wait for at least *w<sub>close</sub>* +σ blocks after the transaction is broadcast to have confidence in the transaction. 

In practice, this *w<sub>close</sub>* - block extra delay is compensated by our protocol’s shortened block interval, so that the usability is not affected.

#### Block and Compact Block Structure

A block in our protocol includes the following fields:

| Name            | Description                          |
| :-------------- | :----------------------------------- |
| header          | block metadata                       |
| commitment zone | transactions committed in this block |
| proposal zone   | `txpid`s proposed in this block      |
| uncle headers   | headers of uncle blocks              |
| uncles’ proposal zones   | `txpid`s proposed in the uncles              |

Similar to NC, in our protocol, a compact block replaces a block’s commitment zone with the transactions’ `shortid`s, a salt and a list of prefilled transactions. All other fields remain unchanged in [the compact block](https://github.com/bitcoin/bips/blob/master/bip-0152.mediawiki).

Additional block structure rules:

- The total size of the first four fields should be no larger than the hard-coded **block size limit**. The main purpose of implementing a block size limit is to avoid overloading public nodes' bandwidth. The uncle blocks’ proposal zones do not count in the limit as they are usually already synchronized when the block is mined. Since [RFC31], the new field `extension` is also counted in the total size.
- The number of `txpid`s in a proposal zone also has a hard-coded upper bound.

[RFC31]: ../0031-variable-length-header-field/0031-variable-length-header-field.md

Two heuristic requirements may help practitioners choose the parameters. First, the upper bound number of `txpid`s in a proposal zone should be no smaller than the maximum number of committed transactions in a block, so that even if *w<sub>close</sub>=w<sub>far</sub>*, this bound is not the protocol's throughput bottleneck. Second, ideally the compact block should be no bigger than 80 KB. According to [a 2016 study by Croman et al.](https://fc16.ifca.ai/bitcoin/papers/CDE+16.pdf), messages no larger than 80 KB have similar propagation latency in the Bitcoin network; larger messages propagate slower as the network throughput becomes the bottleneck. This number may change as the network condition improves.

#### Block Propagation Protocol

In line with [[1](https://www.cs.cornell.edu/~ie53/publications/btcProcFC.pdf), [2](https://arxiv.org/abs/1312.7013), [3](https://eprint.iacr.org/2014/007.pdf)], nodes should broadcast all blocks with valid proofs-of-work, including orphans, as they may be referred to in the main chain as uncles. Valid proofs-of-work cannot be utilized to pollute the network, as constructing them is time-consuming. 

Our protocol’s block propagation protocol removes the extra round trip of fresh transactions in most occasions. When the round trip is inevitable, our protocol ensures that it only lasts for one hop in the propagation. This is achieved by the following three rules: 

**R1: non-blocking transaction query.**
As soon as the commitment zone is reconstructed, a node forwards the CBs to its downstream peers and queries the newly-proposed transactions from its upstream peers simultaneously.

The block propagation will not be affected by these transaction queries as long as they are answered before the next $w_{\rm close}$-th block is mined.
Transactions are validated as soon as their full content is received.
The `txpid`s and their block heights are stored in the memory until they are no longer in the proposal window, regardless of whether their corresponding transactions are missing or invalid.
This will not become a DoS attack vector as the maximum sizes of the proposal window and the proposal zone are hard-coded.

To prevent the attacker from launching memory exhaustion attacks with large-sized transactions, an additional upper limit is prescribed on the total size of all newly-proposed transactions in a proposal zone.
Once this limit is reached, the node (1) deletes all large-sized transactions that appear only in this proposal zone from its memory pool, and (2) blacklists the upstream peer that contributes the most to these large-sized transactions.
The threshold for tagging large-sized transactions is not a consensus parameter, and thus can be set locally, as, e.g., twice the average size of transactions confirmed in the ten most recent blocks.

**R2: missing transactions, now or never.**
If certain committed transactions are unknown to a CB receiver, the receiver queries the sender with a short timeout.
Failure to send these transactions in time leads to the receiver turning off the HB mode (following the notations of [Compact Blocks](https://bitcoincore.org/en/2016/06/07/compact-blocks-faq/)) for the sender and turning on the HB mode for the next fastest peer.
If the downgraded sender was an outgoing connection, the receiver establishes a new connection to a random node.
Moreover, the incomplete block will not be propagated further before receiving these transactions from another peer.
No punishment is prescribed to upstream peers who do not respond to the queries on newly-proposed transactions, as it is difficult to locate the responsible parties for the delay.

Proposed-but-not-received transactions are committed either (1) in a successful transaction withholding attack, or (2) when $w_{\rm close}$ consecutive blocks are mined before the transactions proposed in the first one are synchronized.
If the upstream peer is honest, as in (2), a short timeout is adequate to transfer the missing transactions, as an honest upstream peer must not send the CBs before receiving these transactions.
In the case of (1), the attacker cannot delay the first hop of the block propagation more than the timeout value without the block being discarded.
In practice, we set the timeout to be 3.5 seconds, which is adequate for the round trip in 95\% of the cases according to our measurement (see the academic paper).

**R3: transaction push.**
If certain committed transactions are previously unknown to a CB sender, they will be embedded in the prefilled transaction list of the outgoing CBs.

This rule removes the round trip if the sender and the receiver share the same list of proposed-but-not-broadcast transactions.
In a transaction withholding attack or a `txpid` collision, this rule ensures that the secret transactions are only queried in the first hop of the block's propagation, and then pushed directly to the receivers in subsequent hops.

<a name="Dynamic-Difficulty-Adjustment-Mechanism"></a>
### Dynamic Difficulty Adjustment Mechanism

Our two-step mechanism enables us to lower the expected block interval.
The next challenge is to locate the interval that best utilizes the nodes' bandwidth without affecting security.
To tackle this challenge, we introduce an accurate dynamic DAM that exploits the bandwidth utilization to the limit of the real-time network condition.
Our goal is twofold: (**G1**) to render selfish mining unprofitable; (**G2**) to dynamically adjust the throughput based on the network's bandwidth and latency.
Meanwhile, to maximize compatibility and attack resistance, our DAM needs to satisfy four constraints, in line with NC:

**C1**. All epochs have the same target duration $L_{\rm ideal}$.

**C2.** The maximum block reward issued in an epoch $R(i)$ depends only on the epoch number $i$ so that the rewards are distributed at a predetermined rate.

**C3.** The hash rate estimation of the last epoch does not change too fast, to prevent attackers from [manipulating the DAM and forging a blockchain](https://arxiv.org/abs/1312.7013), even if some miners' network is temporarily controlled by the attacker.

**C4.** The expected block interval should abide by predetermined upper and lower bounds.
The upper bound guarantees service availability; the lower bound guarantees that NC-Max does not generate more traffic than most nodes' capacity, thus ensuring decentralization.

To achieve **G1**, NC-Max incorporates all blocks, instead of only the main chain, in calculating the *hash rate estimation* of the last epoch, and then applies a dampening factor to the estimation so that the adjusted output conforms to **C3**.
This output determines the computing efforts required in the next epoch for each reward unit.
To achieve **G2**, our DAM targets a fixed orphan rate $o_{\rm ideal}$, rather than a fixed block interval as in NC.
As our two-step confirmation ensures a relatively stable block propagation process, we can solve the expected block interval matching the target orphan rate with the last epoch's duration, orphan rate, and the main chain block number.
As the target epoch duration is fixed (**C1**), we can solve the next epoch's main chain block number, block reward, and difficulty target after applying several dampening factors and upper/lower bounds to safeguard **C2** and **C4**.

Combined with the two-step mechanism, targeting a fixed orphan rate allows us to pipeline the synchronization of previously-proposed transactions and the confirmation of recently-committed transactions, reducing NC's long idle time.

#### Notations

Similar to Nakamoto Consensus , our protocol’s difficulty adjustment algorithm is executed at the end of every epoch. It takes four inputs:

| Name            | Description                          |
| :-------------- | :----------------------------------- |
| *T*<sub>*i*</sub>          | Last epoch’s target                       |
| *L*<sub>*i*</sub> | Last epoch’s duration: the timestamp difference between epoch *i* and epoch (*i* − 1)’s last blocks |
| *C*<sub>*i*,m</sub>   | Last epoch’s main chain block count      |
| *C*<sub>*i*,o</sub>   | Last epoch’s orphan block count:  the number of uncles embedded in epoch *i*’s main chain         |

Among these inputs, *T<sub>i</sub>* and *C*<sub>*i*,m</sub> are determined by the last iteration of difficulty adjustment; *L*<sub>*i*</sub> and *C*<sub>*i*,o</sub> are measured after the epoch ends. The orphan rate *o*<sub>*i*</sub> is calculated as *C*<sub>*i*,o</sub> / *C*<sub>*i*,m</sub>. We do not include *C*<sub>*i*,o</sub> in the denominator to simplify the equation. As some orphans at the end of the epoch might be excluded from the main chain by an attack, *o*<sub>*i*</sub> is a lower bound of the actual number. However, [the proportion of deliberately excluded orphans is negligible](https://eprint.iacr.org/2014/765.pdf) as long as the epoch is long enough, as the difficulty of orphaning a chain grows exponentially with the chain length. Yet we still quantify the effect of these blocks on our defense in our second academic paper.

The algorithm outputs three values:

| Name            | Description                          |
| :-------------- | :----------------------------------- |
| *T*<sub>*i*+1</sub>          | Next epoch’s target                       |
| *C*<sub>i+1,m</sub> | Next epoch’s main chain block count |
| *r*<sub>*i*+1</sub>   | Next epoch’s block reward     |

If the network hash rate and block propagation latency remains constant, *o*<sub>*i*+1</sub> should reach the ideal value *o*<sub>ideal</sub>, unless *C*<sub>*i*+1,m</sub> is equal to its upper bound *C*<sub>m</sub><sup>max</sup>  or its lower bound *C*<sub>m</sub><sup>min</sup> . Epoch *i* + 1 ends when it reaches *C*<sub>*i*+1,m</sub> main chain blocks, regardless of how many uncles are embedded.

#### Computing the Adjusted Hash Rate Estimation

The adjusted hash rate estimation, denoted as $\hat{H}\_i$ is computed by applying a dampening factor τ to the last epoch’s actual hash rate $\hat{H}\_i'$. The actual hash rate is calculated as follows:

$$
\hat{H}\_i'
= \frac{\rm HSpace}{T_i}\cdot (C_{i,{\rm m}}+C_{i,{\rm o}})/L_i \enspace, ~~(4)
$$

where:

- HSpace is the size of the entire hash space, e.g., $2^{256}$ in Bitcoin,
- HSpace/*T<sub>i</sub>* is the expected number of hash operations to find a valid block, and 
- *C*<sub>*i*,m</sub> + *C*<sub>*i*,o</sub> is the total number of blocks in epoch *i*

$\hat{H}_i'$ is computed by dividing the expected total hash operations with the duration *L<sub>i</sub>*.

Note that (4) is the equation number. We choose not to start from (1) in order to be consistent with a prior version of the academic paper.

Now we apply the dampening filter:

$$
\hat{H}\_i =
	\left\lbrace
	\begin{array}{ll}
		{\hat{H}}\_{i-1} \cdot\frac{1}{\tau\_1}, &\hat{H}\_i'<\hat{H}\_{i-1}\cdot\frac{1}{\tau\_1}\\
		{\hat{H}}\_{i-1}\cdot \tau\_1, &\hat{H}\_i'>{\hat{H}}\_{i-1}\cdot \tau\_1\\
		{\hat{H}\_i'}, & \text{otherwise}\\
	\end{array}
	\right.\enspace ,
 $$

where ${\hat{H}}\_{i-1}$ denotes the adjusted hash rate estimation output by the last iteration of the difficulty adjustment algorithm. The dampening factor ensures that the adjusted hash rate estimation does not change more than a factor of $\tau\_1$ between two consecutive epochs. This adjustment is equivalent to the Nakamoto Consensus application of a dampening filter. Bounding the adjustment speed prevents the attacker from arbitrarily biasing the difficulty and forging a blockchain, even if some victims’ network is temporarily controlled by the attacker.

#### Modeling the Block Propagation

It is difficult, if not impossible, to model the detailed block propagation procedure, given that the network topology changes constantly over time. Luckily, for our purpose, it is adequate to express the influence of block propagation with two parameters, which will be used to compute *C*<sub>*i*+1,m</sub>  later.

We assume all blocks follow a similar propagation model, in line with [[1](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.395.8058&rep=rep1&type=pdf), [2](https://fc16.ifca.ai/bitcoin/papers/CDE+16.pdf)]. In the last epoch, it takes *d* seconds for a block to be propagated to the entire network, and during this process, the average fraction of mining power working on the block’s parent is *p*. Therefore, during this *d* seconds, $\hat{H}\_i'\times dp$ hash operations work on the parent, thus not contributing to extending the blockchain. 
Consequently, in the last epoch, the total number of hashes that do not extend the blockchain is $\hat{H}\_i'\times dp \times C\_{i,{\rm m}}$.

On the other hand, the number of hash operations working on observed orphaned blocks is ${\rm HSpace}/T_i\times C_{i,{\rm o}}$.
When the orphan rate is relatively low, we can ignore the rare event that more than two competing blocks are found at the same height. Therefore we have

$$
	\hat{H}\_i'\times dp \times C\_{i,{\rm m}}={\rm HSpace}/T_i\times C_{i,{\rm o}}\enspace.~~(5)
$$

If we combine Eqn. (4) and (5), we can solve $dp$:

$$
	dp=\frac{{\rm HSpace}/T_i\times C_{i,{\rm o}}}{\hat{H}\_i' \times C\_{i,{\rm m}}}=\frac{o_i\times L_i}{(1+o_i)C_{i,{\rm m}}}\enspace,~~(6)
$$

#### Computing the Outputs

**Main Chain Block Count.**
If the next epoch's block propagation situation is identical to the last epoch's, the value $dp$ should remain unchanged.
In order to achieve the ideal orphan rate $o_{\rm ideal}$ and **C1**---the ideal epoch duration $L_{\rm ideal}$, following the same reasoning with Eqn. (6), we should have

$$
	dp=\frac{o_{\rm ideal}\times L_{\rm ideal}}{(1+o_{\rm ideal})C_{i+1,{\rm m}}'}\enspace,~~(7)
$$

where $C_{i+1,{\rm m}}'$ is the number of main chain blocks in the next epoch, if our only goal is to achieve $o_{\rm ideal}$ and $L_{\rm ideal}$.

By combining Eqn. (6) and (7), when $o_i\neq 0$, we can solve $C_{i+1,{\rm m}}'$:

$$
	C_{i+1,{\rm m}}'=\frac{o_{\rm ideal}(1+o_i)\times L_{\rm ideal}\times C_{i,{\rm m}}}{o_i(1+o_{\rm ideal})\times L_i}\enspace.~~(8)
$$

Now in order to achieve **C4**, we can apply the upper and lower bounds to $C_{i+1,{\rm m}}'$ and get $C_{i+1,{\rm m}}$:

$$
	C_{i+1,{\rm m}} =
	\left\lbrace
	\begin{array}{ll}
		\min\lbrace C_{\rm m}^{\rm max}, \tau_2 C_{i, {\rm m}}\rbrace, &\\
		~~o_i=0~\text{or}~C_{i+1,{\rm m}}'>\min\lbrace C_{\rm m}^{\rm max}, \tau_2 C_{i, {\rm m}}\rbrace\\
		\max\lbrace C_{\rm m}^{\rm min}, C_{i, {\rm m}}/\tau_2\rbrace, &\\
		~~C_{i+1,{\rm m}}'<\max\lbrace C_{\rm m}^{\rm max}, C_{i, {\rm m}}/\tau_2\rbrace\\
		C_{i+1,{\rm m}}', & \text{otherwise}\\
	\end{array}
	\right.\enspace .~~(9)
$$

Equation (9) also covers the case of $o_i=0$.
When $L_{\rm ideal}$ is fixed (**C1**), a lower bound on $C_{i+1,{\rm m}}$ is equivalent to an upper bound on the expected block interval $\overline{t_{\rm in}}$, and vice versa.
The dampening factor $\tau_2$, also instantiated as 2 in Nervos CKB, serves both **C3** and **C4**, preventing the main chain block number from changing too fast.

**Target.**
To compute the target, we introduce an adjusted orphan rate estimation $o_{i+1}'$:

$$
	o_{i+1}' =
	\left\lbrace
	\begin{array}{ll}
		0, & o_i=0\\
		o_{\rm ideal}, & C_{i+1,{\rm m}}=C_{i+1,{\rm m}}'\\
		1/(\frac{(1+o_i)\cdot L_{\rm ideal}\cdot C_{i,{\rm m}}}{o_i\cdot L_i\cdot C_{i+1,{\rm m}}}-1), & \text{otherwise}\\
	\end{array}
	\right.\enspace .
$$

Using $o_{i+1}'$ instead of $o_{\rm ideal}$ prevents some undesirable situations when $C_{i+1,{\rm m}}$ reaches its upper or lower bound. Now we can compute $T_{i+1}$:

$$
	T_{i+1} ={\rm HSpace}/\frac{{\hat{H}\_i} \cdot L\_{\rm ideal}}{(1+o_{i+1}')\cdot C_{i+1,{\rm m}}}
	\enspace,~~(10)
$$

where ${\hat{H}\_i}\cdot L\_{\rm ideal}$ is the total number of hashes, $(1+o_{i+1}')\cdot C_{i+1,{\rm m}}$ is the total number of blocks.
The denominator in Eqn. (10) is the number of hashes required to find a block.

Note that if none of the edge cases is triggered, i.e., $\hat{H}\_i=\hat{H}\_i'$ and $C\_{i+1,{\rm m}}=C\_{i+1,{\rm m}}'$, we can combine Eqn. (4), (8), (10) and get $T_{i+1}' =T_i \times o_{\rm ideal}/o_i$.
This result is consistent with our intuition.
On the one hand, if $o_i$ is larger than the ideal value $o_{\rm ideal}$, the target lowers, increasing the difficulty of finding a block and raising the block interval if the hash rate is unchanged. Therefore, the orphan rate is lowered as it is more unlikely to find a block during another block's propagation.
On the other hand, the target increases if the last epoch's orphan rate is lower than the ideal value, decreasing the block interval and raising the system's throughput.

#### Computing the Reward for Each Block

Now we can compute the reward for each block:

$$
r_{i+1}=\min\left\lbrace\frac{R(i+1)}{C_{i+1,{\rm m}}}, \frac{R(i+1)}{C_{i+1,{\rm m}}'}\cdot \frac{T_{i+1}'}{T_{i+1}}\right\rbrace\enspace,~~(11)
$$

The two cases differ only in the edge cases. The first case guarantees that the total reward issued in epoch *i* + 1 will not exceed R(*i* + 1).

<a name="Protocol-Parameters"></a>
### Protocol Parameters

We now list the actual parameters used when implementing this protocol in Nervos CKB.

| Name/Explanation            | Related Name in our Client | Value |
| :-------------- | :----------------------------------- | :-------------- |
| maximum number of uncles per block          | MAX_UNCLE_NUM | 2 |
| $o_{\rm ideal}$ | DEFAULT_ORPHAN_RATE_TARGET | 2.5% |
| $L_{\rm ideal}$ | DEFAULT_EPOCH_DURATION_TARGET | 4 hour |
| *w<sub>close</sub>* | TX_PROPOSAL_WINDOW | 2 |
| *w<sub>far</sub>* | TX_PROPOSAL_WINDOW | 10 |
| $C_{\rm m}^{\rm max}$ | MAX_EPOCH_LENGTH | 1800 |
| $C_{\rm m}^{\rm min}$ | MIN_EPOCH_LENGTH | 300 |
| block size limit   | MAX_BLOCK_BYTES     | 597000 |
| block cycle limit   | MAX_BLOCK_CYCLES     | 3500000000 |
| The upper bound number of `txpid`s in a proposal zone | MAX_BLOCK_PROPOSALS_LIMIT | 1500 |
| Proposer fee share |PROPOSER_REWARD_RATIO | 40% |
| Before which a block reward cannot be spent | CELLBASE_MATURITY | 4 epoch |

Several other parameters are implemented in the consensus component of our client. While a detailed description of these parameters falls outside the scope of this document, you can find them in [RFC15](https://github.com/nervosnetwork/rfcs/blob/master/rfcs/0015-ckb-cryptoeconomics/0015-ckb-cryptoeconomics.md).

* DEFAULT_SECONDARY_EPOCH_REWARD: 1.344 billion per year
* INITIAL_PRIMARY_EPOCH_REWARD: 4.2 billion per year
* DEFAULT_PRIMARY_EPOCH_REWARD_HALVING_INTERVAL: 4 years


================================================
File: rfcs/0021-ckb-address-format/0021-ckb-address-format.md
================================================
---
Number: "0021"
Category: Standards Track
Status: Active
Author: Cipher Wang <cipher@nervina.io>, Axel Wan <wancencen@cryptape.com>
Created: 2019-01-20
---

# CKB Address Format

## Abstract

*CKB Address Format* is an application level cell **lock script** display recommendation. The lock script consists of three key parameters, including *code_hash*, *hash_type* and *args*. CKB address packages lock script into a single line, verifiable, and human read friendly format.

## Data Structure

### Payload Format Types

To generate a CKB address, we firstly encode lock script to bytes array, name *payload*. And secondly, we wrap the payload into final address format.

There are several methods to convert lock script into payload bytes array. We use 1 byte to identify the payload format.

| format type |                   description                                |
|:-----------:|--------------------------------------------------------------|
|  0x00       | full version identifies the hash_type                        |
|  0x01       | short version for locks with popular code_hash, deprecated   |
|  0x02       | full version with hash_type = "Data", deprecated             |
|  0x04       | full version with hash_type = "Type", deprecated             |

### Full Payload Format

Full payload format directly encodes all data fields of lock script.
The encode rule of full payload format is Bech32m.

```c
payload = 0x00 | code_hash | hash_type | args
```

The `hash_type` field is for CKB VM version selection.

* When the hash_type is 0, the script group matches code via data hash and will run the code using the CKB VM version 0.
* When the hash_type is 1, the script group matches code via type script hash and will run the code using the CKB VM version 1.
* When the hash_type is 2, the script group matches code via data hash and will run the code using the CKB VM version 1.

### Deprecated Short Payload Format

Short payload format is a compact format which identifies common used [code_hash][genesis-script-list] by 1 byte code_hash_index instead of 32 bytes code_hash.
The encode rule of short payload format is Bech32.

```c
payload = 0x01 | code_hash_index | args
```

To translate payload to lock script, one can convert code_hash_index to code_hash and hash_type with the following *popular code_hash table*. And args as the args.

| code_hash_index |        code_hash     |   hash_type  |          args           |
|:---------------:|----------------------|:------------:|-------------------------|
|      0x00       | SECP256K1 + blake160 |     Type     |  blake160(PK)*          |
|      0x01       | SECP256K1 + multisig |     Type     |  multisig script hash** |
|      0x02       | anyone_can_pay       |     Type     |  blake160(PK)           |

\* The blake160 here means the leading 20 bytes truncation of Blake2b hash result.

\*\* The *multisig script hash* is the 20 bytes blake160 hash of multisig script. The multisig script should be assembled in the following format:

```
S | R | M | N | blake160(Pubkey1) | blake160(Pubkey2) | ...
```

Where S/R/M/N are four single byte unsigned integers, ranging from 0 to 255, and blake160(Pubkey1) it the first 160bit blake2b hash of SECP256K1 compressed public keys. S is format version, currently fixed to 0. M/N means the user must provide M of N signatures to unlock the cell. And R means the provided signatures at least match the first R items of the Pubkey list.

For example, Alice, Bob, and Cipher collectively control a multisig locked cell. They define the unlock rule like "any two of us can unlock the cell, but Cipher must approve". The corresponding multisig script is:

```
0 | 1 | 2 | 3 | Pk_Cipher_h | Pk_Alice_h | Pk_Bob_h
```

Notice that the length of args in payload here is always 20 bytes. So, if you want to append [CKByte minimum field or/and UDT minimum field](https://github.com/nervosnetwork/rfcs/blob/master/rfcs/0026-anyone-can-pay/0026-anyone-can-pay.md#script-structure) to anyone_can_pay script, you should use the full payload format.

### Deprecated Full Payload Format

The deprecated full payload format directly encodes all data field of lock script.
The encode rule of deprecated full payload format is Bech32.

```c
payload = 0x02/0x04 | code_hash | args
```

The first byte identifies the lock script's hash_type, 0x02 for "Data", 0x04 for "Type".

Two reasons have caused this address format to be deprecated. First, a [flaw](https://github.com/sipa/bech32/issues/51) of Bech32 enables attackers to generate valid but unexpected addresses by deleting or inserting characters into certain full addresses. Last, the hard fork of [ckb2021](https://github.com/nervosnetwork/rfcs/blob/master/rfcs/0037-ckb2021/0037-ckb2021.md) requires a new field to indicate the CKB VM version for each script group.

## Wrap to Address

We follow [Bitcoin bech32 address format (BIP-173)][bip173] or [Bitcoin bech32m address format (BIP-350)][bip350] rules to wrap payload into address, which uses Bech32/Bech32m encoding and a [BCH checksum][bch].

The original version of Bech32/Bech32m allows at most 90 characters long. Similar with [BOLT][BOLT_url], we simply remove the length limit. The error correction function is disabled when the Bech32/Bech32m string is longer than 90. We don't intent to use this function anyway, because there is a risk to get wrong correction result.

A Bech32/Bech32m string consists of the **human-readable part**, the **separator**, and the **data part**. The last 6 characters of data part is checksum. The data part is base32 encoded. Here is the readable translation of base32 encoding table.

|       |0|1|2|3|4|5|6|7|
|-------|-|-|-|-|-|-|-|-|
|**+0** |q|p|z|r|y|9|x|8|
|**+8** |g|f|2|t|v|d|w|0|
|**+16**|s|3|j|n|5|4|k|h|
|**+24**|c|e|6|m|u|a|7|l|

The human-readable part is "**ckb**" for CKB mainnet, and "**ckt**" for the testnet. The separator is always "1".

![](images/ckb-address.png)

## Examples and Demo Code

```yml
== full address test ==
code_hash to encode:     9bd7e06f3ecf4be0f2fcd2188b23f1b9fcc88e5d4b65a8637b17723bbda3cce8
hash_type to encode:     01
with args to encode:     b39bbc0b3673c7d36450bc14cfcdad2d559c6c64
full address generated:  ckb1qzda0cr08m85hc8jlnfp3zer7xulejywt49kt2rr0vthywaa50xwsqdnnw7qkdnnclfkg59uzn8umtfd2kwxceqxwquc4

== deprecated short address (code_hash_index = 0x00) test ==
args to encode:          b39bbc0b3673c7d36450bc14cfcdad2d559c6c64
address generated:       ckb1qyqt8xaupvm8837nv3gtc9x0ekkj64vud3jqfwyw5v

== deprecated short address (code_hash_index = 0x01) test ==
multi sign script:       00 | 01 | 02 | 03 | bd07d9f32bce34d27152a6a0391d324f79aab854 | 094ee28566dff02a012a66505822a2fd67d668fb | 4643c241e59e81b7876527ebff23dfb24cf16482
args to encode:          4fb2be2e5d0c1a3b8694f832350a33c1685d477a
address generated:       ckb1qyq5lv479ewscx3ms620sv34pgeuz6zagaaqklhtgg

 == deprecated full address test ==
code_hash to encode:     9bd7e06f3ecf4be0f2fcd2188b23f1b9fcc88e5d4b65a8637b17723bbda3cce8
with args to encode:     b39bbc0b3673c7d36450bc14cfcdad2d559c6c64
full address generated:  ckb1qjda0cr08m85hc8jlnfp3zer7xulejywt49kt2rr0vthywaa50xw3vumhs9nvu786dj9p0q5elx66t24n3kxgj53qks
```

Demo code: https://github.com/rev-chaos/ckb-address-demo

[bip173]: https://github.com/bitcoin/bips/blob/master/bip-0173.mediawiki

[bip350]: https://github.com/sipa/bips/blob/bip-bech32m/bip-0350.mediawiki

[bch]: https://en.wikipedia.org/wiki/BCH_code

[BOLT_url]: https://github.com/lightningnetwork/lightning-rfc/blob/master/11-payment-encoding.md

[multisig_code]: https://github.com/nervosnetwork/ckb-system-scripts/blob/master/c/secp256k1_blake160_multisig_all.c

[genesis-script-list]: https://github.com/nervosnetwork/rfcs/blob/master/rfcs/0024-ckb-genesis-script-list/0024-ckb-genesis-script-list.md


================================================
File: rfcs/0022-transaction-structure/0022-transaction-structure.md
================================================
---
Number: "0022"
Category: Informational
Status: Draft
Author: Ian Yang <@doitian>
Created: 2019-08-26
---

# CKB Transaction Structure

This RFC is about an essential data structure in CKB, the transaction. CKB is under active development. At the time of writing, the corresponding CKB version is v0.25.0.

The document contains two parts. The first one covers the core transaction features, and the second one introduces some extensions.

![](transaction-overview.png)

The diagram above is an overview of the transaction structure. Instead of explaining field by field, the following paragraphs introduce various features which the CKB transaction provides and how the fields play their roles in these features.

## TOC

* [Part I: Core Features](#part-i-core-features)
  + [Value Storage](#value-storage)
  + [Cell Data](#cell-data)
  + [Code Locating](#code-locating)
  + [Lock Script](#lock-script)
  + [Type Script](#type-script)
  + [Recap of The Transaction Structure in Part I](#recap-of-the-transaction-structure-in-part-i)
* [Part II: Extensions](#part-ii-extensions)
  + [Dep Group](#dep-group)
  + [Upgradable Script](#upgradable-script)
  + [Type ID](#type-id)
  + [Header Deps](#header-deps)
  + [Other Fields](#other-fields)
  + [Exceptions](#exceptions)
* [Appendix A: Compute Various Hash](#appendix-a-compute-various-hash)
  + [Crypto Primitives](#crypto-primitives)
  + [Transaction Hash](#transaction-hash)
  + [Cell Data Hash](#cell-data-hash)
  + [Script Hash](#script-hash)

## Part I: Core Features

### Value Storage

CKB adopts UTXO model. A transaction destroys some outputs created in previous transactions and creates some new outputs. We call the transaction output a cell in CKB. Thus the name cell and transaction output are interchangeable.

The following diagram shows the fields used in this layer.

![](value-storage.png)

The transaction destroys the cells in `inputs` and creates the cells in `outputs`. 

The CKB chain packages transactions into blocks. We can use block number to refer to a block in the chain, which is an increasing non-negative integer starting from 0, the genesis block. The transactions in a block are also ordered. We say a block is older if it has a smaller block number, a transaction is older if either it is in an older block, or its position in a block is before another transaction. In the following example, Block i is older than Block i + 1. Transaction tx1 is older than tx2 and is older than tx3.

![Block i is older than Block i + 1. Transaction tx1 is older than tx2 and is older than tx3.](older-block-and-transaction.png)

A live cell is the one that appears as an output but not as an input in all the older transactions. A dead cell has already been used as an input in any older transaction. A transaction can only use live cells as inputs.

We can compute the transaction hash from all transaction fields except `witnesses`. See Appendix A how to calculate the transaction hash.

The transaction hash is considered unique. Since cell is always created by a transaction, and every new cell has its position in the transaction outputs array, we can refer to a cell by transaction hash and outputs index. The structure `OutPoint` is just such reference type. The transaction uses `OutPoint` in inputs to reference the previously created cells instead of embedding them.

![](out-point.png)

The cell stores the CKB Token in the field `capacity`. A transaction cannot mint capacities from the air, so a transaction must meet the following rule:

```json
sum(cell's capacity for each cell in inputs)
≥ sum(cell's capacity for each cell in outputs)
```

Miners can collect the difference as a fee.

```json
fee = sum(cell's capacity for each cell in inputs)
    - sum(cell's capacity for each cell in outputs)
```

If you are familiar with Bitcoin, you'll find out that the Value Storage layer is similar to Bitcoin, but lacking the locking script to protect the transaction output ownership. CKB does have that feature, but before that, I have to introduce Cell Data and Code Locating layer, which are the dependencies of any scripting feature in CKB.

### Cell Data

Instead of holding only the token value, CKB cell can store arbitrary data as well.

![](cell-data.png)

The field `outputs_data` is a parallel array of outputs. The data of the i-th cell in `outputs` is the i-th item in `outputs_data`.

![](outputs-data.png)

The `capacity` in the cell is not only just the amount of the stored tokens, but it is also a limit on how many data the cell can store. That's where the name comes from, It is the storage capacity of the cell.

The capacity is not only used to store data but it also has to cover all the fields in the cell, including `data`,  `lock`, `type`, and `capacity` itself.

The transaction must create an output cell which occupied capacity is less than the cell capacity.

```json
occupied(cell) ≤ cell's capacity
```
### Code Locating

The cell has two fields which type is `Script`. The CKB VM will run the `lock` scripts of all the cells in inputs, and run the `type` scripts of all the cells in both inputs and outputs.

We differentiate the terms script and code.

- A **script** is a data structure represents a reference to a piece of on-chain runnable code.
- A **code** is the RISC-V binary, it's runnable in CKB-VM and can be referred to by `script` structure.
- A **code cell** is a cell whose data is RISC-V binary code.

The script does not include the code directly. See the script structure below. Let's ignore the hash type `Type` and the field `args` now.

![](script.png)

When a CKB VM needs to run a script, it must find its code first. The fields `code_hash` and `hash_type` are used to locate the code.

In CKB, the script code is compiled into RISC-V binary. The binary is stored as the data in a cell. When `hash_type` is "Data",  the script locates a cell whose data's hash equals the script's `code_hash`. The **cell data hash**, as the name suggests, is computed from the cell data (see Appendix A). The scope is limited in the transaction, script can only find a matched cell from `cell_deps`.

![](cell-deps.png)

The following diagram shows how CKB finds a matched script code.

![](code-locating.png)

The general workflow of using a script looks like below:

- Deploy code cell
    - Compile your code into RISC-V binary. You can find some examples in the [repository](https://github.com/nervosnetwork/ckb-system-scripts) which builds the code for system cells.
    - Create a cell which stores the binary as data in a transaction, and send the transaction to the chain.
- Use code cell
    - Construct a script structure, which `hash_type` is "Data"[^1], and `code_hash` is just the hash of the built binary.
    - Use the script as the type or the lock script in a cell.
    - Include the code cell's out point in the `cell_deps` if the script will be executed (i.e. an input's lock script, or a type script).

[^1]: Or "Data1" after [rfc32] is activated.

[rfc32]: ../0032-ckb-vm-version-selection/0032-ckb-vm-version-selection.md

The cells in `cell_deps` must be live, just like `inputs`. Unlike `inputs`, a cell only used in `cell_deps` is not considered dead.

The following two chapters will talk about how the script is used in a transaction to lock the cells and establish contracts on cells.

### Lock Script

Every cell has a lock script. The lock script must run when the cell is used as an input in a transaction. When the script only appears in the outputs, it is not required to reveal the corresponding code in `cell_deps`. A transaction is valid only when all the lock scripts in the inputs exit normally. Since the script runs on inputs, it acts as a lock to control who can unlock and destroy the cell, as well as spend the capacity stored in the cell.

![](lock-script.png)

Following is an example lock script code which always exits normally. Anyone can destroy the cell if it uses the code as the lock script.

```c
int main(int argc, char *argv[]) {
  return 0;
}
```

The most popular way to lock a digital asset is the digital signature created by asymmetric cryptography.

A lock based on asymmetric signatures:

- must contain the information of the public key, so that only a corresponding private key can create a valid signature.
- must verify the signature, which usually takes the whole transaction as the signing message, provided in a spending transaction is valid.

In CKB, the public key information can be stored in the `args` field in the script structure, and the signature can be stored in the `witnesses` fields in transaction. It is a recommended convention as used in the default [secp256k1 lock script](https://github.com/nervosnetwork/ckb-system-scripts/blob/master/c/secp256k1_blake160_sighash_all.c). The script code is able to read any part of a transaction, so the lock script can choose a different convention, for example, storing the public key information in the cell data.

![](lock-script-cont.png)

CKB does not run the lock script input by input. It first groups the inputs by lock script and runs the same script only once. CKB runs a script in 3 steps:

1. script grouping
2. code locating
3. execution

The diagram below shows the first two steps.

1. First, CKB groups inputs by lock script. In the example transaction, there are two different lock scripts used in inputs. Although they locate to the same code, they have different args. Let's focus on g1. It has two inputs with index 0 and 2. The script and the input indices will be used in step 3 later.
2. Then CKB locates the code from cell deps. It resolves to the cell with data hash `Hs` and uses its data as the code.

![](lock-script-grouping.png)

Then CKB will load the script's code and execute the code, starting from the entry function.

Various CKB syscalls are provided to help scripts read transaction data. These syscalls usually have an argument to specify where to read the data.

For example, to read the script itself:

```c
ckb_load_script(addr, len, offset)
```

To load the first witness:

```c
ckb_load_witness(addr, len, offset, 0, CKB_SOURCE_INPUT);
```

The first three arguments control where to store the read data and how many bytes to read. Let's ignore them in the following paragraphs.

The fifth argument is the data source. `CKB_SOURCE_INPUT` means reading from transaction inputs, and the fourth argument `0` is the index into the inputs array. `CKB_SOURCE_INPUT` is also used to read `witnesses`.

Remember that we have saved the indices of the input when grouping inputs by the lock script. This info is used to create the virtual witnesses and inputs array for the group. The code can read input or witness using the index in the virtual array via a special source `CKB_SOURCE_GROUP_INPUT`. Reading a witness using `CKB_SOURCE_GROUP_INPUT` just reads the witnesses which has the same position with the specified input.

![](group-input.png)

All the syscalls that read inputs data can use `CKB_SOURCE_GROUP_INPUT` and the index in the virtual inputs array, such as `ckb_load_cell_*` syscalls family.

### Type Script

Type script is similar to lock script, with two differences:

- Type script is optional.
- CKB run the type scripts in both inputs and outputs in a transaction.

Lock and type script bear different responsibilities. Lock scripts are for asset ownership, while type scripts are for application logic.

A lock script is only executed for inputs, so its primary responsibility is to authenticate the spending of cells. Only the owner is allowed to use the cell as input and spend the token stored along with it.

A type script is a state transition contract of the cells in the same type. When you see a cell output with a specified type, you are guaranteed that the cell has passed the validation rules coded in its tpye script. And the code is also executed when the cell is destroyed (when the cell is used as an input).

A typical use case of type script is in user-defined tokens - in token issuance, new tokens will be created as new cell outputs, so a type script is required to validate that new tokens are created according to rules.

The steps to run type script is also similar to lock script except

1. Cells without a type script are ignored.
2. Type scripts in both inputs and outputs will be used to form script groups.

![](type-script-grouping.png)

Like `CKB_SOURCE_GROUP_INPUT`, there's a special data source `CKB_SOURCE_GROUP_OUTPUT` to use the index into the virtual outputs array in the script group.

### Recap of The Transaction Structure in Part I

![](transaction-p1.png)

## Part II: Extensions

In part I, I have introduced the core features which the transaction provides. The features introduced in this part are extensions that are introduced to make the Cell model more powerful and also easier to use.

The diagram below is the overview of the new fields covered in this part.

![](transaction-p2.png)

### Dep Group

Dep Group is a cell which bundles several cells as its members. When a dep group cell is used in `cell_deps`, it has the same effect as adding all its members into `cell_deps`.

Dep Group stores the serialized list of `OutPoint` in cell data. Each `OutPoint` points to one of the group members.

The structure `CellDep` has a field `dep_type` to differentiate the normal cells which provide the code directly, and the dep groups which is expanded to its members inside `cell_deps`.

![](cell-dep-structure.png)

The dep group is expanded before code locate and execution phase, in which only the expanded `cell_deps` are visible.

![Example of Dep Group Expansion](dep-group-expansion.png)

Example: The lock script secp256k1 is split into code cell and data cell. The code cell depends on the data cell to work. If a transaction needs to consume a cell locked by secp256k1, it must have both cells in `cell_deps`. With dep group, the transaction only needs one dep group.

There are two reasons why secp256k1 is split into two cells:

- The code cell is small, which allows us to update it when the block size limit is low.
- The data cell can be shared. For example, we have implemented another lock script which uses ripemd160 to verify the public key hash. This script reuses the data cell.

### Upgradable Script

Because a script locates its code via cell data hash, once a cell is created its associated script code cannot change, since it is known infeasible to find a different piece of code that has the same hash. However, sometimes we do want to change the code without modifiying the script refers to it.

That's why script has another option for `hash_type`, named `Type`.

![](script-p2.png)

When the script uses the hash type `Type`, it matches the cell whose **type script hash** equals the `code_hash`. The type script hash is the hash computed from the cell's `type` field (see Appendix A).

![](code-locating-via-type.png)

Codes located by type script hash can be upgraded by replacing the code cell with a new code cell having the same type script. The new cell has the updated code. The transaction which include the new code cell in `dep_cells` will use the new version. Because the code upgrade requires the old code cell being spent, the upgrade rules (e.g. conditions under which the code cell can be upgraded) can be coded in the lock script of the code cell. 

`Type` hash type provides a mechanism for scripts to locate code in cells with certain type. As long as a cell's type script meets the criterias and is included in the `cell_deps`, its code will be executed. There could be multiple code cells with the same type script, providing different versions of code for the script. An adversary could create a flawed version and use it as an exploit, you should always keep this possibility in mind and be extremely cautious on using `Type` hash type. It's possible to implement logic in the code cell's type script to limit who can create cells with this certain type. The `Type ID` described below is another solution to this problem.

Because the code referenced by type script hash can change, you must trust the script author to use such kind of scripts - it's possible that bugs are introduced intentionally or unintentionally in an upgrade. The lock script of the code cell defines the upgradation policies, e.g. if it can be upgrade arbitrarily or only under certain conditions. It's recommended to check its upgradation policies before using a code cell via `Type` hash type.

### Type ID

Type ID describes a way of using a special type script which can create a singleton type - there's only one live cell of this type. With Type ID nobody could create another code cell with the same type script hash, which makes it a useful companion to `Type` hash type. 

The most common Type ID pattern involves several type scripts:

- The **Type ID code cell** is the cell which stores the code to verify that a type id is unique.
- The Type ID code cell has a type script as well. We don't care the actual content for now, let's assume the type script hash is TI.
- A **Type ID** is a type script which `hash_type` is "Type", and `code_hash` is TI.

![](type-id.png)

As a type script, Type ID type scripts in inputs and outputs will be grouped (we call it **type id groups**) then executed. All the inputs and outputs in a Type ID group have the same type id.

The Type ID code verifies that, in any type id group, there is at most one input and at most one output. A transaction could have multiple type id groups. Depending on the number of inputs and outputs, the type id groups are categorized into three different types:

- Type ID Creation Group has only one output.
- Type ID Deletion Group has only one input.
- Type ID Transfer Group has one input and one output.

![](type-id-group.png)

The transaction in the diagram above has all the three kinds of type id group. 

- G1 is a Type ID Transfer Group which transfers the type id from cell1 to cell4.
- G2 is a Type ID Deletion Group which deletes the type id along with cell2.
- G3 is a Type ID Creation Group which creates a new type id for cell3.

In the Type ID Creation Group, the only argument in `args` is the hash of this transaction first `CellInput` structure and the output index of the cell in the group. For example, in the group g3, id3 is a hash on `tx.inputs[0]` and 0 (cell3's index in `tx.outputs`).

There are two ways to create a new cell with a specific type id.

1. Create a transaction which uses any out point as `tx.inputs[0]` and has a output cell whose type script is Type ID. The output cell's type script `args` is the hash of `tx.inputs[0]` and its output index. Because any out point can only be used once as an input, `tx.inputs[0]` and thus the new type id must be different in each creation transaction.
2. Destroy an old cell with a specific type id and create a new cell with the same type id in the same transaction.

We assume that method 2 is the only way to create a cell which equals to an existing type id. Because it must consume the original type id cell, it requires the authorization of the type id's owner.

The Type ID type script can be implemented as an ordinaly script, but we choose to implement it in the CKB node as a special genesis script. Because if we want to make it upgradable, it has to use itself as the type script via type script hash, which is a recursive dependency.

![TI is a hash of the content which contains TI itself.](type-id-recursive-dependency.png)

As a genesis script, the Type ID code cell uses a special type script hash, which is just the ascii codes in hex of the text `TYPE_ID`.

```
0x00000000000000000000000000000000000000000000000000545950455f4944
```

### Header Deps

Header Deps allows the script to read block headers. This feature has some limitation to ensure the transaction is determined.

We say a transaction is determined that if all the scripts in the transaction have the determined results.

Header Deps allows the scripts to read a block header which hashes are listed in `header_deps`. There's another precondition that the transaction can only be added to the chain if all the block listed in `header_deps` are already in the chain (uncles excluded).

There are two ways to load a header in a script using syscall `ckb_load_header`:

- Via header deps index.
- Via an input or a cell dep. The syscall will return the block in which the cell is created if that block is listed in `header_deps`.

The second way to load a header has another benefit that the script knows the cell is in the loaded block. DAO withdraw transaction uses it to get the block number where the capacity was deposited.

![](header-deps.png)

Here are some examples of loading header In the diagram above.

```c
// Load the first block in header_deps, the block Hi
load_header(..., 0, CKB_SOURCE_HEADER_DEP);

// Load the second block in header_deps, the block Hj
load_header(..., 1, CKB_SOURCE_HEADER_DEP);

// Load the block in which the first input is created, the block Hi
load_header(..., 0, CKB_SOURCE_INPUT);

// Load the block in which the second input is created.
// Since the block creating cell2 is not in header_deps, this call loads nothing.
load_header(..., 1, CKB_SOURCE_INPUT);
```
### Other Fields

The field `since` prevents a transaction been mined before a specific time. It already has its own [RFC](https://github.com/nervosnetwork/rfcs/blob/master/rfcs/0017-tx-valid-since/0017-tx-valid-since.md).

The field `version` is reserved for future usage. It must equal 0 in current version.

### Exceptions

There are two special transactions in the system.

The first one is the cellbase, which is the first transaction in every block. The cellbase transaction has only one dummy input. In the dummy input, the `previous_outpoint` does not refer to any cell but set to a special value. The `since` must be set to the block number.

The outputs of the cellbase are the reward and transaction fees for an older block in the chain.

Cellbase is special because the output capacities do not come from inputs.

Another special transaction is the DAO withdraw transaction. It also has a portion of output capacities that do not come from inputs. This portion is the interest by locking the cells in the DAO. CKB recognizes the DAO withdraw transaction by check whether there's any input uses DAO as the type script.

## Appendix A: Compute Various Hash

### Crypto Primitives

**ckbhash**

CKB uses blake2b as the default hash algorithm. We use **ckbhash** to denote the blake2b hash function with following configuration:

* output digest size: 32
* personalization: ckb-default-hash

### Transaction Hash

Transaction hash is `ckbhash(molecule_encode(tx_excluding_witness))` where

* `molecule_encode` serializes a structure into binary using [molecule](https://github.com/nervosnetwork/rfcs/blob/master/rfcs/0008-serialization/0008-serialization.md).
* `tx_excluding_witness` is the transaction structure excluding the witness field.  See the definition `RawTransaction` in the [schema file](https://github.com/nervosnetwork/ckb/blob/a6733e6af5bb0da7e34fb99ddf98b03054fa9d4a/util/types/schemas/blockchain.mol#L55-L62).

### Cell Data Hash

Cell data hash is just `ckbhash(data)`.

### Script Hash

Script hash is `ckbhash(molecule_encode(script))` where `molecule_encode` turns the script structure into a block of binary via molecule. See the definition `Script` in the [schema file](https://github.com/nervosnetwork/ckb/blob/a6733e6af5bb0da7e34fb99ddf98b03054fa9d4a/util/types/schemas/blockchain.mol#L28-L32).



================================================
File: rfcs/0023-dao-deposit-withdraw/0023-dao-deposit-withdraw.md
================================================
---
Number: "0023"
Category: Standards Track
Status: Active
Author: Jan Xie <jan@cryptape.com>, Xuejie Xiao <xxuejie@gmail.com>, Ian Yang <@doitian>
Created: 2019-10-30
---

# Deposit and Withdraw in Nervos DAO

## Abstract

This document describes the details of Nervos DAO deposit and withdraw transaction.

## Motivation

Holders can deposit their CKBytes into Nervos DAO at any time. Nervos DAO deposit is a time deposit with a minimum deposit period (counted in blocks). Holders can only withdraw after a whole deposit period. If the holder does not withdraw at the end of the deposit period, those CKBytes should enter a new deposit period automatically, so holders' interaction with CKB could be minimized. This document provides necessary details for users or applications interacting with Nervos DAO.

Nervos DAO is a smart contract with which users can interact the same way as any smart contract on CKB. One function of Nervos DAO is to provide a dilution counter-measure for CKByte holders. By deposit in Nervos DAO, holders get proportional secondary rewards, which guarantee their holdings are only affected by hardcapped primary issuance as in Bitcoin.


## Background

CKB's token issuance curve consists of two components:

- Primary issuance: Hardcapped issuance for miners, using the same issuance curve as Bitcoin, half at every 4 years.
- Secondary issuance: Constant issuance, the same amount of CKBytes will be issued at every epoch, which means the secondary issuance rate approaches zero gradually over time. [Because epoch length is dynamically adjusted](https://github.com/nervosnetwork/rfcs/blob/master/rfcs/0020-ckb-consensus-protocol/0020-ckb-consensus-protocol.md), secondary issuance at every block is a variable, flucutates in a range.

If there's only primary issuance, but no secondary issuance in CKB, the total supply of CKBytes would have a hardcap, and the issuance curve would be the exact same as Bitcoin. To counter the dilution effect caused by secondary issuance, CKBytes locked in Nervos DAO will get the proportion of secondary issuance tantamount to the locked CKByte's percentage in circulation.

For more information on Nervos DAO and CKB's economic model, please refer to [Nervos RFC #0015](https://github.com/nervosnetwork/rfcs/blob/master/rfcs/0015-ckb-cryptoeconomics/0015-ckb-cryptoeconomics.md).

## Deposit

Users can send a transaction to deposit CKBytes into Nervos DAO at any time. CKB includes a particular Nervos DAO type script in the genesis block. To deposit to Nervos DAO, one needs to create any transaction containing a new output cell with the following requirements:

- MUST use the Nervos DAO script as type script.
- MUST have 8 bytes length cell data, filled with all zeros.

For convenience, a cell satisfying the above conditions will be called a `deposit cell`. To pass CKB's script validation, one also needs to include a reference to Nervos DAO type script in the `cell_deps` part of the enclosing transaction. Notice there is no limit on the number of deposits completed in one transaction. A single transaction can includes more than one deposit cells.

## Withdraw

Users can send a transaction to withdraw deposited CKBytes from Nervos DAO at any time(but a locking period will be applied to determine when precisely the tokens can be withdrawn). Nervos DAO issues compensation to deposited cells in the withdrawal phase. For a transaction including Nervos DAO withdrawal, the sum of all output cells' capacity might exceed the sum of all input cells' capacity. Unlike the deposit, withdrawal is a 2-phase process:

- In phase 1, the first transaction transforms a `deposit cell` into a `withdrawing cell`.
- In phase 2, a second transaction unlocks and transfers CKBytes in `withdrawing cell` to the recipient.

### Withdraw Phase 1

Phase 1 transforms a `deposit cell` into `withdrawing cell` so the deposit duration of the cell can be determined. Once the `withdrawing cell` is included in CKB, the deposit duration can be calculated by taking the difference of `deposit cell` and `withdrawing cell`'s inclusion block number. With deposit duration, both deposit compensation and the remaining locking period of `deposit cell` can be calculated.

A phase 1 transaction MUST satisfy the following conditions:

- One or more `deposit cell`s MUST be included in the transaction as inputs.
- For each `deposit cell`, the transaction MUST also include reference to its associated including block in `header_deps`, which will be used by Nervos DAO type script as the starting point of deposit.
- For a `deposit cell` at input index `i`, a `withdrawing cell` MUST be created at output index `i` with the following requirements:
    - The `withdrawing cell` MUST use the same Nervos DAO type script as the `deposit cell`.
    - The `withdrawing cell` MUST have the same capacity as the `deposit cell`.
    - The `withdrawing cell` MUST also have 8 bytes length cell data, but instead of 8 zero, it MUST store the block number of the `deposit cell`'s including block. The number MUST be packed in 64-bit unsigned little-endian integer format.
- The Nervos DAO type script MUST be included in the `cell_deps`.

Once this transaction is included in CKB, the user can start preparing the phase 2 transaction.

### Withdraw Phase 2

Phase 2 transaction unlocks deposited tokens together with compensation from Nervos DAO and sends them to the withdrawal recipient. Unlike phase 1 transaction, which can be sent at any time the user wishes, the phase 2 transaction has a since field set to fulfill the locking period requirement. Therefore, one may only be able to create a phase 2 transaction first but must wait for some time before getting the phase 2 transaction included in CKB.

A phase 2 transaction MUST satisfy the following conditions:

- One or more `withdrawing cell`s MUST be included in the transaction as inputs.
- For each `withdrawing cell`, the transaction MUST also include the reference to its associated including block in `header_deps`, which will be used by Nervos DAO type script as the endpoint of deposit.
- For a `withdrawing cell` at input index `i`, the transaction builder should locate the deposit block header, meaning the header of the original `deposit cell`'s inclusion block. With the deposit block header:
    - The deposit block header hash MUST be included in `header_deps`.
    - The index of the deposit block header hash in `header_deps` MUST be put in the type-script-part of the corresponding witness at index `i`, using 64-bit unsigned little-endian integer format. The example below explains data placement in transaction witnesses.
- For a `withdrawing cell`, the `since` field in the cell input MUST conform to the Nervos DAO's locking period requirement, which is 180 epochs. For example, if one deposits into Nervos DAO at epoch 5, he/she can only expect to withdraw Nervos DAO at epoch 185, 365, 545, etc.
- The sum of all input cells' capacity plus compensation MUST be larger or equal to the sum of all output cells' capacity.
- The Nervos DAO type script MUST be included in the `cell_deps`.

Notice the locking period is independent of the compensation calculation period. It's possible to deposit at epoch 5, initiate withdrawal and get `withdrawing cell` included at epoch 100, and construct a phase 2 withdrawal transaction with a `since` at epoch 185. Please refer to the [since RFC](https://github.com/nervosnetwork/rfcs/blob/master/rfcs/0017-tx-valid-since/0017-tx-valid-since.md) on how to set epoch number in the field. Nervos DAO type script only accepts absolute epoch numbers as since values.

It's possible to include multiple phase 2 withdrawals in one transaction.

One transaction can mix and include many deposit and withdrawal actions. For example, a single transaction can consist of all the following actions:

1. Deposit tokens into Nervos DAO.
2. Transform some `deposit cell`s to `withdrawing cell`s.
3. Withdraw from some other `withdrawing cell`s.

## Calculation

This section explains the calculation of Nervos DAO compensation and relevant fields in the CKB block header.

CKB's block header has a particular field named `dao` containing auxiliary information for Nervos DAO's use. Specifically, the following data are packed in the 32 bytes `dao` field in the following order:

- `C_i` : the total issuance up to and including block `i`.
- `AR_i`: the current `accumulated rate` at block `i`. `AR_j / AR_i` reflects the CKByte amount if one deposit 1 CKB to Nervos DAO at block `i`, and withdraw at block `j`.
- `S_i`: the total unissued secondary issuance up to and including block `i`, including unclaimed Nervos DAO compensation and treasury funds.
- `U_i` : the total `occupied capacities` currently in the blockchain up to and including block `i`. Occupied capacity is the sum of capacities used to store all cells.

Each value is encoded as an unsigned 64-bit little-endian number in the `dao` field. To maintain enough precision, `AR_i` is encoded as the original value multiplied by `10 ** 16`.

For a single block `i`, the following values are known:

- `p_i`: primary issuance for block `i`
- `s_i`: secondary issuance for block `i`
- `U_{in,i}` : occupied capacities for all input cells in block `i`
- `U_{out,i}` : occupied capacities for all output cells in block `i`
- `C_{in,i}` : total capacities for all input cells in block `i`
- `C_{out,i}` : total capacities for all output cells in block `i`
- `I_i` : total compensation of completed Nervos DAO withdrawals in block `i` (not includes withdrawing compensation)

In genesis block, the values are defined as follows:

- `C_0` : `C_{out,0}` - `C_{in,0}` + `p_0` + `s_0`
- `U_0` : `U_{out,0}` - `U_{in,0}`
- `S_0` : `s_0`
- `AR_0` : `10 ^ 16`

Then from the genesis block, the values for each succeeding block can be calculated in an induction way:

- `C_i` : `C_{i-1}` + `p_i` + `s_i`
- `U_i` : `U_{i-1}` + `U_{out,i}` - `U_{in,i}`
- `S_i` : `S_{i-1}` - `I_i` + `s_i` - floor( `s_i` * `U_{i-1}` / `C_{i-1}` )
- `AR_i` : `AR_{i-1}` + floor( `AR_{i-1}` * `s_i` / `C_{i-1}` )

With those values, Nervos DAO compensation can be calculated for any deposited cell. Assuming a Nervos DAO cell is deposited at block `m`, i.e. the `deposit cell` is included at block `m`. One initiates withdrawal and gets phase 1 `withdrawing cell` included at block `n`. The total capacity of the `deposit cell` is `c_t`, the occupied capacity for the `deposit cell` is `c_o`. Then its Nervos DAO compensation is calculated as:

( `c_t` - `c_o` ) * `AR_n` / `AR_m` - ( `c_t` - `c_o` )

Meaning that the maximum withdrawable capacity one can get from this Nervos DAO input cell is:

( `c_t` - `c_o` ) * `AR_n` / `AR_m` + `c_o`

## Example

The following type script represents the [Nervos DAO script on CKB mainnet](https://github.com/nervosnetwork/rfcs/blob/master/rfcs/0024-ckb-genesis-script-list/0024-ckb-genesis-script-list.md#nervos-dao):

    {
      "code_hash": "0x82d76d1b75fe2fd9a27dfbaa65a039221a380d76c926f378d3f81cf3e7e13f2e",
      "args": "0x",
      "hash_type": "type"
    }

And the following OutPoint refers to a cell containing NervosDAO script:

    {
      "out_point": {
        "tx_hash": "0xe2fb199810d49a4d8beec56718ba2593b665db9d52299a0f9e6e75416d73ff5c",
        "index": "0x2"
      },
      "dep_type": "code"
    }

The following transaction deposits 200 CKB into Nervos DAO:

    {
      "version": "0x0",
      "cell_deps": [
        {
          "out_point": {
            "tx_hash": "0x71a7ba8fc96349fea0ed3a5c47992e3b4084b031a42264a018e0072e8172e46c",
            "index": "0x0"
          },
          "dep_type": "dep_group"
        },
        {
          "out_point": {
            "tx_hash": "0xe2fb199810d49a4d8beec56718ba2593b665db9d52299a0f9e6e75416d73ff5c",
            "index": "0x2"
          },
          "dep_type": "code"
        }
      ],
      "header_deps": [],
      "inputs": [
        {
          "previous_output": {
            "tx_hash": "0xeb4644164c4dc64f195bb3b0c6e4f417e11519b1931e5f7177ff8008d96dbe83",
            "index": "0x1"
          },
          "since": "0x0"
        }
      ],
      "outputs": [
        {
          "capacity": "0x2e90edd000",
          "lock": {
            "code_hash": "0x9bd7e06f3ecf4be0f2fcd2188b23f1b9fcc88e5d4b65a8637b17723bbda3cce8",
            "args": "0xe5f99902495d04d9dcb013aefc96093d365b77dc",
            "hash_type": "type"
          },
          "type": {
            "code_hash": "0x82d76d1b75fe2fd9a27dfbaa65a039221a380d76c926f378d3f81cf3e7e13f2e",
            "args": "0x",
            "hash_type": "type"
          }
        },
        {
          "capacity": "0x101db898cb1",
          "lock": {
            "code_hash": "0x9bd7e06f3ecf4be0f2fcd2188b23f1b9fcc88e5d4b65a8637b17723bbda3cce8",
            "args": "0x9776eaa16af9cd8b6a2d169ae95671b0bcb8b0c4",
            "hash_type": "type"
          },
          "type": null
        }
      ],
      "outputs_data": [
        "0x0000000000000000",
        "0x"
      ],
      "witnesses": [
        "0x5500000010000000550000005500000041000000c22c72efb85da607ac48b220ad5b7132dc7abe50c3337c9a51e75102e8efaa5557e8b0567f9e0d9753016ebd52be3091bd55d4b87d7d4845f0d56ccf06e6ffe400"
      ],
      "hash": "0x81c400a761b0b5f1d8b00d8939e5a729d21d25a08e14e54f0661cb4f6fc6fb81"
    }

This transaction is actually committed in the following block:

    {
      "compact_target": "0x1a2158d9",
      "hash": "0x37ef8cf2407044d74a71f927a7e3dcd3be7fc5e7af0925c0b685ae3bedeec3bc",
      "number": "0x105f",
      "parent_hash": "0x36990fe91a0ee3755fd6faaa2563349425b56319f06aa70d2846af47e3132262",
      "nonce": "0x19759fb43000000000000000b28a9573",
      "timestamp": "0x16e80172dbf",
      "transactions_root": "0x66866dcfd5426b2bfeecb3cf4ff829d353364b847126b2e8d2ce8f8aecd28fb8",
      "proposals_hash": "0x0000000000000000000000000000000000000000000000000000000000000000",
      "uncles_hash": "0x0000000000000000000000000000000000000000000000000000000000000000",
      "version": "0x0",
      "epoch": "0x68d0288000002",
      "dao": "0x8268d571c743a32ee1e547ea57872300989ceafa3e710000005d6a650b53ff06"
    }

As mentioned above, the `dao` field contains 4 values, `AR` is the second field in the list. Extracting the little-endian integer from offset `8` through offset `16`, the current deposit `AR` is `10000435847357921`, which is `1.0000435847357921` considering `AR` is encoded with the original value multiplied by `10 ** 16` .

The following transaction can then be used to initiate the phase 1 of withdrawal, which transforms `deposit cell` to `withdrawing cell`:

    {
      "version": "0x0",
      "cell_deps": [
        {
          "out_point": {
            "tx_hash": "0x71a7ba8fc96349fea0ed3a5c47992e3b4084b031a42264a018e0072e8172e46c",
            "index": "0x0"
          },
          "dep_type": "dep_group"
        },
        {
          "out_point": {
            "tx_hash": "0xe2fb199810d49a4d8beec56718ba2593b665db9d52299a0f9e6e75416d73ff5c",
            "index": "0x2"
          },
          "dep_type": "code"
        }
      ],
      "header_deps": [
        "0x37ef8cf2407044d74a71f927a7e3dcd3be7fc5e7af0925c0b685ae3bedeec3bc"
      ],
      "inputs": [
        {
          "previous_output": {
            "tx_hash": "0x81c400a761b0b5f1d8b00d8939e5a729d21d25a08e14e54f0661cb4f6fc6fb81",
            "index": "0x0"
          },
          "since": "0x0"
        },
        {
          "previous_output": {
            "tx_hash": "0x043639b6aedcd0d897583e3d056e5a9c4875538533733818aca31fbeabfd5fba",
            "index": "0x1"
          },
          "since": "0x0"
        }
      ],
      "outputs": [
        {
          "capacity": "0x2e90edd000",
          "lock": {
            "code_hash": "0x9bd7e06f3ecf4be0f2fcd2188b23f1b9fcc88e5d4b65a8637b17723bbda3cce8",
            "args": "0xe5f99902495d04d9dcb013aefc96093d365b77dc",
            "hash_type": "type"
          },
          "type": {
            "code_hash": "0x82d76d1b75fe2fd9a27dfbaa65a039221a380d76c926f378d3f81cf3e7e13f2e",
            "args": "0x",
            "hash_type": "type"
          }
        },
        {
          "capacity": "0x179411d65",
          "lock": {
            "code_hash": "0x9bd7e06f3ecf4be0f2fcd2188b23f1b9fcc88e5d4b65a8637b17723bbda3cce8",
            "args": "0x5df75f10330a05ec9f862dec9bb37b5e11171475",
            "hash_type": "type"
          },
          "type": null
        }
      ],
      "outputs_data": [
        "0x5f10000000000000",
        "0x"
      ],
      "witnesses": [
        "0x5500000010000000550000005500000041000000d952a9b844fc441529dd310e49907cc5eba009dcf0fcd7a5fb1394017c29b90b7c68e1d0db52c67d444accec4c04670d197630656837b33d07f0cbdd1f33907d01",
        "0x5500000010000000550000005500000041000000d8e77676d57742b9b1e3a47e53f023ade294af5ca501f33406e992af01b1d0dd4a4f22d478c9497b184b04ea56c4ce71fccd9f0d4c25f503324edff5f2b26f0d00"
      ],
      "hash": "0x9ab05d622dc6d9816f70094242740cca594e677009b88c3f2b367d8b32f928fd"
    }

A couple of things worth mentioning in this transaction:

- The input `deposit cell` is included in the `0x37ef8cf2407044d74a71f927a7e3dcd3be7fc5e7af0925c0b685ae3bedeec3bc` block, hence it is included in `header_deps`.
- The including block number is `4191`, which is `0x5f10000000000000` packed in 64-bit unsigned little-endian integer number.
- Looking at the above 2 transactions, the output cell in this transaction has the same type and capacity as the previous `deposit cell`, while storing different cell data.

Assume this transaction is included in the following block:

    {
      "compact_target": "0x1a2dfb48",
      "hash": "0xba6eaa7e0acd0dc78072c5597ed464812391161f0560c35992ae0c96cd1d6073",
      "number": "0x11ea4",
      "parent_hash": "0x36f16c9a1abea1cb44bc1d923feb9f62ff45b9327188dca954968dfdecc03bd0",
      "nonce": "0x74e39f370400000000000000bb4b3299",
      "timestamp": "0x16ea78c300f",
      "transactions_root": "0x4efccc5beeeae3847aa65f2e987947957d68f13687af069f52be361d0648feb8",
      "proposals_hash": "0x0000000000000000000000000000000000000000000000000000000000000000",
      "uncles_hash": "0x0000000000000000000000000000000000000000000000000000000000000000",
      "version": "0x0",
      "epoch": "0x645017e00002f",
      "dao": "0x77a7c6ea619acb2e4b841a96c88e2300b6b274a096c1080000ea07db0efaff06"
    }

The following phase 2 transaction can be used to complete the withdrawal:

    {
      "version": "0x0",
      "cell_deps": [
        {
          "out_point": {
            "tx_hash": "0x71a7ba8fc96349fea0ed3a5c47992e3b4084b031a42264a018e0072e8172e46c",
            "index": "0x0"
          },
          "dep_type": "dep_group"
        },
        {
          "out_point": {
            "tx_hash": "0xe2fb199810d49a4d8beec56718ba2593b665db9d52299a0f9e6e75416d73ff5c",
            "index": "0x2"
          },
          "dep_type": "code"
        }
      ],
      "header_deps": [
        "0x37ef8cf2407044d74a71f927a7e3dcd3be7fc5e7af0925c0b685ae3bedeec3bc",
        "0xba6eaa7e0acd0dc78072c5597ed464812391161f0560c35992ae0c96cd1d6073"
      ],
      "inputs": [
        {
          "previous_output": {
            "tx_hash": "0x9ab05d622dc6d9816f70094242740cca594e677009b88c3f2b367d8b32f928fd",
            "index": "0x0"
          },
          "since": "0x20068d02880000b6"
        }
      ],
      "outputs": [
        {
          "capacity": "0x2e9a2ed603",
          "lock": {
            "code_hash": "0x9bd7e06f3ecf4be0f2fcd2188b23f1b9fcc88e5d4b65a8637b17723bbda3cce8",
            "args": "0x89e1914565e6fcc74e36d6c7bec4bdfa222b3a25",
            "hash_type": "type"
          },
          "type": null
        }
      ],
      "outputs_data": [
        "0x"
      ],
      "witnesses": [
        "0x61000000100000005500000061000000410000006114fee94f91ed089a32df9c3b0cda0ca1e1e97879d0aae253d0785fc6f7019b20cccbc7ea338ea96e64172f4a810ef531ab5ca1570a9742f0fb23378e260d9f01080000000000000000000000"
      ],
      "hash": "0x1c375948bae003ef1a9e86e6b049199480987d7dcf96bdfa2a914ecd4dadd42b"
    }

A couple of things worth mentioning in this transaction:

- The `header_deps` in this transaction contains 2 headers: `0x37ef8cf2407044d74a71f927a7e3dcd3be7fc5e7af0925c0b685ae3bedeec3bc` contains block header hash in which the original `deposit cell` is included, while `0xba6eaa7e0acd0dc78072c5597ed464812391161f0560c35992ae0c96cd1d6073` is the block in which the `withdrawing cell` is included.
- Since `0x37ef8cf2407044d74a71f927a7e3dcd3be7fc5e7af0925c0b685ae3bedeec3bc` is at index 0 in `header_deps`. The number `0` is packed in 64-bit little-endian unsigned integer, which is `0000000000000000`, and appended to the end of the witness corresponding with the Nervos DAO input cell.
- The Nervos DAO input cell has a `since` field of `0x20068d02880000b6`, which is calculated as follows:
    - The deposit block header has an epoch value of `0x68d0288000002`, which means the `2 + 648 / 1677` epoch
    - The block header in which `withdrawing cell` is included has an epoch value of `0x645017e00002f`, which means the `47 + 382 / 1605` epoch
    - The closest epoch that is past `47 + 382 / 1605` but still satisfies locking period requirement is `182 + 648 / 1677` epoch, which is encoded as `0x68d02880000b6`.
    - Since absolute epoch number is used, necessary flags are needed to make the value of since field `0x20068d02880000b6`. Please refer to [`since` RFC](https://github.com/nervosnetwork/rfcs/blob/master/rfcs/0017-tx-valid-since/0017-tx-valid-since.md) for more details.

Using the same calculation as above, the `AR` for the withdrawing block `0xba6eaa7e0acd0dc78072c5597ed464812391161f0560c35992ae0c96cd1d6073` is `1.0008616347796555`.

Now the maximum withdrawable capacity can be calculated:

`total_capacity` = 200000000000
`occupied_capacity` = 10200000000 (8 bytes for capacity, 53 bytes for lock script, 33 bytes for type script and another 8 bytes for cell data are needed cost, the sum of those is 102 bytes, which is exactly 10200000000 shannons)
`counted_capacity` = 200000000000 - 10200000000 = 189800000000
`maximum_withdraw_capacity` = 189800000000 * 10008616347796555 / 10000435847357921 + 10200000000 = 200155259131

200155259131 shannons is hence the maximum withdrawable capacity. The transaction has one output with capacity `0x2e9a2ed603` = 200155256323 shannons, it also pays a transaction fee of 2808 shannons, and 200155259131 = 200155256323 + 2808.

## Gotchas

* Nervos DAO only supports *absolute epoch number* as since value in the withdrawal process. If you are using a lock that supports lock period, such as the system included [multi-sign script](https://github.com/nervosnetwork/ckb-system-scripts/blob/master/c/secp256k1_blake160_multisig_all.c), please make sure to ONLY use *absolute epoch number* as lock period. Otherwise, the locked Nervos DAO cell cannot be spent.
* For simplicity a transaction containing Nervos DAO script is currently limited to 64 output cells so that processing is simplified, this limitation may be relaxed later on in a future Nervos DAO script update.


================================================
File: rfcs/0024-ckb-genesis-script-list/0024-ckb-genesis-script-list.md
================================================
---
Number: "0024"
Category: Informational
Status: Final
Author: Dylan Duan <duanyytop@gmail.com>
Created: 2020-05-21
---

# CKB Genesis Script List

## Abstract

Genesis scripts are the smart contracts built by the CKB team and set in the CKB genesis block. Genesis scripts provide core functions like (e.g. [SECP256k1/blake160](#secp256k1blake160) and [Nervos DAO](#nervos-dao)). This document presents the information of all Nervos CKB genesis scripts, including a brief introduction and _code_hash_, _hash_type_, _out_point_(_tx_hash_ and _index_), _dep_type_ on mainnet Lina and testnet Aggron.

## Motivation

Genesis scripts are used frequently in dapps, wallets, and other application development. A list of genesis scripts provides a handy reference to developers.

## List of Genesis Scripts

- [Locks](#Locks)

  - [SECP256K1/blake160](#secp256k1blake160)
  - [SECP256K1/multisig](#secp256k1multisig)

- [Types](#Types)

  - [Nervos DAO](#nervos-dao)
  - [Type ID](#type-id)

To construct transactions with genesis scripts, the _code_hash_, _hash_type_, _out_point_ and _dep_type_ of genesis scripts in mainnet Lina and testnet Aggron are needed.

## Locks

### SECP256K1/blake160

[SECP256K1/blake160](https://github.com/nervosnetwork/ckb-system-scripts/wiki/How-to-sign-transaction#p2ph) ([Source Code](https://github.com/nervosnetwork/ckb-system-scripts/blob/master/c/secp256k1_blake160_sighash_all.c)) is the default lock script to verify CKB transaction signature.

SECP256K1/blake160 script is for **lock script**:

- Lina

| parameter   | value                                                                |
| ----------- | -------------------------------------------------------------------- |
| `code_hash` | `0x9bd7e06f3ecf4be0f2fcd2188b23f1b9fcc88e5d4b65a8637b17723bbda3cce8` |
| `hash_type` | `type`                                                               |
| `tx_hash`   | `0x71a7ba8fc96349fea0ed3a5c47992e3b4084b031a42264a018e0072e8172e46c` |
| `index`     | `0x0`                                                                |
| `dep_type`  | `dep_group`                                                          |

**Note:**

The `dep_type` of `SECP256K1/blake160` in Lina is `dep_group` means that the content of this dep cell contains two cell deps which are `secp256k1_data` and `secp256k1_blake160_sighash_all` whose `dep_type` are `code`.

The `out_point` of `secp256k1_data` is

```
{
  tx_hash: 0xe2fb199810d49a4d8beec56718ba2593b665db9d52299a0f9e6e75416d73ff5c,
  index: 0x3
}
```

and the `out_point` of `secp256k1_blake160_sighash_all` is

```
{
  tx_hash: 0xe2fb199810d49a4d8beec56718ba2593b665db9d52299a0f9e6e75416d73ff5c,
  index: 0x1
}
```

- Aggron

| parameter   | value                                                                |
| ----------- | -------------------------------------------------------------------- |
| `code_hash` | `0x9bd7e06f3ecf4be0f2fcd2188b23f1b9fcc88e5d4b65a8637b17723bbda3cce8` |
| `hash_type` | `type`                                                               |
| `tx_hash`   | `0xf8de3bb47d055cdf460d93a2a6e1b05f7432f9777c8c474abf4eec1d4aee5d37` |
| `index`     | `0x0`                                                                |
| `dep_type`  | `dep_group`                                                          |

**Note:**

The `dep_type` of `SECP256K1/blake160` in Aggron is `dep_group` means that the content of this dep cell contains two cell deps which are `secp256k1_data` and `secp256k1_blake160_sighash_all` whose `dep_type` are `code`.

The `out_point` of `secp256k1_data` is

```
{
  tx_hash: 0x8f8c79eb6671709633fe6a46de93c0fedc9c1b8a6527a18d3983879542635c9f,
  index: 0x3
}
```

and the `out_point` of `secp256k1_blake160_sighash_all` is

```
{
  tx_hash: 0x8f8c79eb6671709633fe6a46de93c0fedc9c1b8a6527a18d3983879542635c9f,
  index: 0x1
}
```

### SECP256K1/multisig

[SECP256K1/multisig](https://github.com/nervosnetwork/ckb-system-scripts/wiki/How-to-sign-transaction#multisig) ([Source Code](https://github.com/nervosnetwork/ckb-system-scripts/blob/master/c/secp256k1_blake160_multisig_all.c)) is a script which allows a group of users to sign a single transaction.

SECP256K1/multisig script is for **lock script**:

- Lina

| parameter   | value                                                                |
| ----------- | -------------------------------------------------------------------- |
| `code_hash` | `0x5c5069eb0857efc65e1bca0c07df34c31663b3622fd3876c876320fc9634e2a8` |
| `hash_type` | `type`                                                               |
| `tx_hash`   | `0x71a7ba8fc96349fea0ed3a5c47992e3b4084b031a42264a018e0072e8172e46c` |
| `index`     | `0x1`                                                                |
| `dep_type`  | `dep_group`                                                          |

**Note:**

The `dep_type` of `SECP256K1/multisig` in Lina is `dep_group` means that the content of this dep cell contains two cell deps which are `secp256k1_data` and `secp256k1_blake160_multisig_all` whose `dep_type` are `code`.

The `out_point` of `secp256k1_data` is

```
{
  tx_hash: 0xe2fb199810d49a4d8beec56718ba2593b665db9d52299a0f9e6e75416d73ff5c,
  index: 0x3
}
```

and the `out_point` of `secp256k1_blake160_multisig_all` is

```
{
  tx_hash: 0xe2fb199810d49a4d8beec56718ba2593b665db9d52299a0f9e6e75416d73ff5c,
  index: 0x4
}
```

- Aggron

| parameter   | value                                                                |
| ----------- | -------------------------------------------------------------------- |
| `code_hash` | `0x5c5069eb0857efc65e1bca0c07df34c31663b3622fd3876c876320fc9634e2a8` |
| `hash_type` | `type`                                                               |
| `tx_hash`   | `0xf8de3bb47d055cdf460d93a2a6e1b05f7432f9777c8c474abf4eec1d4aee5d37` |
| `index`     | `0x1`                                                                |
| `dep_type`  | `dep_group`                                                          |

**Note:**

The `dep_type` of `SECP256K1/blake160` in Aggron is `dep_group` means that the content of this dep cell contains two cell deps which are `secp256k1_data` and `secp256k1_blake160_multisig_all` whose `dep_type` are `code`.

The `out_point` of `secp256k1_data` is

```
{
  tx_hash: 0x8f8c79eb6671709633fe6a46de93c0fedc9c1b8a6527a18d3983879542635c9f,
  index: 0x3
}
```

and the `out_point` of `secp256k1_blake160_multisig_all` is

```
{
  tx_hash: 0x8f8c79eb6671709633fe6a46de93c0fedc9c1b8a6527a18d3983879542635c9f,
  index: 0x4
}
```

## Types

### Nervos DAO

[Nervos DAO](https://github.com/nervosnetwork/rfcs/blob/master/rfcs/0023-dao-deposit-withdraw/0023-dao-deposit-withdraw.md) ([Source Code](https://github.com/nervosnetwork/ckb-system-scripts/blob/master/c/dao.c)) is the script implements Nervos DAO.

Nervos DAO script is for **type script**:

- Lina

| parameter   | value                                                                |
| ----------- | -------------------------------------------------------------------- |
| `code_hash` | `0x82d76d1b75fe2fd9a27dfbaa65a039221a380d76c926f378d3f81cf3e7e13f2e` |
| `hash_type` | `type`                                                               |
| `tx_hash`   | `0xe2fb199810d49a4d8beec56718ba2593b665db9d52299a0f9e6e75416d73ff5c` |
| `index`     | `0x2`                                                                |
| `dep_type`  | `code`                                                               |

- Aggron

| parameter   | value                                                                |
| ----------- | -------------------------------------------------------------------- |
| `code_hash` | `0x82d76d1b75fe2fd9a27dfbaa65a039221a380d76c926f378d3f81cf3e7e13f2e` |
| `hash_type` | `type`                                                               |
| `tx_hash`   | `0x8f8c79eb6671709633fe6a46de93c0fedc9c1b8a6527a18d3983879542635c9f` |
| `index`     | `0x2`                                                                |
| `dep_type`  | `code`                                                               |


### Type ID

Strictly speaking, [Type ID](https://github.com/nervosnetwork/rfcs/blob/master/rfcs/0022-transaction-structure/0022-transaction-structure.md#type-id) ([Source Code](https://github.com/nervosnetwork/ckb/blob/master/script/src/type_id.rs)) is a builtin script directly implemented in Rust within CKB, it doesn't run in CKB-VM. It's still included in this document because it can be used the same way as other genesis scripts since genesis block.

Type ID is used to support unique and immutable references. The _code_hash_ and _hash_type_ are fixed and the same on mainnet Lina and testnet Aggron and the `dep_type` is no need. 

Type ID script is for **type script**:

- Lina & Aggron

| parameter   | value                                                                |
| ----------- | -------------------------------------------------------------------- |
| `code_hash` | `0x00000000000000000000000000000000000000000000000000545950455f4944` |
| `hash_type` | `type`                                                               |


================================================
File: rfcs/0025-simple-udt/0025-simple-udt.md
================================================
---
Number: "0025"
Category: Standards Track
Status: Proposal
Author: Xuejie Xiao <xxuejie@gmail.com>
Created: 2020-09-03
---

# Simple UDT

This RFC defines the Simple User Defined Tokens(Simple UDT or SUDT) specification. Simple UDT provides a way for dapp developers to issue custom tokens on Nervos CKB. The simple part in Simple UDT means we are defining a minimal standard that contains whats absolutely needed, more sophisticated actions are left to CKBs flexibility to achieve.

## Data Structure

### SUDT Cell

A SUDT cell in Simple UDT specification looks like following:

```
data:
    amount: uint128
type:
    code_hash: simple_udt type script
    args: owner lock script hash (...)
lock:
    <user_defined>
```

The following rules should be met in a SUDT Cell:

* **Simple UDT Rule 1**: a SUDT cell must store SUDT amount in the first 16 bytes of cell data segment, the amount should be stored as little endian, 128-bit unsigned integer format. In the case of composable scripts, the SUDT amount must still be located at the initial 16 bytes in the data segment which corresponds to the composed SUDT script
* **Simple UDT Rule 2**: the first 32 bytes of the SUDT cells type script args must store the lock script hash of *owner lock*. Owner lock will be explained below
* **Simple UDT Rule 3**: each SUDT must have unique type script[^1], in other words, 2 SUDT cells using the same type script are considered to be the same SUDT.

[^1]: As per definition a type script is comprised of three fields: `code_hash`, `hash_type` and `args`, see [script definition](https://docs.nervos.org/docs/reference/script/).

User shall use any lock script as they wish in the SUDT Cell.

### Owner lock script

Owner lock shall be used for governance purposes, such as issuance, mint, burn as well as other operations. The SUDT specification does not enforce specific rules on the behavior of owner lock script. It is expected that owner lock script should at least provide enough security to ensure only token owners can perform governance operations.

## Operations

This section describes operations that must be supported in Simple UDT implementation

### Transfer

Transfer operation transfers SUDTs from one or more SUDT holders to other SUDT holders.

```
// Transfer
Inputs:
    <vec> SUDT_Cell
        Data:
            amount: uint128
        Type:
            code_hash: simple_udt type script
            args: owner lock script hash (...)
        Lock:
            <user defined>
    <...>
Outputs:
    <vec> SUDT_Cell
        Data:
            amount: uint128
        Type:
            code_hash: simple_udt type script
            args: owner lock script hash (...)
        Lock:
            <user defined>
    <...>
```

Transfer operation must satisfy the following rule:

* **Simple UDT Rule 4**: in a transfer transaction, the sum of all SUDT tokens from all input cells must be larger or equal to the sum of all SUDT tokens from all output cells. Allowing more input SUDTs than output SUDTs enables burning tokens.

## Governance Operations

This section describes governance operations that should be supported by Simple UDT Implementation. All goverance operations must satisfy the following rule:

* **Simple UDT Rule 5**: in a governance operation, at least one input cell in the transaction should use owner lock specified by the SUDT as its cell lock.

### Issue/Mint SUDT

This operation enables issuing new SUDTs.

```
// Issue new SUDT
Inputs:
    <... one of the input cell must have owner lock script as lock>
Outputs:
    SUDT_Cell:
        Data:
            amount: uint128
        Type:
            code_hash: simple_udt type script
            args: owner lock script hash (...)
        Lock:
            <user defined>
```

## Notes

An [implementation](https://github.com/nervosnetwork/ckb-production-scripts/blob/e570c11aff3eca12a47237c21598429088c610d5/c/simple_udt.c) of the Simple UDT spec above has been deployed to Lina CKB mainnet and Aggron testnet:


- Lina

| parameter   | value                                                                |
| ----------- | -------------------------------------------------------------------- |
| `code_hash` | `0x5e7a36a77e68eecc013dfa2fe6a23f3b6c344b04005808694ae6dd45eea4cfd5` |
| `hash_type` | `type`                                                               |
| `tx_hash`   | `0xc7813f6a415144643970c2e88e0bb6ca6a8edc5dd7c1022746f628284a9936d5` |
| `index`     | `0x0`                                                                |
| `dep_type`  | `code`                                                               |

- Aggron

| parameter   | value                                                                |
| ----------- | -------------------------------------------------------------------- |
| `code_hash` | `0xc5e5dcf215925f7ef4dfaf5f4b4f105bc321c02776d6e7d52a1db3fcd9d011a4` |
| `hash_type` | `type`                                                               |
| `tx_hash`   | `0xe12877ebd2c3c364dc46c5c992bcfaf4fee33fa13eebdf82c591fc9825aab769` |
| `index`     | `0x0`                                                                |
| `dep_type`  | `code`                                                               |


Reproducible build is supported to verify the deploy script. To bulid the deployed Simple UDT script above, one can use the following steps:

```bash
$ git clone https://github.com/nervosnetwork/ckb-production-scripts
$ cd ckb-production-scripts
$ git checkout e570c11aff3eca12a47237c21598429088c610d5
$ git submodule update --init --recursive
$ make all-via-docker
```

Now you can compare the simple udt script generated at `build/simple_udt` with the one deployed to CKB, they should be identical.

A draft of this specification has already been released, reviewed, and discussed in the community at [here](https://talk.nervos.org/t/rfc-simple-udt-draft-spec/4333) for quite some time.


================================================
File: rfcs/0026-anyone-can-pay/0026-anyone-can-pay.md
================================================
---
Number: "0026"
Category: Standards Track
Status: Proposal
Author: Xuejie Xiao <xxuejie@gmail.com>
Created: 2020-09-03
---

# Anyone-Can-Pay Lock

This RFC describes a new lock script for CKB that can accept any amount of [Simple UDT](../0025-simple-udt/0025-simple-udt.md) or CKB payment. Previously, one can only transfer to another user at least 61 CKBytes when using the default lock, possibly more when using other lock scripts or type scripts. This is becoming a bigger problem when UDT support lands in CKB: a naive UDT transfer operation will not only require UDTs, but CKByte to keep the UDTs in a cell as well.

Here we try to solve the problem by introducing a new anyone-can-pay lock script, which can be unlocked not only by the validation of a signature, but also by accepting any amount of payment. This way, a user should be able to send any amount of CKBytes or UDTs to a cell using anyone-can-pay lock instead of always creating a new cell. It thus provides a solution to both problems above.

## Script Structure

The anyone-can-pay lock is built upon the default secp256k1-blake2b-sighash-all lock with additions to the script args part. The new anyone-can-pay lock can accept any of the following script args format:

```
<20 byte blake160 public key hash>
<20 byte blake160 public key hash> <1 byte CKByte minimum>
<20 byte blake160 public key hash> <1 byte CKByte minimum> <1 byte UDT minimum>
```

The additions of CKByte & UDT minimums enforce the minimal amount that one can transfer to the anyone-can-pay lock. This provides a mitigation against DDoSing on the cell level: if a cell is setup using the anyone-can-pay lock, an attacker can keep creating transactions that transfer only 1 shannon or 1 UDT to the cell, making it difficult for the cell owner to claim the tokens stored in the cell. By providing a minimal transfer amount, a user can raise the attacking cost, hence protecting his/her own cells against DDoS attacks. This mechanism won't prevent all kinds of DDoS of course, but it serves as a quick solution to mitigate cheaper ones.

The value stored in CKByte & UDT minimum are interpreted in the following way: if `x` is stored in the field, the minimal transfer amount will be `10^x`, for example:

* If 3 is stored in CKByte minimum, it means the minimal amount that can be accepted by the cell is 1000 shannons
* If 4 is stored in UDT base unit minimum, it means the minimal amount that can be accepted by the cell is 10000 UDT base units.

Note the minimum fields are completely optional. If a minimum is not provided, we will treat the minimum value as 0, meaning no minimum is enforced on the transfer operation. It is worth mentioning that different minimums also lead to different lock scripts used by the cell.

## UDT Interpretation

The anyone-can-pay lock assumes that the locked cell follows the [Simple UDT specification](https://talk.nervos.org/t/rfc-simple-udt-draft-spec/4333), thus the cell 1) has a type script; 2) has at least 16 bytes in the cell data part. Its up to the user to ensure one only uses anyone-can-pay lock with a type script implementing Simple UDT specification.

## Unlock Rules

The anyone-can-pay lock will work following the rules below:

1. If a signature is provided, it works exactly as the default secp256k1-blake2b-sighash-all lock, if a signature is provide in witness and can be validated, the lock returns with a success state.

    1.a. If the provided signature fails validation, the lock returns with an error state

2. If a signature is not provided, the lock continues with the added anyone-can-pay logic below:

    2.a. It loops through all input cells using the current anyone-can-pay lock script(notice here the lock script we refer to include public key hash, meaning if a transaction contains 2 cells using the same anyone-can-pay lock code, but different public key hash, they will be treated as different lock script, and each will perform the script unlock rule checking independently), if 2 input cells are using the same type script, or are both missing type scripts, the lock returns with an error state

    2.b. It loops through all output cells using the current anyone-can-pay lock script, if 2 output cells are using the same type script, or are both missing type scripts, the lock returns with an error state

    2.c. It loops through all input cells and output cells using the current anyone-can-pay lock script, if there is a cell that is missing type script, but has cell data set, it returns with an error state.

    2.d. It loops through all input cells and output cells using the current anyone-can-pay lock script, if there is a cell that has type script, but a cell data part with less than 16 bytes of data, it returns with an error state.

    2.e. It then pairs input cells and output cells with matching type scripts(input cell without type script will match with output cell without type script). If there is an input cell without matching output cell, or if there is an output cell without matching input cell, it returns with an error state.

    2.f. It loops through all pairs of input & output cells, if there is a pair in which the input cell has more CKBytes than the output cell; or if the pair of cells both have type script and cell data part, but the input cell has more UDT than the output cell, it returns with an error state.

    2.g. If CKByte minimum or UDT minimum is set, it loops through all pairs of input & output cells. If it could find a pair of input & output cells in which the output amount is less than the input amount plus the set minimum, it returns with an error state. Note only one minimum needs to be matched if both CKByte minimum and UDT minimum are set.

The reason of limiting one input cell and one output cell for each lock/type script combination, is that the lock script should prevent attackers from merging or splitting cells:

* Allowing merging anyone-can-pay cells can result in less cells being available, resulting in usability problems. For example, an exchange might create hundreds of anyone-can-pay cells to perform sharding so deposit transactions are less likely to conflict with each other.
* Allowing splitting anyone-can-pay cells has 2 problems: 1) it increases CKByte usage on chain, putting unwanted pressure on miners; 2) it might result in fee increase when later the owner wants to claim tokens in anyone-can-pay cells, since more input cells than expect would result in both transaction size increase, and validation cycle increase

Giving those considerations, anyone-can-pay lock script here forbids merging or splitting anyone-can-pay cells from non-owners, as allowing more than one input/output anyone-can-pay cell in each lock/type combination would only complicate lock validation rules without significant gains.

## Examples

Here we describe useful transaction examples involving anyone-can-pay lock.

### Create an Anyone-can-pay Cell

```
Inputs:
    Normal Cell:
        Capacity: 1000 CKBytes
        Lock:
            code_hash: secp256k1_blake2b lock
            args: <public key hash A>
Outputs:
    Anyone-can-pay Cell:
        Capacity: 999.99 CKBytes
        Type:
            code_hash: simple udt lock
            args: <owner lock C>
        Lock:
            code_hash: anyone-can-pay lock
            args: <public key hash B> <CKByte minimum: 9> <UDT minimum: 5>
        Data:
            Amount: 0 UDT
Witnesses:
    <valid signature for public key hash A>
```

Note here we assume 0.01 CKByte is paid as the transaction fee, in production one should calculate the fee based on factors including transaction size, running cycles as well as network status. 0.01 CKByte will be used in all examples as fees for simplicity. The new anyone-can-pay cell created by this transaction impose a minimum transfer value of 10^9 shannons (10 CKBytes) and 10^5 UDT base units respectively.

### Unlock via Signature

```
Inputs:
    Anyone-can-pay Cell:
        Capacity: 1000 CKBytes
        Lock:
            code_hash: anyone-can-pay lock
            args: <public key hash A> <CKByte minimum: 2>
Outputs:
    Normal Cell:
        Capacity: 999.99 CKBytes
        Lock:
            code_hash: secp256k1_blake2b lock
            args: <public key hash B>
Witnesses:
    <valid signature for public key hash A>
```

When a signature is provided, the cell can be unlocked in anyway the owner wants, anyone-can-pay lock here just behaves as a normal cell. In this example an anyone-can-pay cell is converted back to a normal cell.

### Unlock via CKB Payment on Cells with No Type Script

```
Inputs:
    Deposit Normal Cell:
        Capacity: 500 CKBytes
        Lock:
            code_hash: secp256k1_blake2b lock
            args: <public key hash B>
    Anyone-can-pay Cell:
        Capacity: 1000 CKBytes
        Lock:
            code_hash: anyone-can-pay lock
            args: <public key hash A> <CKByte minimum: 2>
Outputs:
    Deposit Change Cell:
        Capacity: 479.99 CKBytes
        Lock:
            code_hash: secp256k1_blake2b lock
            args: <public key hash B>
    Anyone-can-pay Cell:
        Capacity: 1020 CKBytes
        Lock:
            code_hash: anyone-can-pay lock
            args: <public key hash A> <CKByte minimum: 2>
Witnesses:
    <valid signature for public key hash B>
```

Here the transaction doesnt contain signature for the anyone-can-pay cell, yet the anyone-can-pay lock succeeds the validation when it detects that someone deposits 20 CKBytes into itself. Note this use case does not involve in UDT at all, anyone-can-pay lock is used to overcome the 61 CKBytes requirement of plain transfer.

### Unlock via UDT Payment

```
Inputs:
    Deposit Normal Cell:
        Capacity: 500 CKBytes
        Lock:
            code_hash: secp256k1_blake2b lock
            args: <public key hash B>
        Type:
            code_hash: simple udt lock
            args: <owner lock C>
        Data:
            Amount: 200000 UDT
    Anyone-can-pay Cell:
        Capacity: 1000 CKBytes
        Lock:
            code_hash: anyone-can-pay lock
            args: <public key hash A>
        Type:
            code_hash: simple udt lock
            args: <owner lock C>
        Data:
            Amount: 3000 UDT
Outputs:
    Deposit Change Cell:
        Capacity: 499.99 CKB
        Lock:
            code_hash: secp256k1_blake2b lock
            args: <public key hash B>
        Type:
            code_hash: simple udt lock
            args: <owner lock C>
        Data:
            Amount: 199999 UDT
    Anyone-can-pay Cell:
        Capacity: 1000 CKBytes
        Lock:
            code_hash: anyone-can-pay lock
            args: <public key hash A>
        Type:
            code_hash: simple udt lock
            args: <owner lock C>
        Data:
            Amount: 3001 UDT
Witnesses:
    <valid signature for public key hash B>
```

Here we are depositing 1 UDT to the anyone-can-pay cell. Because theres no extra arguments in the anyone-can-pay lock script except a public key hash, the cell enforces no minimum on the CKByte or UDT one can transfer, a transfer of 1 UDT will be accepted here.

### Unlock via CKByte Payment With Minimums

```
Inputs:
    Deposit Normal Cell:
        Capacity: 500 CKBytes
        Lock:
            code_hash: secp256k1_blake2b lock
            args: <public key hash B>
        Type:
            code_hash: simple udt lock
            args: <owner lock C>
        Data:
            Amount: 200000 UDT
    Anyone-can-pay Cell:
        Capacity: 1000 CKBytes
        Lock:
            code_hash: anyone-can-pay lock
            args: <public key hash A> <CKByte minimum: 9> <UDT minimum: 5>
        Type:
            code_hash: simple udt lock
            args: <owner lock C>
        Data:
            Amount: 3000 UDT
Outputs:
    Deposit Change Cell:
        Capacity: 489.99 CKBytes
        Lock:
            code_hash: secp256k1_blake2b lock
            args: <public key hash B>
        Type:
            code_hash: simple udt lock
            args: <owner lock C>
        Data:
            Amount: 200000 UDT
    Anyone-can-pay Cell:
        Capacity: 1010 CKBytes
        Lock:
            code_hash: anyone-can-pay lock
            args: <public key hash A> <CKByte minimum: 9> <UDT minimum: 5>
        Type:
            code_hash: simple udt lock
            args: <owner lock C>
        Data:
            Amount: 3000 UDT
Witnesses:
    <valid signature for public key hash B>
```

Here CKByte minimum is set to 9, which means in each transaction, one must at least transfers `10^9` shannons, or 10 CKBytes into the anyone-can-pay cell. Note that even though UDT minimum is set to 5, meaning one should at least transfer 100000 UDT base units to the anyone-can-pay cell, satisfying the CKByte minimal transfer minimum alone already satisfy the validation rules, allowing CKB to accept the transaction. Likewise, a different transaction might only send 100000 UDT base units to the anyone-can-pay cell without sending any CKBytes, this will also satisfy the validation rules of anyone-can-pay cell here.

## Notes

An [implementation](https://github.com/nervosnetwork/ckb-production-scripts/blob/e570c11aff3eca12a47237c21598429088c610d5/c/anyone_can_pay.c) of the anyone-can-pay lock spec above has been deployed to Lina CKB mainnet and Aggron testnet:

- Lina

| parameter   | value                                                                |
| ----------- | -------------------------------------------------------------------- |
| `code_hash` | `0xd369597ff47f29fbc0d47d2e3775370d1250b85140c670e4718af712983a2354` |
| `hash_type` | `type`                                                               |
| `tx_hash`   | `0x4153a2014952d7cac45f285ce9a7c5c0c0e1b21f2d378b82ac1433cb11c25c4d` |
| `index`     | `0x0`                                                                |
| `dep_type`  | `dep_group`                                                          |

**Note:**

The `dep_type` of `anyone_can_pay` in Lina is `dep_group` means that the content of this dep cell contains two cell deps which are `secp256k1_data` and `anyone_can_pay` whose `dep_type` are `code`.

The `out_point` of `secp256k1_data` is

```
{
  tx_hash: 0xe2fb199810d49a4d8beec56718ba2593b665db9d52299a0f9e6e75416d73ff5c,
  index: 0x3
}
```

and the `out_point` of `anyone_can_pay` whose `dep_type` is `code` is

```
{
  tx_hash: 0x58eb58e2e3dd9852099a19424cf6e63b5238afe92e3085561b8feafced6d6876,
  index: 0x0
}
```

- Aggron

| parameter   | value                                                                |
| ----------- | -------------------------------------------------------------------- |
| `code_hash` | `0x3419a1c09eb2567f6552ee7a8ecffd64155cffe0f1796e6e61ec088d740c1356` |
| `hash_type` | `type`                                                               |
| `tx_hash`   | `0xec26b0f85ed839ece5f11c4c4e837ec359f5adc4420410f6453b1f6b60fb96a6` |
| `index`     | `0x0`                                                                |
| `dep_type`  | `dep_group`                                                          |

**Note:**

The `dep_type` of `anyone_can_pay` in Aggron is `dep_group` means that the content of this dep cell contains two cell deps which are `secp256k1_data` and `anyone_can_pay` whose `dep_type` are `code`.

The `out_point` of `secp256k1_data` is

```
{
  tx_hash: 0x8f8c79eb6671709633fe6a46de93c0fedc9c1b8a6527a18d3983879542635c9f,
  index: 0x3
}
```

and the `out_point` of `anyone_can_pay` is

```
{
  tx_hash: 0xce29e27734b3eb6f8b6a814cef217753ac2ccb4e4762ecc8b07d05634d8ba374,
  index: 0x0
}
```

Reproducible build is supported to verify the deploy script. To bulid the deployed anyone-can-pay lock script above, one can use the following steps:

```bash
$ git clone https://github.com/nervosnetwork/ckb-production-scripts
$ cd ckb-production-scripts
$ git checkout e570c11aff3eca12a47237c21598429088c610d5
$ git submodule update --init
$ make all-via-docker
```

Now you can compare the anyone-can-pay lock script generated at `build/anyone_can_pay` with the one deployed to CKB, they should be identical.

A draft of this specification has already been released, reviewed, and discussed in the community at [here](https://talk.nervos.org/t/rfc-anyone-can-pay-lock/4438) for quite some time.


================================================
File: rfcs/0027-block-structure/0027-block-structure.md
================================================
---
Number: "0027"
Category: Informational
Status: Draft
Author: Ian Yang <@doitian>
Created: 2020-04-30
---

# CKB Block Structure

In CKB, Block is a container of transactions. It carries the information required by consensus so the participants can verify and recognize the canonical chain.

The snippet below lists the molecule schema definitions related to block. The following paragraphs will explain these structures field by field.

```
array ProposalShortId [byte; 10];

vector UncleBlockVec <UncleBlock>;
vector TransactionVec <Transaction>;
vector ProposalShortIdVec <ProposalShortId>;

table Block {
    header:                 Header,
    uncles:                 UncleBlockVec,
    transactions:           TransactionVec,
    proposals:              ProposalShortIdVec,
}

struct Header {
    raw:                    RawHeader,
    nonce:                  Uint128,
}

struct RawHeader {
    version:                Uint32,
    compact_target:         Uint32,
    timestamp:              Uint64,
    number:                 Uint64,
    epoch:                  Uint64,
    parent_hash:            Byte32,
    transactions_root:      Byte32,
    proposals_hash:         Byte32,
    uncles_hash:            Byte32,
    dao:                    Byte32,
}

table UncleBlock {
    header:                 Header,
    proposals:              ProposalShortIdVec,
}
```


※ [blockchain.mol in ckb v0.34.1](https://github.com/nervosnetwork/ckb/blob/v0.34.1/util/types/schemas/blockchain.mol)

![](ckb_block_structure.png)

## Block

A Block can be split into two parts, header, and body. The field `header` is the header part. The remaining fields, `uncles`, `transactions`, and `proposals` are the body part.

```
table Block {
    header:                 Header,
    uncles:                 UncleBlockVec,
    transactions:           TransactionVec,
    proposals:              ProposalShortIdVec,
}
```

The header contains commitments on the body fields to ensure data integrity. CKB client can download and verify the header first, then download the much larger body part. Since PoW verification only requires header and uncles count in an epoch, this design can avoid wasting the bandwidth to download garbage data.

## Header

To ease PoW computation, the header is split into `raw` and `nonce`. 

```
struct Header {
    raw:                    RawHeader,
    nonce:                  Uint128,
}
```

The header must meet the last inequality in the following snippet:

```
pow_hash := ckbhash(molecule_serialize(raw))
pow_message := pow_hash || to_le(nounce)
pow_output := eaglesong(pow_message)
// for testnet, there is another round of hash
// pow_output = ckbhash(pow_output)

from_be(pow_output) <= compact_to_target(raw.compact_target)
```

※ [eaglesong in ckb v0.34.1](https://github.com/nervosnetwork/ckb/blob/v0.34.1/pow/src/eaglesong.rs)

Functions used in the pseudocode:

* `:=`: assignment
* `||`: binary concatenation.
* `ckbhash`: Blake2b hash with CKB specific configuration, see Appendix.
* `to_le`: Convert unsigned integer to bytes in little-endian. The bytes count is the same with the integer width.
* `from_be`: Convert bytes encoded in big-endian to an unsigned integer.
* `molecule_serialize`: Serialize a structure into binary using its schema.
* `eaglesong`: See RFC [Eaglesong (Proof-of-Work Function for Nervos CKB)](https://github.com/nervosnetwork/rfcs/blob/master/rfcs/0010-eaglesong/0010-eaglesong.md).
* `compact_to_target`: `raw.compact_target` encodes the difficulty target in a compact form. This function restores the target from the compact form.

The block is referenced by the header hash, for example, in `raw.parent_hash`.

```
header_hash := ckb_hash(molecule_serialize(header))
```

※ [HeaderReader::calc_header_hash in util/types/src/extension/calc_hash.rs](https://github.com/nervosnetwork/ckb/blob/v0.34.1/util/types/src/extension/calc_hash.rs#L103)

Notice that Header and RawHeader are all fixed-size structures. The serialization of them is just the simple binary concatenation of the fields in order.

## RawHeader

RawHeader is the payload of the block header.

### `version (Uint32)`

It must equal to 0 now and is reserved for future upgrades.

### `compact_target (Uint32)`

The header `compact_target` is the encoded form of the target threshold as it appears in the block header.

It is similar to `nBits` in bitcoin, the original `nBits` implementation inherits properties from a signed data class, allowing the target threshold to be negative if the high bit of the significant is set. This is useless—the header hash is treated as an unsigned number, so it can never be equal to or lower than a negative target threshold.

In CKB, the "compact" format is a representation of a whole number N using an unsigned 32bit number similar to a floating-point format. 

* The most significant 8 bits are the unsigned exponent of base 256.
* This exponent can be thought of as "number of bytes of N" in which the first 3 bytes are the mantissa. 
* The lower 24 bits are the mantissa. 

```
N = mantissa * 256^(exponent-3)
```

![](compact_target.png)

Python 3 Example and test vectors:

``` python
import unittest

def compact_to_target(compact):
    exponent = compact >> 24
    mantissa = compact & 0x00ffffff
    rtn = 0
    if (exponent <= 3):
        mantissa >>= (8 * (3 - exponent))
        rtn = mantissa
    else:
        rtn = mantissa
        rtn <<= (8 * (exponent - 3))
    overflow = mantissa != 0 and (exponent > 32)
    return rtn, overflow


def target_to_compact(target):
    bits = (target).bit_length()
    exponent = ((bits + 7) // 8)
    compact = target << (
        8 * (3 - exponent)) if exponent <= 3 else (target >> (8 * (exponent - 3)))
    compact = (compact | (exponent << 24))
    return compact


class TestCompactTarget(unittest.TestCase):

    def test_compact_target1(self):
        compact = target_to_compact(0x2)
        self.assertEqual('0x1020000', hex(compact))
        target, overflow = compact_to_target(0x1020000)
        self.assertTupleEqual((2, False), (target, overflow))

    def test_compact_target2(self):
        compact = target_to_compact(0xfe)
        self.assertEqual('0x1fe0000', hex(compact))
        target, overflow = compact_to_target(0x1fedcba)
        self.assertTupleEqual((0xfe, False), (target, overflow))


if __name__ == '__main__':
    unittest.main()
```

See details in the source code [difficulty.rs](https://github.com/nervosnetwork/ckb/blob/develop/util/types/src/utilities/difficulty.rs).

The `compact_target` does not change in an epoch. In a new epoch, the difficulty is adjusted according to all the headers and the total uncles count in the previous epoch. See [Dynamic Difficulty Adjustment Mechanism](https://github.com/nervosnetwork/rfcs/blob/master/rfcs/0020-ckb-consensus-protocol/0020-ckb-consensus-protocol.md#Dynamic-Difficulty-Adjustment-Mechanism) in the consensus protocol RFC.

The genesis block `compact_target` is hardcoded in the consensus specification.

### `timestamp (Uint64)`

The time when the block is created encoded as Unix Timestamp, in milliseconds. For example

```
1588233578000 is Thu, 30 Apr 2020 07:59:38 +0000
```

There's a consensus rule to verify that the block timestamp must be larger than the median timestamp of the previous 37 blocks.

The Nervos Network CKB client rejects blocks in which timestamp is more than 15 seconds in the future, however, this is not a consensus rule.

![](timestamp.png)

The genesis block `timestamp` is hardcoded in the consensus specification.

### `number (Uint64)`

A sequential number which encodes the genesis block as 0 and the child block number is the parent block number plus 1.

![](number.png)

```
genesis_header.number := 0
header.number := parent_header.number + 1
```

### `epoch (Uint64)`

This field encodes the epoch number and the fraction position of this block in the epoch.

The lower 56 bits of the epoch field are split into 3 parts (listed in the order from higher bits to lower bits):

* The highest 16 bits represent the epoch length
* The next 16 bits represent the current block index in the epoch, starting from 0.
* The lowest 24 bits represent the current epoch number.

![](epoch.png)

Assume there's a block, which number is 11555 and in epoch 50. The epoch 50 starts from block 11000 and have 1000 blocks. The epoch field for this particular block will then be 1,099,520,939,130,930, which is calculated in the following way:

```
50 | ((11555 - 11000) << 24) | (1000 << 40)
```

The genesis epoch number is 0 and the genesis block relative index in the epoch is also 0. So the genesis block epoch field only depends on the genesis epoch length, which is hardcoded in the consensus specification.

### `parent_hash (Byte32)`

The header hash of the parent block. The genesis block `parent_hash` is hardcoded in the consensus specification.

### `transactions_root (Byte32)`

This is the commitment to all the transactions in the block.

It is a hash on two Merkle Tree roots

```
ckbhash(T || W)
```

The function `ckbhash` is the default digest algorithm in CKB, see Appendix.

`T` is the root of a CKB Merkle Tree, which items are the transaction hashes of all the transactions in the block.

`W` is also the root of a CKB Merkle Tree, but the items are the transaction witness hashes of all the transactions in the block.

![](transactions_root.png)

See Appendix for the references of CKB Merkle Tree and two different transaction hashes.

### `proposals_hash (Byte32)`

Field `proposals_hash` is the hash on `proposals` in the block body.

It is all zeros when `proposals` is empty, or `ckbhash` on all the bytes concatenated together.

```
proposals_hash = 0 when proposals are empty, otherwise

proposals_hash = ckb_hash(P1 || P2 || ... || Pn)
  where Pi is the i-th ProposalShortId in proposals
```

### `uncles_hash (Byte32)`

Field `uncles_hash` is the hash on `uncles` in the block body.

It is all zeros when `uncles` is empty, or `ckbhash` on all the uncle header hashes concatenated together.

```
uncles_hash = 0 when uncles is empty, otherwise

uncles_hash = ckb_hash(U1 || U2 || ... || Un)
  where Ui is the header_hash of the i-th uncle in uncles
```

Recall that header hash is the hash of the whole serialized header.

```
header_hash := ckb_hash(molecule_serialize(header))
```

### `dao (Byte32)`

The dao field compacts 4 64-bits unsigned integers in little-endian.

* `C_i`, bytes 0 to 7
* `AR_i`, bytes 8 to 15
* `S_i`, bytes 16 to 23
* `U_i`, bytes 24 to 31

See RFC [Deposit and Withdraw in Nervos DAO](https://github.com/nervosnetwork/rfcs/blob/master/rfcs/0023-dao-deposit-withdraw/0023-dao-deposit-withdraw.md#calculation).

## Transactions

The field `block.transactions` is the ordered list of transactions in the block. The first transaction must be the cellbase. See the transaction informational RFC.

## Uncles

The field `block.uncles` is the ordered list of uncle blocks.

A block B1 is considered to be the uncle of another block B2 if all the following conditions are met:

1. They are in the same epoch, sharing the same difficulty;
2. B2 block number is larger than B1;
3. B1's parent is either B2's ancestor or an uncle embedded in B2 or any of B2's ancestors.
4. B2 is the first block in its chain to refer to B1.

The chain stores only the uncle block header and proposal IDs. The header ensures the block is covered by PoW and can pass the consensus rules on uncle blocks. Proposal IDs are there because a block can commit transactions proposed in an uncle.

## Proposals

Transaction proposal ID is the first 10 bytes of the Transaction Hash.

Unlike Bitcoin, CKB requires to propose the transaction proposal IDs before committing the transaction into the chain.

A transaction is said proposed in block B if its proposal ID appears in B's or B's uncles' `proposals` field. A transaction is commit if it is included in the block `transactions` field.

Two protocol parameters *close* and *far* define the closest and farthest on-chain distance between a transaction's proposal and commitment.

A non-cellbase transaction commit in block which number is *c* must have been proposed in block with number *p*, where

```
close <= c - p <= far
```

In CKB Lina the mainnet, *close* is 2 and *far* is 10. Thus

```
2 <= c - p <= 10
```

![](proposals.png)

## Appendix

### Molecule

Molecule is a serialization framework.

- [Specification](https://github.com/nervosnetwork/rfcs/blob/master/rfcs/0008-serialization/0008-serialization.md)
- [Rust/C Code Generator](https://github.com/nervosnetwork/molecule)

The molecule schema used in CKB can be found in [util/types/schemas](https://github.com/nervosnetwork/ckb/tree/develop/util/types/schemas)

### ckbhash

CKB uses [blake2b](https://blake2.net/blake2.pdf) as the default hash algorithm with following configurations:

- output digest size: 32
- personalization: ckb-default-hash

Python 3 Example and test vectors:

```python
import hashlib
import unittest

def ckbhash():
    return hashlib.blake2b(digest_size=32, person=b'ckb-default-hash')

class TestCKBBlake2b(unittest.TestCase):

    def test_empty_message(self):
        hasher = ckbhash()
        hasher.update(b'')
        self.assertEqual('44f4c69744d5f8c55d642062949dcae49bc4e7ef43d388c5a12f42b5633d163e', hasher.hexdigest())

if __name__ == '__main__':
    unittest.main()
```

## CKB Merkle Tree 

CKB Merkle Tree is a [CBMT](https://github.com/nervosnetwork/rfcs/blob/master/rfcs/0006-merkle-tree/0006-merkle-tree.md) using following merge function:

```
ckbhash(left || right)
```

* ckbhash is the hash function.
* `||` denotes binary concatenation.

## Transaction Hash

The transaction is serialized via Molecule in CKB. Its schema is:

```
table Transaction {
    raw:            RawTransaction,
    witnesses:      BytesVec,
}
```

The transaction hash is the `ckbhash` on the serialized `raw`.

## Transaction Witness Hash

The transaction is serialized via Molecule in CKB.

The transaction witness hash is the `ckbhash` on the whole serialized transaction.


================================================
File: rfcs/0028-change-since-relative-timestamp/0028-change-since-relative-timestamp.md
================================================
---
Number: "0028"
Category: Standards Track
Status: Draft
Author: Ian Yang <@doitian>
Created: 2021-02-03
---

# Use input cell committing block timestamp as the start time for the relative timestamp in `since`

## Abstract

This document proposes a transaction verification consensus change. When the `since` field of the transaction input uses a relative timestamp, the commitment block header timestamp is used as the base value instead of the median timestamp of previous 37 blocks.

This is a modification to the RFC17 [Transaction valid since](../0017-tx-valid-since/0017-tx-valid-since.md).

## Motivation

The current consensus rule uses the median of the timestamps in the 37 blocks preceding the referenced cell commitment block. Getting the median timestamp is resource consuming because it requires either getting 37 block headers or caching the median timestamp for each block. The intention of using median time was to prevent miners from manipulating block timestamp to include more transactions. But it is safe to use the committing block timestamp as the start time because of two reasons:

1. The timestamp in the block header has already been verified by the network that it must be larger than the median of the previous 37 blocks and less than or equal to the current time plus 15 seconds. (See [RFC27](../0027-block-structure/0027-block-structure.md#timestamp-uint64))
2. The transaction consuming a cell with the `since` requirement must wait until the cell is mature. During this waiting time, the transaction that created the cell has accumulated enough confirmations that it is difficult for the miner to manipulate it.

## Specification

When an input `since` field is present, and

* The `metric_flag` is block timestamp (10).
* The `relative_flag` is relative (1).

The input since precondition is fulfilled when

```
MedianTimestamp ≥ StartTime + SinceValue
```

where

* `StartTime` is the timestamp field in the commitment block header.
* `SinceValue` is the `value` part of the `since` field.
* `MedianTimestamp` is the median timestamp of the previous 37 blocks preceding the block if the transaction is in the block, or the latest 37 blocks if the transaction is in the pool.

The only change is `StartTime`, which was the median of the previous 37 blocks preceding the one that has committed the consumed cell. Because block timestamp must be larger than the median of its previous 37 blocks, the new consensus rule is more strict than the old rule. A transaction that is mature under the old rule may be immature under the new rule, but a transaction that is mature under the new rule must be mature under the old rule.

## Test Vectors

Following is an example that a transaction is mature using the new rule but is immature using the old rule.

Assuming that:

* A transaction consumes a cell in block S and is about to be committed into block T with a since requirement that:
	* The `metric_flag` is block timestamp (10).
	* The `relative_flag` is relative (1).
	* The `value` is 600,000 (10 minutes).
* The median of the previous 37 blocks preceding block S is 10,000.
* The timestamp of block S is 20,000.
* The median of the previous 37 blocks preceding block T is 615,000

In the old consensus, `StartTime` + `SinceValue` = 10,000 + 600,000 = 610,000, which is less than the `MedianTimestamp` 615,000, thus the transaction is mature.

But in the new rule, `StartTime` + `SinceValue` = 20,000 + 600,000 = 620,000 ≥ 615,000, so the transaction is still immature.

## Deployment

The deployment can be performed in two stages.

The first stage will activate the new consensus rule starting from a specific epoch. The mainnet and testnet will use different starting epochs and all other chains will use the new rule from epoch 0.

After the fork is activated, and if the transactions in the old epochs all satisfy the new rule, the old consensus rule will be removed and the new rule will be applied from the genesis block.

## Backward compatibility

Because the new consensus rule is more strict than the old one, this proposal can be deployed via a soft fork.


================================================
File: rfcs/0029-allow-script-multiple-matches-on-identical-code/0029-allow-script-multiple-matches-on-identical-code.md
================================================
---
Number: "0029"
Category: Standards Track
Status: Proposal
Author: Ian Yang <@doitian>
Created: 2021-02-03
---

# Allow Multiple Cell Dep Matches When There Is No Ambiguity

## Abstract

This document proposes a transaction verification consensus change to allow multiple cell dep matches on type script hash when all the matches are resolved to the same script code.

## Motivation

CKB locates the code for lock and type script to execute via data hash or type script hash.

CKB allows multiple matches on data hash because it is safe. Data hash is the hash on the code, thus multiple matches must have the same code. This does not hold for type hash. Two cells with the same type script hash may have different contents.

Currently, CKB does not allow multiple matches on type script hash. But in many cases, multiple matches on type script hash do not introduce ambiguity if all the matches have the same data hash as well. Because in the most scenarios, the cause is that the transaction uses two dep groups which contain duplicated cells, the multiple matches on type script hash really point to the same cell.

```
# An example that multiple matches on the type script hash really are the same cell.
cell_deps:
  - out_point: ...
    # Expands to
    # - out_point: Cell A
    dep_group: DepGroup

  - out_point: ...
    # Expands to
    # - out_point: Cell A
    dep_group: DepGroup

inputs:
  - out_point: ...
    lock: ...
    type:
      code_hash: hash(Cell A.type)
      hash_type: Type
```

Based on the observation above, this RFC proposes to allow the multiple matches on the type script hash if they all have the same data.

## Specification

When the transaction verifier locates script code in dep cell via data hash, multiple matches are allowed. This is the same as before.

When the verifier locates code via type hash, multiple matches are allowed if all the matched cells have the same data, otherwise, the transaction is invalid and the verification fails. This is the modification introduced by this RFC.

## Test Vectors

Multiple matches of data hash. This works in both the old rule and the new one.

```
#  hash(Cell B.data) equals to hash(Cell A.data)
cell_deps:
  - out_point: ...
    # Expands to
    # - out_point: Cell A
    dep_group: DepGroup

  - out_point: ...
    # Expands to
    # - out_point: Cell B
    dep_group: DepGroup

inputs:
  - out_point: ...
    lock:
      code_hash: hash(Cell A.data)
      hash_type: Data
```

Multiple matches of type hash which all resolve to the same code. This transaction is invalid using the old rule but valid using the new rule.

```
#  hash(Cell B.data) equals to hash(Cell A.data)
# and hash(Cell B.type) equals to hash(Cell A.type)
cell_deps:
  - out_point: ...
    # Expands to
    # - out_point: Cell A
    dep_group: DepGroup

  - out_point: ...
    # Expands to
    # - out_point: Cell B
    dep_group: DepGroup

inputs:
  - out_point: ...
    lock: ...
    type:
      code_hash: hash(Cell A.type)
      hash_type: Type
```

## Deployment

The deployment can be performed in two stages.

The first stage will activate the new consensus rule starting from a specific epoch. The mainnet and testnet will use different starting epochs and all the development chains initialized via the default settings in this stage will use the new rule from epoch 0.

After the fork is activated, the old rule will be replaced by the new rule starting from the genesis block by new CKB node versions.

## Backward compatibility

The consensus rule proposed in this document is looser, so it must be activated via a hard fork. The blocks accepted by new version clients may be rejected by the old versions.


================================================
File: rfcs/0030-ensure-index-less-than-length-in-since/0030-ensure-index-less-than-length-in-since.md
================================================
---
Number: "0030"
Category: Standards Track
Status: Proposal
Author: Ian Yang <@doitian>
Created: 2021-02-04
---

# Ensure That Index Is Less Than Length In the Input Since Field Using Epoch With Fraction

## Abstract

This document proposes adding a new consensus rule to verify the `since` field in the transaction.

As described in the RFC17, [Transaction valid since](../0017-tx-valid-since/0017-tx-valid-since.md), when a transaction input uses the epoch with fraction in the `since` field, the `value` is an encoded rational number `E I/L`, where

- `E` is the epoch number.
- `I` is the block index in the epoch.
- `L` is the epoch length.

This RFC requires that when any transaction uses the epoch with fraction as the unit, the encoded number `E I/L` is valid only if

- `I` is less than `L`, or
- `I` and `L` are both zero.

If any `since` field is invalid, the transaction is rejected.

## Motivation

The `since` field prevents the transaction from being mined before an absolute or relative time.

When the `since` field uses epoch with fraction number as the unit, the `value` is an encoded rational number `E I/L`. If it is a relative time, the rational number is used as it is. But when it is the absolute time, the special rule, **Absolute Epoch With Fraction Value Normalization** as mentioned in RFC17, requires normalizing the number to `E+1 0/1` when `I` equals to or is larger than `L`.
This document suggests adding a new rule to verify that when `since` uses epoch as the unit, it must ensure that the index `I` is less than the length `L`.

## Specification

This RFC adds a new verification requirement on the transaction `since` field.

When an input `since` field is present, and the `metric_flag` is epoch (01), the `value` part is the encoded number `E I/L`. No matter whether the relative flag is `relative` or `absolute`, the number is valid if and only if

- `I` is less than `L`, or
- `I` and `L` are both zero.

There are no changes to the rules in RFC17, except that **Absolute Epoch With Fraction Value Normalization** is no longer needed.

## Test Vectors

When `since` uses the absolute epoch `99 360/180`, and the current epoch is `100 0/180`, the transaction is mature using the old consensus rule but is invalid using the new rule.

## Deployment

The deployment can advance in two stages.

The first stage will activate the new consensus rule, starting from a specific epoch. The mainnet and testnet will use different starting epochs and all other chains will use the new rule from epoch 0.

The second stage is optional. After the new rule is active, and the blocks in the chain before activation can also pass the new consensus rule, the old rule is redundant and can be safely removed.

## Backward compatibility

The new rule is stricter than the old one thus it can be deployed via a soft fork. When most mining nodes have upgraded to the new version, the old version full nodes can keep up to date. Blocks generated by old version mining nodes may be rejected by new version full nodes.


================================================
File: rfcs/0031-variable-length-header-field/0031-variable-length-header-field.md
================================================
---
Number: "0031"
Category: Standards Track
Status: Proposal
Author: Ian Yang <@doitian>
Created: 2021-02-07
---

# Add a variable length field in the block

## Abstract

This document proposes adding an optional variable length field to the block.

## Motivation

In the consensus version before activating this RFC, the block header is a fixed length structure. Each header consists of 208 bytes.

Many extensions require adding new fields into the block, but there’s no enough reserved bits for them. For example, flyclient requires a 64-byte hash in the header.
Workaround exists such as storing these data in the cellbase transaction, but it has a big overhead for clients which want to verify the chain using PoW only. Because they have to download the cellbase transaction and the merkle tree proof of the cellbase transaction, which can be larger than the block header itself.

This document proposes a solution to add a variable length field in the block. How to interpret the new field is beyond the scope of this document and must be defined and deployed via a future soft fork. Although the field is added to the block body, nodes can synchronize the block header and this field together in the future version.

## Specification

The block header is encoded as a molecule struct, which consists of fixed length fields. The header binary is just the concatenation of all the fields in sequence.

There are many ways to add the variable length field to the block header. This RFC proposes to replace the `uncles_hash` in the header with the new field `extra_hash`, which is also a 32-byte hash. The block will have a new field `extension`.

There are two important time points to deploy this RFC, activation epoch A and extension application epoch B.

In blocks before epoch A, the `extension` must be absent. The value of `extra_hash` is the same as the original `uncles_hash` in these blocks, so this RFC will not change the serialized headers of existing blocks. The field `extra_hash` is all zeros when the `uncles` field is empty, or `ckbhash` on all the uncle header hashes concatenated together.

```
uncles_hash = 0 when uncles is empty, otherwise

uncles_hash = ckbhash(U1 || U2 || ... || Un)
    where Ui is the header_hash of the i-th uncle in uncles
```

See Appendix for the default hash function `ckbhash`. The annotation `||` means bytes concatenation.

In blocks generated since epoch A, `extension` can be absent, or any binary with 1 to 96 bytes. The upper limit 96 prevents abusing this field because there's no consensus rule to verify the content of `extension`. The 96 bytes limit allows storing the 64-byte flyclient hash and an extra 32-byte hash on further extension bytes.

The `extra_hash` is defined as:

* When `extension` is empty, `extra_hash` is the same as the `uncles_hash`.
* Otherwise `extra_hash = ckbhash(uncles_hash || ckbhash(extension))`

Since epoch B, consensus will define the schema of `extension` and verify the content. This is a soft fork if the `extension` is at most 96 bytes, because nodes deployed since epoch A do not verify the content of `extension`.  

### P2P Protocols Changes

The field `uncles_hash` in the block header is renamed to `extra_hash`.

```
struct RawHeader {
    version:                Uint32,
    compact_target:         Uint32,
    timestamp:              Uint64,
    number:                 Uint64,
    epoch:                  Uint64,
    parent_hash:            Byte32,
    transactions_root:      Byte32,
    proposals_hash:         Byte32,
    extra_hash:             Byte32,
    dao:                    Byte32,
}
```

The new field `extension` will be added to the block body and following data structures:

```
table Block {
    header:       Header,
    uncles:       UncleBlockVec,
    transactions: TransactionVec,
    proposals:    ProposalShortIdVec,
    extension:    Bytes,
}

table CompactBlock {
    header:                 Header,
    short_ids:              ProposalShortIdVec,
    prefilled_transactions: IndexTransactionVec,
    uncles:                 Byte32Vec,
    proposals:              ProposalShortIdVec,
    extension:              Bytes,
}
```

For blocks before the activation epoch A, `extension` must be absent. After activation, the node must verify that `extension` is absent or a binary with 1 to 96 bytes, and `uncles` and `extension` match the `extra_hash` in the header.

Pay attention that the `extension` field will occupy the block size. See section [Block and Compact Block Structure](../0020-ckb-consensus-protocol/0020-ckb-consensus-protocol.md#block-and-compact-block-structure) in RFC20 for details.

The uncle blocks packaged in `uncles` will not include the `extension` field.

### RPC Changes

* The `uncles_hash` is renamed to `extra_hash`.
* The new field `extension` is added to the block body RPC response. For blocks generated in ckb2019, it is always empty.

## Comparison With Alternative Solutions

1. [Appending the Field At the End](./1-appending-the-field-at-the-end.md)
2. [Using Molecule Table in New Block Headers](./2-using-molecule-table-in-new-block-headers.md)
3. [Appending a Hash At the End](./3-appending-a-hash-at-the-end.md)

## Test Vectors

### Block Hash

<details><summary>Block Template</summary>

```json
{
  "version": "0x0",
  "compact_target": "0x20010000",
  "current_time": "0x17af3f66555",
  "number": "0x3",
  "epoch": "0x3e80003000000",
  "parent_hash": "0xebf229020f333100942279dc33303ae0dfcbe720d8d11818687e6654c157294c",
  "cycles_limit": "0x2540be400",
  "bytes_limit": "0x91c08",
  "uncles_count_limit": "0x2",
  "uncles": [],
  "transactions": [
    {
      "hash": "0x9110ca9266f89938f09ae6f93cc914b2c856cc842440d56fda6d16ee62543f5c",
      "required": false,
      "cycles": "0x19f2d1",
      "depends": null,
      "data": {
        "version": "0x0",
        "cell_deps": [
          {
            "out_point": {
              "tx_hash": "0xace5ea83c478bb866edf122ff862085789158f5cbff155b7bb5f13058555b708",
              "index": "0x0"
            },
            "dep_type": "dep_group"
          }
        ],
        "header_deps": [],
        "inputs": [
          {
            "since": "0x0",
            "previous_output": {
              "tx_hash": "0xa563884b3686078ec7e7677a5f86449b15cf2693f3c1241766c6996f206cc541",
              "index": "0x7"
            }
          }
        ],
        "outputs": [
          {
            "capacity": "0x2540be400",
            "lock": {
              "code_hash": "0x709f3fda12f561cfacf92273c57a98fede188a3f1a59b1f888d113f9cce08649",
              "hash_type": "data",
              "args": "0xc8328aabcd9b9e8e64fbc566c4385c3bdeb219d7"
            },
            "type": null
          },
          {
            "capacity": "0x2540be400",
            "lock": {
              "code_hash": "0x9bd7e06f3ecf4be0f2fcd2188b23f1b9fcc88e5d4b65a8637b17723bbda3cce8",
              "hash_type": "type",
              "args": "0xc8328aabcd9b9e8e64fbc566c4385c3bdeb219d7"
            },
            "type": null
          },
          {
            "capacity": "0x2540be400",
            "lock": {
              "code_hash": "0x709f3fda12f561cfacf92273c57a98fede188a3f1a59b1f888d113f9cce08649",
              "hash_type": "data1",
              "args": "0xc8328aabcd9b9e8e64fbc566c4385c3bdeb219d7"
            },
            "type": null
          }
        ],
        "outputs_data": [
          "0x",
          "0x",
          "0x"
        ],
        "witnesses": [
          "0x550000001000000055000000550000004100000070b823564f7d1f814cc135ddd56fd8e8931b3a7040eaf1fb828adae29736a3cb0bc7f65021135b293d10a22da61fcc64f7cb660bf2c3276ad63630dad0b6099001"
        ]
      }
    }
  ],
  "proposals": [],
  "cellbase": {
    "hash": "0x185d1c46fe3c4a0a1a5ae47203df2aeebbb97ac353abcf2c6a3fc2548ecd4eda",
    "cycles": null,
    "data": {
      "version": "0x0",
      "cell_deps": [],
      "header_deps": [],
      "inputs": [
        {
          "since": "0x3",
          "previous_output": {
            "tx_hash": "0x0000000000000000000000000000000000000000000000000000000000000000",
            "index": "0xffffffff"
          }
        }
      ],
      "outputs": [],
      "outputs_data": [],
      "witnesses": [
        "0x590000000c00000055000000490000001000000030000000310000009bd7e06f3ecf4be0f2fcd2188b23f1b9fcc88e5d4b65a8637b17723bbda3cce80114000000c8328aabcd9b9e8e64fbc566c4385c3bdeb219d700000000"
      ]
    }
  },
  "work_id": "0x2",
  "dao": "0x105cabf31c1fa12eacfa6990f2862300bdaf44b932000000008d5fff03fbfe06",
  "extension": "0x626c6f636b202333"
}
```

</details>

<details><summary>Block</summary>

```json
{
  "header": {
    "version": "0x0",
    "compact_target": "0x20010000",
    "timestamp": "0x17af3f66555",
    "number": "0x3",
    "epoch": "0x3e80003000000",
    "parent_hash": "0xebf229020f333100942279dc33303ae0dfcbe720d8d11818687e6654c157294c",
    "transactions_root": "0x0bbf9d8946932c9c33a46c8d13b9ecfcf850ccc1728fc9c9c5d14710ad9428ad",
    "proposals_hash": "0x0000000000000000000000000000000000000000000000000000000000000000",
    "extra_hash": "0xfbbfbaaa0afac7730f4a6102b376986f1f288f3eccb18e0d16d58422aab28aad",
    "dao": "0x105cabf31c1fa12eacfa6990f2862300bdaf44b932000000008d5fff03fbfe06",
    "nonce": "0x6e43a02f3ed8bb00dea7f78c12fe94f5"
  },
  "uncles": [],
  "transactions": [
    {
      "version": "0x0",
      "cell_deps": [],
      "header_deps": [],
      "inputs": [
        {
          "since": "0x3",
          "previous_output": {
            "tx_hash": "0x0000000000000000000000000000000000000000000000000000000000000000",
            "index": "0xffffffff"
          }
        }
      ],
      "outputs": [],
      "outputs_data": [],
      "witnesses": [
        "0x590000000c00000055000000490000001000000030000000310000009bd7e06f3ecf4be0f2fcd2188b23f1b9fcc88e5d4b65a8637b17723bbda3cce80114000000c8328aabcd9b9e8e64fbc566c4385c3bdeb219d700000000"
      ]
    },
    {
      "version": "0x0",
      "cell_deps": [
        {
          "out_point": {
            "tx_hash": "0xace5ea83c478bb866edf122ff862085789158f5cbff155b7bb5f13058555b708",
            "index": "0x0"
          },
          "dep_type": "dep_group"
        }
      ],
      "header_deps": [],
      "inputs": [
        {
          "since": "0x0",
          "previous_output": {
            "tx_hash": "0xa563884b3686078ec7e7677a5f86449b15cf2693f3c1241766c6996f206cc541",
            "index": "0x7"
          }
        }
      ],
      "outputs": [
        {
          "capacity": "0x2540be400",
          "lock": {
            "code_hash": "0x709f3fda12f561cfacf92273c57a98fede188a3f1a59b1f888d113f9cce08649",
            "hash_type": "data",
            "args": "0xc8328aabcd9b9e8e64fbc566c4385c3bdeb219d7"
          },
          "type": null
        },
        {
          "capacity": "0x2540be400",
          "lock": {
            "code_hash": "0x9bd7e06f3ecf4be0f2fcd2188b23f1b9fcc88e5d4b65a8637b17723bbda3cce8",
            "hash_type": "type",
            "args": "0xc8328aabcd9b9e8e64fbc566c4385c3bdeb219d7"
          },
          "type": null
        },
        {
          "capacity": "0x2540be400",
          "lock": {
            "code_hash": "0x709f3fda12f561cfacf92273c57a98fede188a3f1a59b1f888d113f9cce08649",
            "hash_type": "data1",
            "args": "0xc8328aabcd9b9e8e64fbc566c4385c3bdeb219d7"
          },
          "type": null
        }
      ],
      "outputs_data": [
        "0x",
        "0x",
        "0x"
      ],
      "witnesses": [
        "0x550000001000000055000000550000004100000070b823564f7d1f814cc135ddd56fd8e8931b3a7040eaf1fb828adae29736a3cb0bc7f65021135b293d10a22da61fcc64f7cb660bf2c3276ad63630dad0b6099001"
      ]
    }
  ],
  "proposals": [],
  "extension": "0x626c6f636b202333"
}
```

</details>

The hashes:

```
Block Hash:
0xb93dad02d24e9d30c49023d08f84dd8ec34118c1bfec9ed432b75619964686c3

Transaction Hashes:
0x185d1c46fe3c4a0a1a5ae47203df2aeebbb97ac353abcf2c6a3fc2548ecd4eda
0x9110ca9266f89938f09ae6f93cc914b2c856cc842440d56fda6d16ee62543f5c
```

## Appendix

### ckbhash

CKB uses [blake2b](https://blake2.net/blake2.pdf) as the default hash algorithm with following configurations:

- output digest size: 32
- personalization: ckb-default-hash

Python 3 Example and test vectors:

```python
import hashlib
import unittest

def ckbhash():
    return hashlib.blake2b(digest_size=32, person=b'ckb-default-hash')

class TestCKBBlake2b(unittest.TestCase):

    def test_empty_message(self):
        hasher = ckbhash()
        hasher.update(b'')
        self.assertEqual('44f4c69744d5f8c55d642062949dcae49bc4e7ef43d388c5a12f42b5633d163e', hasher.hexdigest())

if __name__ == '__main__':
    unittest.main()
```


================================================
File: rfcs/0031-variable-length-header-field/1-appending-the-field-at-the-end.md
================================================
### Appending the Field At the End

The block header size is at least 208 bytes. The first 208 bytes are encoded the same as the current header. The remaining bytes are the variable length field.

```
+-----------------------+-----------+
|                       |           |
|    208-bytes header   | New Field |
|                       |           |
+-----------------------+-----------+
```


Pros

- Apps that are not interested in the new field can just read the first 208 bytes.

Cons

- It's not a valid Molecule buffer.
- It may break the old contract which assumes that the header has only 208 bytes.
- Nodes that do not need the new field still has to download it.
- Header is a variable length structure now.

================================================
File: rfcs/0031-variable-length-header-field/2-using-molecule-table-in-new-block-headers.md
================================================
### Using Molecule Table in New Block Headers

This solution uses a different molecule schema for the new block headers. If the block header size is 208 bytes, it's encoded using the old schema, otherwise it uses the new one. The new schema converts `RawHeader` into a molecule table and adds a variable length bytes field at the end of `RawHeader`.

```
old one:
        208 bytes
+-----+-----------------+
|     |                 |
|Nonce| RawHeader Stuct |
|     |                 |
+-----+-----------------+

new one:

+-----+-------------------------------+
|     |                               |
|Nonce| RawHeader Table               |
|     |                               |
+-----+-------------------------------+
```

Pros

- It is a valid Molecule buffer.

Cons

- It may break the old contract which assumes that the header has only 208 bytes and is just the concatenation of all members.
- Nodes that do not need the new field still has to download it.
- The molecule table header overhead.
- Header is a variable length structure now.

================================================
File: rfcs/0031-variable-length-header-field/3-appending-a-hash-at-the-end.md
================================================
### Appending a Hash At the End

Instead of adding the new field directly at the end of the header, this solution adds a 32 bytes hash at the end of the header which is the hash of the new variable length field. The header is still a fixed length struct but is 32 bytes larger. If client does not need the extra field, it only has the 32 bytes overhead. Otherwise it has to download both the header and the extra field and verify that the hash matches.

```
+-----------------------+--+
|                       |  |
|    208-bytes header   | +----+
|                       |  |   |
+-----------------------+--+   |
                               | Hash of
                               |
                               v
                         +-----+-----+
                         |           |
                         | New Field |
                         |           |
                         +-----------+

```

Pros

- It is a valid Molecule buffer.
- The header still has the fixed length.
- Nodes that do not want the new field only need to download an extra hash to verify the PoW.

Cons

- It may break the old contract which assumes that the header has only 208 bytes.
- Extra P2P messages must be added to download the new extension field.


================================================
File: rfcs/0032-ckb-vm-version-selection/0032-ckb-vm-version-selection.md
================================================
---
Number: "0032"
Category: Standards Track
Status: Proposal
Author: Ian Yang <@doitian>
Created: 2021-04-26
---

# CKB VM Version Selection

## Abstract

This RFC proposes a mechanism to decide on the CKB VM version to execute the transaction scripts.

## Motivation

It's essential to keep improving CKB VM because it is the computation bottleneck of the whole network. The upgrade packages can improve the performance, bring bug fixings and add new RISC-V extensions. However the upgrade should not break the old code, users must have the opt-in option to specify the VM version.

This RFC proposes a general mechanism that determines how the CKB node chooses the CKB VM version for a transaction script group.

## Specification

When CKB launches the testnet Lina, it only has one VM version, the version 0. The first hard fork will bring VM version 1 which coexists with version 0. Users have the opt-in option to specify which VM version to run the script of a cell by setting the `hash_type` field.

In CKB, each VM version also has its bundled instruction set, syscalls and cost model. The [rfc3], [rfc5], [rfc9] and [rfc14] have defined what is VM version 0. VM version 1 is version 0 plus the revisions mentioned in [rfc33] and [rfc34].

[rfc3]: ../0003-ckb-vm/0003-ckb-vm.md
[rfc5]: ../0005-priviledged-mode/0005-priviledged-mode.md
[rfc9]: ../0009-vm-syscalls/0009-vm-syscalls.md
[rfc14]: ../0014-vm-cycle-limits/0014-vm-cycle-limits.md
[rfc33]: ../0033-ckb-vm-version-1/0033-ckb-vm-version-1.md
[rfc34]: ../0034-vm-syscalls-2/0034-vm-syscalls-2.md

The first hard fork takes effect from an epoch decided by the community consensus. For all the transactions in the blocks before the activation epoch, they must run the CKB VM version 0 to verify all the script groups. In these transactions, the `hash_type` in cell lock and type script must be 0 or 1 in the serialized molecule data.

After the fork is activated, CKB nodes must choose the CKB VM version for each script group. The allowed values for the `hash_type` field in the lock and type script are 0, 1, and 2. Cells are sorted into different groups if they have different `hash_type`. According to the value of `hash_type`:

* When the `hash_type` is 0, the script group matches code via data hash and will run the code using the CKB VM version 0.
* When the `hash_type` is 1, the script group matches code via type script hash and will run the code using the CKB VM version 1.
* When the `hash_type` is 2, the script group matches code via data hash and will run the code using the CKB VM version 1.

| `hash_type` | matches by       | VM version |
| ----------- | ---------------- | ---------- |
| 0           | data hash        | 0          |
| 1           | type script hash | 1          |
| 2           | data hash        | 1          |

The transaction is invalid if any `hash_type` is not in the allowed values 0, 1, and 2.

See more information about code locating using `hash_type` in [rfc22].

[rfc22]: ../0022-transaction-structure/0022-transaction-structure.md

The `hash_type` encoding pattern ensures that if a script matches code via type hash, CKB always uses the latest available version of VM depending when the script is executed. But if the script matches code via data hash, the VM version to execute is determined when the cell is created.

Here is an example of when VM version 2 is available:

| `hash_type` | matches by       | VM version |
| ----------- | ---------------- | ---------- |
| 0           | data hash        | 0          |
| 1           | type script hash | 2          |
| 2           | data hash        | 1          |
| \*          | data hash        | 2          |

> \* The actual value to represent data hash plus VM version 2 is undecided yet.

Cell owners can trade off between the determination and VM performance boost when creating the cell. They should use data hash for determination, and type hash for the latest VM techniques.

In [nervosnetwork/ckb](https://github.com/nervosnetwork/ckb), the `hash_type` is returned in the JSON RPC as an enum. Now it has three allowed values:

* 0: "data"
* 1: "type"
* 2: "data1"

## RFC Dependencies

This RFC depends on [rfc33], [rfc34], and [rfc35]. The 4 RFCs must be activated together at the same epoch.

[rfc35]: ../0035-ckb2021-p2p-protocol-upgrade/0035-ckb2021-p2p-protocol-upgrade.md

The first two RFCs, [rfc33] and [rfc34] are the specification of VM version 1. The [rfc35] proposes to run two versions of transaction relay protocols during the fork, because the VM selection algorithm depends on which epoch the transaction belongs to, thus it is not deterministic for transactions still in the memory pool.

## Rationale

There are many other solutions to select VM versions. The current solution results from discussion and trade-off. Following are some example alternatives:

Consistently uses the latest VM version. The users cannot specify the VM versions for transactions, and the version selection will be non-determine cause it will depend on the chain state.
* Depend on the script code cell epoch. Use the old VM version if the code cell is deployed before the fork, and use the new one otherwise. The problem with this solution is that anyone can re-deploy the cell and construct the transaction using the new code cell to choose VM versions.

## Backward compatibility

For cell scripts which reference codes via data hash, they will use the same VM before and after the fork. For those referenced by type hash, they will use the different VM versions. The dApps developers must ensure the compatibility of their scripts and upgrade them if necessary.

## Test Vectors

### Transaction Hash

This is a transaction containing `data1` hash type.

<details><summary>JSON</summary>

```json
{
    "version": "0x0",
    "cell_deps": [
    {
        "out_point": {
        "tx_hash": "0xace5ea83c478bb866edf122ff862085789158f5cbff155b7bb5f13058555b708",
        "index": "0x0"
        },
        "dep_type": "dep_group"
    }
    ],
    "header_deps": [],
    "inputs": [
    {
        "since": "0x0",
        "previous_output": {
        "tx_hash": "0xa563884b3686078ec7e7677a5f86449b15cf2693f3c1241766c6996f206cc541",
        "index": "0x7"
        }
    }
    ],
    "outputs": [
    {
        "capacity": "0x2540be400",
        "lock": {
        "code_hash": "0x709f3fda12f561cfacf92273c57a98fede188a3f1a59b1f888d113f9cce08649",
        "hash_type": "data",
        "args": "0xc8328aabcd9b9e8e64fbc566c4385c3bdeb219d7"
        },
        "type": null
    },
    {
        "capacity": "0x2540be400",
        "lock": {
        "code_hash": "0x9bd7e06f3ecf4be0f2fcd2188b23f1b9fcc88e5d4b65a8637b17723bbda3cce8",
        "hash_type": "type",
        "args": "0xc8328aabcd9b9e8e64fbc566c4385c3bdeb219d7"
        },
        "type": null
    },
    {
        "capacity": "0x2540be400",
        "lock": {
        "code_hash": "0x709f3fda12f561cfacf92273c57a98fede188a3f1a59b1f888d113f9cce08649",
        "hash_type": "data1",
        "args": "0xc8328aabcd9b9e8e64fbc566c4385c3bdeb219d7"
        },
        "type": null
    }
    ],
    "outputs_data": [
    "0x",
    "0x",
    "0x"
    ],
    "witnesses": [
    "0x550000001000000055000000550000004100000070b823564f7d1f814cc135ddd56fd8e8931b3a7040eaf1fb828adae29736a3cb0bc7f65021135b293d10a22da61fcc64f7cb660bf2c3276ad63630dad0b6099001"
    ]
}
```

</details>

The Transaction Hash is `0x9110ca9266f89938f09ae6f93cc914b2c856cc842440d56fda6d16ee62543f5c`.

## Acknowledgments

The authors would like to thank Jan Xie and Xuejie Xiao for their comments and insightful suggestions. The members of the CKB Dev team also helped by participating in the discussion and review. Boyu Yang is the primary author of the code changes, and his experiments and feedbacks are essential to complete this document.


================================================
File: rfcs/0033-ckb-vm-version-1/0033-ckb-vm-version-1.md
================================================
---
Number: "0033"
Category: Informational
Status: Draft
Author: Wanbiao Ye <mohanson@outlook.com>
Created: 2021-05-25
---

# VM version1

This RFC describes version 1 of the CKB-VM, in comparison to version 0, which

- Fixed several bugs
- Behavioural changes not affecting execution results
- New features
- Performance optimisations

## 1 Fixed Several Bugs

CKB-VM Version 1 has fixed identified bugs discovered in Version 0.

### 1.1 Enabling Stack Pointer SP To Be Always 16-byte Aligned

In the previous version, SP incorrectly aligned during stack initialisation. See [issue](https://github.com/nervosnetwork/ckb-vm/issues/97).

### 1.2 Added a NULL To Argv

C Standard 5.1.2.2.1/2 states: `argv[argc]` should be a null pointer. `NULL` was unfortunately omitted during the initialization of the stack, and now it has returned. See [issue](https://github.com/nervosnetwork/ckb-vm/issues/98).

### 1.3 JALR Caused Erroneous Behaviour on AsmMachine When rs1 and rd utilised the same register

The problem arose with the JALR instruction, where the CKB-VM had made an error in the sequence of its different steps. The correct step to follow would be to calculate the pc first and then update the rd. See [problem](https://github.com/nervosnetwork/ckb-vm/issues/92).

### 1.4 Error OutOfBound was triggered by reading the last byte of memory

We have fixed it, as described in the title.

### 1.5 Unaligned executable pages from loading binary would raise an error

We have fixed it, as described in the title.

### 1.6 Frozen writable pages by error

This error occurred during the loading of elf. The CKB-VM has incorrectly set a freeze flag on a writeable page, which made the page unmodifiable.

It happened mainly with external variables that have dynamic links.

### 1.7 Update crate goblib

goblin is a cross-platform trifecta of binary parsing and loading fun. ckb-vm uses it to load RISC-V programs. But in the past period of time goblin fixed many bugs and produced destructive upgrades, we decided to upgrade goblin: this will cause the binary that could not be loaded before can now be normal Load, or vice versa.

## 2 Behavioural Changes that will not affect the execution outcomes

### 2.1 Skip writing 0 to memory when argc equals 0 during stack initialisation

For ckb scripts, argc is always 0 and the memory is initialised to 0, so memory writing can be safely skipped. Note that when "chaos_mode" is enabled and "argv" is empty, the reading of "argc" will return an unexpected data. This happens uncommonly, and never happens on the mainnet.

### 2.2 Redesign of the internal instruction format

For the sake of fast decoding and cache convenience, RISC-V instruction is decoded into the 64-bit unsigned integer. Such a format used only internally in ckb-vm rather than the original RISC-V instruction format.

## 3 New features

### 3.1 B extension

We have added the RISC-V B extension (v1.0.0) [1]. This extension aims at covering the four major categories of bit manipulation: counting, extracting, inserting and swapping. For all B instructions, 1 cycle will be consumed.

### 3.2 Chaos memory mode

Chaos memory mode was added for the debugging tools. Under this mode, the program memory forcibly initializes randomly, helping us to discover uninitialized objects/values in the script.

### 3.3 Suspend/resume a running VM

It is possible to suspend a running CKB VM, save the state to a certain place and to resume the previously running VM later on, possibly even on a different machine.

## 4 Performance optimization

### 4.1 Lazy initialization memory

In version 0, when the VM was initialised, the program memory would be initialised to zero value. Now, we have deferred the initialisation of program memory. The program memory is divided into several different frames, so that only when a frame is used (read, write), the corresponding program memory area of that frame will be initialised with zero value. As a result , small programs that do not need to use large volumes of memory will be able to run faster.

### 4.2 MOP

Macro-Operation Fusion (also Macro-Op Fusion, MOP Fusion, or Macrofusion) is a hardware optimization technique found in many modern microarchitectures whereby a series of adjacent macro-operations are merged into a single macro-operation prior or during decoding. Those instructions are later decoded into fused-µOPs.

The cycle consumption of the merged instructions is the maximum cycle value of the two instructions before the merge. We have verified that the use of MOPs can lead to significant improvements in some encryption algorithms.

|            Opcode            |            Origin            |      Cycles       |
| ---------------------------- | ---------------------------- | ----------------- |
| ADC [2]                      | add + sltu + add + sltu + or | 1 + 0 + 0 + 0 + 0 |
| SBB                          | sub + sltu + sub + sltu + or | 1 + 0 + 0 + 0 + 0 |
| WIDE_MUL                     | mulh + mul                   | 5 + 0             |
| WIDE_MULU                    | mulhu + mul                  | 5 + 0             |
| WIDE_MULSU                   | mulhsu + mul                 | 5 + 0             |
| WIDE_DIV                     | div + rem                    | 32 + 0            |
| WIDE_DIVU                    | divu + remu                  | 32 + 0            |
| FAR_JUMP_REL                 | auipc + jalr                 | 0 + 3             |
| FAR_JUMP_ABS                 | lui + jalr                   | 0 + 3             |
| LD_SIGN_EXTENDED_32_CONSTANT | lui + addiw                  | 1 + 0             |

# Reference

* [1]: [B extension][1]
* [2]: [Macro-op-fusion: Pattern design of ADC and SBB][2]

[1]: https://github.com/riscv/riscv-bitmanip
[2]: https://github.com/nervosnetwork/ckb-vm/issues/169


================================================
File: rfcs/0034-vm-syscalls-2/0034-vm-syscalls-2.md
================================================
---
Number: "0034"
Category: Standards Track
Status: Proposal
Author: Wanbiao Ye <mohanson@outlook.com>
Created: 2021-05-25
---

# VM Syscalls 2

## Abstract

This document describes the addition of the syscalls during the ckb2021. These syscalls are only available since ckb-vm version 1 and ckb2021 [2].

- [VM Version]
- [Current Cycles]
- [Exec]

### VM Version
[vm version]: #vm-version

As shown above, *VM Version* syscall has a signature like following:

```c
int ckb_vm_version()
{
  return syscall(2041, 0, 0, 0, 0, 0, 0);
}
```

*VM version* syscall returns current running VM version, so far 2 values will be returned:

- Error for Lina CKB-VM version
- 1 for the new hardfork CKB-VM version.

This syscall consumes 500 cycles.

### Current Cycles
[current cycles]: #current-cycles

*Current Cycles* syscall has a signature like following:

```c
uint64_t ckb_current_cycles()
{
  return syscall(2042, 0, 0, 0, 0, 0, 0);
}
```

*Current Cycles* returns current cycle consumption just before executing this syscall. This syscall consumes 500 cycles.


### Exec
[exec]: #exec

Exec runs an executable file from specified cell data in the context of an already existing machine, replacing the previous executable. The used cycles does not change, but the code, registers and memory of the vm are replaced by those of the new program. It's cycles consumption consists of two parts:

- Fixed 500 cycles
- Initial Loading Cycles [1]

*Exec* syscall has a signature like following:

```c
int ckb_exec(size_t index, size_t source, size_t place, size_t bounds, int argc, char* argv[])
{
  return syscall(2043, index, source, place, bounds, argc, argv);
}
```

The arguments used here are:

* `index`: an index value denoting the index of entries to read.
* `source`: a flag denoting the source of cells or witnesses to locate, possible values include:
    + 1: input cells.
    + `0x0100000000000001`: input cells with the same running script as current script
    + 2: output cells.
    + `0x0100000000000002`: output cells with the same running script as current script
    + 3: dep cells.
* `place`: A value of 0 or 1:
    + 0: read from cell data
    + 1: read from witness
* `bounds`: high 32 bits means `offset`, low 32 bits means `length`. if `length` equals to zero, it read to end instead of reading 0 bytes.
* `argc`: argc contains the number of arguments passed to the program
* `argv`: argv is a one-dimensional array of strings


# Reference

* [1]: [Vm Cycle Limits][1]
* [2]: [CKB VM version selection][2]

[1]: ../0014-vm-cycle-limits/0014-vm-cycle-limits.md
[2]: ../0032-ckb-vm-version-selection/0032-ckb-vm-version-selection.md


================================================
File: rfcs/0035-ckb2021-p2p-protocol-upgrade/0035-ckb2021-p2p-protocol-upgrade.md
================================================
---
Number: "0035"
Category: Standards Track
Status: Proposal
Author: Chao Luo <@driftluo>, Ian Yang <@doitian>
Created: 2021-07-01
---
# P2P protocol upgrade

## Abstract

This RFC describes how the network protocol changes before and after the ckb hard fork, and how the network protocols smoothly upgrade along the hard fork.

## Motivation

The network protocol is the foundation of distributed applications. Before and after hard fork, there will be small changes in data format, but the network should not be disconnected or split because of this change. After hard fork, only clients that support hard fork are allowed to connect.

This RFC describes in detail how the ckb node implements this functionality.

## Specification

We divide the entire hard fork process into three phases: before hard fork, the moment that hard fork activates, and after hard fork. The protocols are divided into two categories which have different upgrade strategies.

- Upgrade the version of a specific protocol and ensure that both versions of the protocol are supported and can be enabled at the same time
- Mount two protocols that are functionally identical but require runtime switching for smooth upgrades

### For protocols whose functionality and implementation do not need to be modified

Including protocols:

- Identify
- Ping
- Feeler
- DisconnectMessage
- Time
- Alert

##### Before hard fork

Change the version support list from `[1]` to `[1, 2]`, the client will support both versions of the protocol, the new client will enable version 2 and the old client will enable version 1

##### Hard fork moment

Disconnect all clients with the protocol version 1 on, and reject this version afterwards.

##### After hard fork

Remove the support for the protocol version 1 from the next version of client code, i.e. change the support list from `[1, 2]` to `[2]`, and clean up the compatibility code

### Implement protocols that requires modification

#### Discovery

##### Before hard fork

1. Change the version support list from `[1]` to `[1, 2]`.
2. Remove redundant codec operations from the previous implementation

##### Hard fork moment

Disconnect all clients with the protocol version 1 on, and reject this version afterwards.

##### After hard fork

Remove the support for the protocol version 1 from the next version of client code, i.e. change the support list from `[1, 2]` to `[2]`, and clean up the compatibility code

#### Sync

##### Before hard fork Before

1. Change the version support list from `[1]` to `[1, 2]`
2. Remove the 16 group limit from the sync request list and keep the maximum number of syncs, new version changes the block sync request limit from 16 to 32

##### Hard fork moment

Disconnect all clients with the protocol version 1 on, and reject this version afterwards.

##### After hard fork

Remove the support for the protocol version 1 from the next version of client code, i.e. change the support list from `[1, 2]` to `[2]`, and clean up the compatibility code

### For protocols whose behavior will conflict before and after fork

#### Relay

##### Before hard fork.

Since relay protocols before and after fork may have inconsistent cycle of transaction validation due to inconsistent vm, such behavior cannot be identified by a simple upgrade, for such protocols, another solution will be adopted to smooth the transition, i.e., open both relay protocols, disable the new protocol relay tx related messages, and let the old protocol work normally

##### Hard fork moment

1. Disable relay tx related messages in version 1 protocol and switch to the new relay
2. Allow opening the version 1 protocols

##### After hard fork

Remove the support for the old relay protocol in the next version of the client code, i.e. remove the support for the old relay protocol and clean up the compatibility code


================================================
File: rfcs/0035-ckb2021-p2p-protocol-upgrade/0035-ckb2021-p2p-protocol-upgrade.zh-CN.md
================================================
---
Number: "0035"
Category: Standards Track
Status: Proposal
Author: Chao Luo <@driftluo>, Ian Yang <@doitian>
Created: 2021-07-01
---

# P2P 协议升级

## Abstract

这个 RFC 用于描述网络协议在 ckb hard fork 前后的变化，以及如何在 hard fork 过程中让网络协议平稳过度。

## Motivation

网络协议是分布式应用的基础，hard fork 前后，数据格式将会有小范围的变化，但网络不应该因为这个变化而导致断开或者分裂，我们应当尽可能平稳地过度这一特殊时期，同时要保证在 hard fork 之前，所有客户端可以连接，hard fork 之后，只允许连接支持 hard fork 的客户端。

这个 RFC 详细描述了 ckb 节点如何实现上述功能。

## Specification

我们将整个 hard fork 的过程分为三个阶段：hard fork 之前，hard fork 时点，hard fork 之后。然后分为两大类手段来描述具体的改动细节，以及如何支持平稳过度。

ckb 对网络协议有两种升级和扩展的方式：

- 对特定协议升级版本，并保证同时支持两个版本协议可同时开启
- 挂载两个功能一样但需要运行时切换的协议用于平滑升级

### 对于功能和实现都不需要修改的协议

包含协议：

- Identify
- Ping
- Feeler
- DisconnectMessage
- Time
- Alert

##### hard fork 之前

将版本支持列表从 `[1]` 修改为 `[1, 2]`，该客户端将同时支持两个版本的协议，新客户端将开启 2 版本，老客户端将开启 1 版本

##### hard fork 时点

将开启 1 版本协议的客户端全部断开连接，同时在之后拒绝此版本协议开启

##### hard fork 之后

在下一个版本客户端代码中移除 1 版本协议的支持，即支持列表从 `[1, 2]` 修改为 `[2]`，并清理兼容代码

### 对实现需要微调的协议

#### Discovery

##### hard fork 之前：

1. 将版本支持列表从 `[1]` 修改为 `[1, 2]`
2. 移除之前实现时多余的编码解码操作

##### hard fork 时点

将开启 1 版本协议的客户端全部断开连接，同时在之后拒绝此版本协议开启

##### hard fork 之后

在下一个版本客户端代码中移除 1 版本协议的支持，即支持列表从 `[1, 2]` 修改为 `[2]`，并清理兼容代码

#### Sync

##### hard fork 之前：

1. 将版本支持列表从 `[1]` 修改为 `[1, 2]`
2. 移除同步时请求列表的 16 一组限制，保留最大同步数的限制，新版本将 block 同步请求上限从 16 改为 32

##### hard fork 时点

将开启 1 版本协议的客户端全部断开连接，同时在之后拒绝此版本协议开启

##### hard fork 之后

在下一个版本客户端代码中移除 1 版本协议的支持，即支持列表从 `[1, 2]` 修改为 `[2]`，并清理兼容代码

### 对行为在 fork 前后会发生冲突的协议

#### Relay

##### hard fork 之前：

由于 relay 协议在 fork 前后可能会因为 vm 不一致而导致交易验证的 cycle 不一致，这样的行为无法通过简单的升级来标识，对于这样的协议，将采取另一种方案进行平滑过度，即打开两个 relay 协议，禁用新协议 relay tx 相关消息，让老协议正常工作

##### hard fork 时点

1. 禁用 1 版本协议中的 relay tx 相关消息，切换为新 relay 工作
2. 允许打开 1 版本协议

##### hard fork 之后

在下一个版本客户端代码中移除老版本 relay 协议的支持，即删除老版本 relay 协议的支持，并清理兼容代码


================================================
File: rfcs/0036-remove-header-deps-immature-rule/0036-remove-header-deps-immature-rule.md
================================================
---
Number: "0036"
Category: Standards Track
Status: Proposal
Author: Ian Yang <@doitian>
Created: 2021-02-07
---

# Remove Header Deps Immature Rule

## Abstract

This document proposes removing the *[Loading Header Immature Rule]*.

[Loading Header Immature Rule]: ../0009-vm-syscalls/0009-vm-syscalls.md#loading-header-immature-error

In the consensus ckb2019, the header dep must reference the block which is 4 epochs ago. After this RFC is activated, the transaction can use any existing blocks in the chain as the header dep.

## Motivation

Header dep is a useful feature for dApps developers because the script can read the block's header in the chain or verify that an input cell or dep cell is in a specific block in the chain.

The *Loading Header Immature Rule* prevents the usage of header deps in many scenarios because the script must reference the block about 16 hours ago.

The intention of the immature rule is like the cellbase immature rule. A transaction and all its descendants may be invalidated after a chain reorganization [^1], because its header deps referred to stale or orphan blocks. Removing the rule lets dApps developers trade-off between responsive header reference and reliable transaction finality.

[^1]: Chain reorganization happens when the node found a better chain with more accumulated proved work and it has to rollback blocks to switch to the new chain.

## Specification

This RFC must be activated via a hard fork. After activation, the consensus no longer verifies that the referenced block in the header deps is mined 4 epochs ago.

The transaction producers can choose to postpone the transaction submission when it has a header dep that has been mined recently. It suggests waiting for at least 4 epochs, but the app can choose the best value in its scenario, like the transaction confirmation period.


================================================
File: rfcs/0037-ckb2021/0037-ckb2021.md
================================================
---
Number: "0037"
Category: Informational
Status: Draft
Author: Ian Yang <@doitian>
Created: 2021-07-24
---

# CKB Consensus Change (Edition CKB2021)

The current edition of CKB consensus rules is CKB2019. CKB2021 refers to the new edition of CKB consensus rules after its first hardfork. The purpose of a hard fork is to upgrade and update the rules encoded in the network. The changes are not backward compatible. This document outlines the changes in this upgrade.

## What's in CKB2021

CKB2021 includes both new features and bug fixes. All changes are proposed via RFCs. The appendix has a list of all the RFCs related to CKB2021.

The upgrade is divided into three categories. 

First, CKB VM gets a major upgrade. CKB2021 will bundle CKB VM v1, in addition to the v0 in CKB2019. Scripts will be executed on v1 unless users opt in to use v0 by setting the script hash type to `data`.  

Second, CKB2021 adds a new field `extension` in the block. This is reserved for future upgrades such as flyclient.

Lastly, there are a bunch of consensus patches to fix bugs and make the consensus rules more robust.

### CKB VM v1

Since CKB2021, there will be multiple VM versions available. ﻿[RFC32] introduces a CKB VM version mechanism. It piggybacks on the `hash_type` field in the Script structure.

| `hash_type` | JSON representation | matches by | VM version |
| ----------- | ---------- | ---------------- | ---------- |
| 0           | "data"     | data hash        | 0          |
| 1           | "type"     | type script hash | 1          |
| 2           | "data1"    | data hash        | 1          |

[RFC33] introduces what's new in CKB VM v1 and [RFC34] adds new syscalls for VM v1.

The new VM version adds new features and performance optimizations. It has fixed identified bugs discovered in v0.

CKB VM v1 supports [RISC-V B extension](https://github.com/riscv/riscv-bitmanip) and [macro-op fusion](https://en.wikichip.org/wiki/macro-operation_fusion). One major rationale behind the changes in CKB-VM is about reducing overheads. RISC-V B extension allows developers to map RISC-V instructions directly with native instructions provided by x86-64 CPUs, while macro-op fusion goes even deeper to exploit modern micro-architectures in CPUs. All those efforts make crypto algorithms more efficiently on CKB-VM, unlocking more potential use cases of Nervos CKB. For example, the BLS signature verification lock consumes too many cycles on CKB now. With the help of B extension, together with macro-op, it's possible to bring the cycles consumption down to a feasible rate.

Given the same transaction, different VM versions may consume different cycles, even give different verification results. [RFC35] proposes to use separate transaction relay protocols for each VM version to help the smooth transition of the CKB2021 activation.

### Extension Field

[RFC31] proposes adding an optional variable length field to the block.

Many extensions require adding new fields into the block. For example, PoA for testnet requires 65 bytes for each signature, and flyclient needs to add a 64 bytes hash. But there's not enough reserved bits in the header for these extensions. The RFC proposes a solution to add a variable length field in the block. 

Although the field is added to the block body, nodes can synchronize the block header and this field together without overhead.

CKB2021 will not parse and verify the field after the activation. Instead, it enables a future soft fork to give the definition of the extension field. For example, flyclient can store the hash in the extension field.

### Consensus Patches

[RFC28] uses block timestamp as the start time for the relative timestamp `since` field, instead of the median of previous 37 blocks. This simplifies the `since` maturity calculation.

[RFC29] allows multiple matches on dep cells via type script hash when these cells have the same data. It removes unnecessary restrictions when there's no ambiguity to choose matched script code.

[RFC30] ensures that the index is less than the length in the `since` field using epoch as the time measure. It avoids the ambiguity because of the inconsistent behavior when using relative and absolute epoch `since`.

[RFC36] removes header deps immature rule, allowing developers to choose how long to wait until a header can be used as a dep header.

## CKB2021 Timeline

The mainnet upgrade is divided into three phases.

* **Stage 1 - Code Preview**: An RC version of 0.100.0 is ready for preview on July 16 2021 via nervosnetwork/ckb [releases](https://github.com/nervosnetwork/ckb/releases). It will introduce the incompatible changes to help developers to adapt their tools and apps to CKB2021. But this version does not activate the consensus incompatible changes in CKB2021. Developers can test the new rules by running a dev chain locally.

* **Stage 2 - Testnet Activation**: With the release of CKB 0.101.0, CKB2021 is set to activate on Aggron testnet on October 24th, 2021. Pudge is the successor guardian of the testnet after activation. Thank you Aggron, Ogre Magi! Look who's coming for dinner, Pudge!

* **Stage 3 - Mainnet Activation**: With the release of CKB 0.103.0, CKB2021 will be set to activate on Lina mainnet. The exact mainnet activation time will be determined after Stage 2 passed successfully. Mirana will be the successor guardian of CKB mainnet after activation. Thank you Lina, our flame burns brighter. The moon lights our way, Mirana!

## Upgrade Strategies

First, the SDK, Tool, and dApps authors must adapt to any 0.100.0 rc version.

There are two strategies for ecosystem developers to upgrade to the CKB2021 consensus. Choose the former one if the developers can pause the app during the fork activation, otherwise, use the latter one.

- Release two different versions or use the feature switcher. Manually deploy the newer version or enable the feature CKB2021 after the fork activation.
- Use feature switcher and enable the feature CKB2021 automatically when the chain grows into the activation epoch. The activation epoch is different in the testnet and the mainnet, which is available via the updated `get_consensus` RPC.

## Appendix

### CKB2021 RFCs List

* [RFC28]: Use Block Timestamp as Start Timestamp in Since.
* [RFC29]: Allow multiple matches on dep cells via type script hash when these cells have the same data.
* [RFC30]: Ensure that index is less than length in input since field using epoch.
* [RFC31]: Add a variable length field in the block header.
* [RFC32]: CKB VM version selection.
* [RFC33]: CKB VM version1 changes.
* [RFC34]: CKB VM syscalls bundle 2.
* [RFC35]: P2P protocol upgrade.
* [RFC36]: Remove header deps immature rule.
* RFC37: This RFC, CKB2021 overview.

[RFC28]: ../0028-change-since-relative-timestamp/0028-change-since-relative-timestamp.md
[RFC29]: ../0029-allow-script-multiple-matches-on-identical-code/0029-allow-script-multiple-matches-on-identical-code.md
[RFC30]: ../0030-ensure-index-less-than-length-in-since/0030-ensure-index-less-than-length-in-since.md
[RFC31]: ../0031-variable-length-header-field/0031-variable-length-header-field.md
[RFC32]: ../0032-ckb-vm-version-selection/0032-ckb-vm-version-selection.md
[RFC33]: ../0033-ckb-vm-version-1/0033-ckb-vm-version-1.md
[RFC34]: ../0034-vm-syscalls-2/0034-vm-syscalls-2.md
[RFC35]: ../0035-ckb2021-p2p-protocol-upgrade/0035-ckb2021-p2p-protocol-upgrade.md
[RFC36]: ../0036-remove-header-deps-immature-rule/0036-remove-header-deps-immature-rule.md


================================================
File: rfcs/0039-cheque/0039-cheque.md
================================================
---
Number: "0039"
Category: Standards Track
Status: Proposal
Author: Dylan Duan <duanyytop@gmail.com>
Created: 2022-01-27
---

# Cheque Lock

## Abstract

This RFC describes a lock script that can be used to transfer assets, such as SUDT ([Simple UDT](../0025-simple-udt/0025-simple-udt.md)) and mNFT ([Multi-purpose NFT](https://talk.nervos.org/t/rfc-multi-purpose-nft-draft-spec/5434)), from one user to another user that does not have an ACP ([Anyone-Can-Pay](../0026-anyone-can-pay/0026-anyone-can-pay.md)) cell available, and without the sender having to provide CKBytes to create the destination cell for the receiver. 

## Summary

The most basic method of transferring an asset from one user to another is to have the sender provide the CKBytes to create a destination cell for the receiver that holds the asset. However, when using this method the required CKBytes must be sent with the asset, and this can become a significant additional cost to the sender.

When ACP is used to transfer an asset, the sender no longer has to provide CKBytes for the receiver. However, the receiver must first create an ACP cell to hold the asset that will be sent to them in the future. This requires that the receiver knows in advance what is being sent. There is no automatic method to do this, which makes it a significant UX burden.

The Cheque Lock attempts to solve this problem by allowing the sender to lock an asset they want to transfer into a Cheque cell (a cell uses the Cheque Lock) with the receiver's address indicated. Only the receiver can claim the asset, and to do so they must provide the CKBytes required to create a destination cell that they own to move the asset into.

The sender must provide the CKBytes to create the Cheque cell that holds the asset while waiting to be claimed by the receiver. When the receiver claims their asset, the CKBytes from the Cheque cell are returned to the sender. This process allows a sender to transfer an asset without sending CKBytes, and without requiring the receiver to first create an ACP cell to receive the asset. 

When the asset is put into the Cheque cell, it is locked for a period of 6 epochs which is approximately 24 hours. During this period, the receiver can freely claim at their convenience. If the asset is not claimed after 6 epochs have passed, the sender has the option to cancel the process and withdraw the asset.

## Specification

The Cheque Lock is designed to work with the [secp256k1-blake2b-sighash-all](https://github.com/nervosnetwork/ckb-system-scripts/wiki/How-to-sign-transaction#p2pkh) lock, also known as the default lock. This is the only fully supported lock that is used for both the sender and receiver. Attempting to use other locks with the Cheque Lock is not recommended.

The Cheque Lock will work with many types of custom assets, but those which require non-standard functionality may not be compatible. The Cheque Lock should only be used with asset types that are known to be compatible to reduce the risk of lost assets or funds. We will use SUDT tokens for all examples below since they are known to be fully compatible.

### Cheque Lock Script Structure

A Cheque cell is a cell that uses the Cheque Lock as the lock script and has a structure that is similar to the following:

```
lock:
    code_hash: <cheque_lock_script>
    args: <20-byte_receiver_lock_hash> <20-byte_sender_lock_hash>
type:
    <simple_udt_type_script>
data:
    <sudt_amount: uint128>
```

The `20-byte_receiver_lock_hash` and the `20-byte_sender_lock_hash` are `Blake2b160` hashes of lock scripts which rely on the default lock ([secp256k1-blake2b-sighash-all](https://github.com/nervosnetwork/ckb-system-scripts/wiki/How-to-sign-transaction#p2pkh)). Specifying the sender lock hash and receiver lock hash in the lock args defines who can access the assets that are locked in the Cheque cell.

A `Blake2b160` hash is calculated as follows. First, a fully populated lock script structure for the sender or receiver must be converted to its binary representation. Next, a `Blake2b256` hash of the binary representation is generated, which results in a 32-byte (256-bit) hash. Finally, this hash is truncated to the first 20 bytes, which is 160 bits.

The sender initiates the process by creating a Cheque Lock cell that contains the asset they wish to deposit. At this time they specify both the sender and receiver lock hashes as lock args on the Cheque cell. To create the Cheque cell, the sender must also provide the CKBytes necessary for the cell. The amount required depends on the requirements of the asset being transferred.

The receiver can claim the asset at any time after the Cheque cell has been created, as long as the asset has not been withdrawn by the sender. After the Cheque cell has been created, the sender cannot withdraw for a period of 6 epochs. This gives a guaranteed window of approximately 24 hours where only the receiver can claim the asset. If the receiver does not claim the asset within 6 epochs, then the sender has the option to cancel the process and withdraw the asset and CKBytes that they provided.

## Unlock Rules

The Cheque Lock follows the rules below when validating a transaction.

1. If a signature is provided in the witness:

    - 1.a. Check the signature against the sender lock hash and receiver lock hash. If the signature does not match either, then return with an error.

    - 1.b. If the provided signature is valid and matches the `20-byte_receiver_lock_hash`:

        - 1.b.i. Loop through all the input cells using the current Cheque Lock script). If any of these inputs has a [since](../0017-tx-valid-since/0017-tx-valid-since.md) value that is not zero, return with an error. This ensures that the receiver is always able to claim immediately without a time delay restriction.

        - 1.b.ii. Loop through all the output cells with the sender's lock hash. If the sum of the capacity in these cells is not equal to the sum of the capacity in the input Cheque cells, return with an error. This ensures that the CKBytes provided by the sender for the Cheque cell are always returned to the sender when the asset is claimed.

    - 1.c. If the provided signature is valid and matches the `20-byte_sender_lock_hash`:

        - 1.c.i. Loop through all the input cells using the current Cheque Lock script. If any of these inputs has a [since](../0017-tx-valid-since/0017-tx-valid-since.md) value that is not set to `0xA000000000000006`, return with an error. A `since` value of `0xA000000000000006` indicates that the cell cannot be committed in a transaction until a minimum of 6 epochs have passed since the Cheque cell was created. This ensures a window of approximately 24 hours where only the receiver can claim the asset.

2. If a signature is not provided in the witness:

   - 2.a. Loop through all the input cells checking their lock script hash against the `20-byte_receiver_lock_hash` and `20-byte_sender_lock_hash`. If no matches are found, return with an error.

   - 2.b. If an input cell is found that matches the `20-byte_receiver_lock_hash`:

     - 2.b.i. Perform the same checks as in rules 1.b.i and 1.b.ii.

     - 2.b.ii. Loop through all the input cells and locate the first input cell with a lock script hash that matches the `20-byte_receiver_lock_hash`, and note the index of the matched cell. Then locate the corresponding witness at the same index that was noted. If the witness at this index is empty, is not a [WitnessArgs](https://github.com/nervosnetwork/ckb/blob/a6733e6af5bb0da7e34fb99ddf98b03054fa9d4a/util/types/schemas/blockchain.mol#L104-L108) structure, or the WitnessArgs structure has an empty lock property, return with an error. This helps ensure proper lock usage and transaction structure.

   - 2.c. If an input cell is found that matches the `20-byte_sender_lock_hash`:

     - 2.c.i. Perform the same checks as in rules 1.c.i.

> Note: The Cheque Lock allows for batching, meaning that a single transaction can contain multiple Cheque cells for different claims and withdrawals which will all be processed at the same time. When two or more Cheque cells have identical scripts (the exact same code_hash, hash_type, and args), they will execute in the same lock script and process together in a single script execution group. If there is any difference in the scripts, such as the same code_hash, hash_type, but a different sender or receiver is provided in the args, then they will execute in separate script execution groups. 

## Examples

Below are example transactions for several operations of the Cheque Lock.

In these examples, a 0.01 CKByte transaction fee is used for simplicity. In a production environment, transaction fees should be [calculated](https://docs.nervos.org/docs/essays/faq/#how-do-you-calculate-transaction-fee) based on factors including transaction size, running cycles as well as network status.

### Create a Cheque Cell

```
inputs:
    sudt_cell:
        capacity: 1000 CKBytes
        lock: <sender_secp256k1_blake2b_lock_script>
        type: <sudt_type_script>
        data:
            sudt_amount: 1000 UDT

outputs:
    cheque_cell:
        capacity: 165 CKBytes
        lock:
            code_hash: <cheque_lock_script>
            args:  <20-byte_receiver_lock_hash> <20-byte_sender_lock_hash>
        type: <sudt_type_script>
        data:
            sudt_amount: 100 UDT
    sudt_cell:
        capacity: 834.99 CKBytes
        lock: <sender_secp256k1_blake2b_lock_script>
        type: <sudt_type_script>
        data:
            sudt_amount: 900 UDT

witnesses:
    <valid_signature_for_sender_secp256k1_blake2b_lock_script>
```

This transaction creates a Cheque cell, locking 100 UDT tokens that can be claimed by the receiver.

The `20-byte_sender_lock_hash` is a match to `sender_secp256k1_blake2b_lock_script`. This will allow the sender to withdraw the asset from the Cheque cell after 6 epochs if the receiver does not claim the asset.

### Claim

#### 1. Claim via Receiver Signature

```
inputs:
    cheque_cell:
        capacity: 165 CKBytes
        lock:
            code_hash: <cheque_lock_script>
            args: <20-byte_receiver_lock_hash> <20-byte_sender_lock_hash>
        type: <sudt_type_script>
        data:
            sudt_amount: 100 UDT
    sudt_cell:
        capacity: 200 CKBytes
        lock: <another_receiver_lock_script>
        type: <sudt_type_script>
        data:
            sudt_amount: 200 UDT

outputs:
    sudt_cell:
        capacity: 199.99 CKBytes
        lock: <another_receiver_lock_script>
        type: <sudt_type_script>
        data:
            sudt_amount: 300 UDT
    basic_cell:
        capacity: 165 CKBytes
        lock: <sender_secp256k1_blake2b_lock_script>

witnesses:
    <valid_signature_for_receiver_secp256k1_blake2b_lock_script>
    <valid_signature_for_another_receiver_lock_script>

```

This transaction claims the 100 UDT tokens in the Cheque cell using the receiver's signature, and sends them to a different address.

The receiver provides his signature (`witnesses[0]`) to unlock the Cheque cell (`inputs[0]`). The signature in witnesses[0] is a valid match to the `20-byte_receiver_lock_hash`. The receiver also provides an SUDT cell (`inputs[1]`) that contains the same SUDT tokens in the Cheque cell (`inputs[0]`), and extra capacity which will be used to cover the transaction fee. The SUDT cell uses a different lock script, `another_receiver_lock_script`, which means it must be unlocked using a different signature (`witnesses[1]`). The output SUDT cell (`outputs[0]`) receives the SUDT tokens from the Cheque cell (`inputs[0]`), and pays the 0.01 CKByte transaction fee. The 165 CKBytes of capacity from the Cheque cell (`inputs[0]`) are returned to the sender in a basic cell (`output[1]`).

#### 2. Claim via Receiver Lock Script

```
inputs:
    cheque_cell:
        capacity: 165 CKBytes
        lock:
            code_hash: <cheque_lock_script>
            args: <20-byte_receiver_lock_hash> <20-byte_sender_lock_hash>
        type: <sudt_type_script>
        data:
            sudt_amount: 100 UDT
    sudt_cell:
        capacity: 200 CKBytes
        lock: <receiver_secp256k1_blake2b_lock_script>
        type: <sudt_type_script>
        data:
            sudt_amount: 200 UDT

outputs:
    sudt_cell:
        capacity: 199.99 CKBytes
        lock: <receiver_secp256k1_blake2b_lock_script>
        type: <sudt_type_script>
        data:
            sudt_amount: 300 UDT
    basic_cell:
        capacity: 165 CKBytes
        lock: <sender_secp256k1_blake2b_lock_script>

witnesses:
    <0x>
    <valid_signature_for_receiver_secp256k1_blake2b_lock_script>

```

This transaction claims the 100 UDT tokens in the Cheque cell (`inputs[0]`) in a very similar way to the previous example, except that the receiver's lock script is used to unlock the Cheque cell instead of a separate signature.

When using the receiver's lock script to claim, no signature needs to be provided for the Cheque cell. Notice that `witnesses[0]` is empty because the Cheque cell (`inputs[0]`) does not require it. The input SUDT cell (`inputs[1]`) has a lock script `receiver_secp256k1_blake2b_lock_script` that matches the Cheque cell receiver lock hash `20-byte_receiver_lock_hash`. The signature in `witnesses[1]` is valid and unlocks the SUDT cell (`inputs[1]`). The Cheque cell receiver `20-byte_receiver_lock_hash` matches the lock on the SUDT cell `receiver_secp256k1_blake2b_lock_script`. The Cheque cell (`inputs[0]`) will unlock without a signature because the receiver's lock script is present in another input cell (`inputs[1]`), and it is unlocked with a signature provided in `witnesses[1]`.

### Withdraw

#### 1. Withdraw via Sender Signature

```
inputs:
    cheque_cell:
        capacity: 165 CKBytes
        lock:
            code_hash: <cheque_lock_script>
            args: <20-byte_receiver_lock_hash> <20-byte_sender_lock_hash>
        type: <sudt_type_script>
        data:
            sudt_amount: 100 UDT
        since: 0xA000000000000006

outputs:
    sudt_cell:
        capacity: 164.99 CKBytes
        lock: <sender_secp256k1_blake2b_lock_script>
        type: <sudt_type_script>
        data:
            sudt_amount: 100 UDT

witnesses:
    <valid_signature_for_sender_secp256k1_blake2b_lock_script>
```

This transaction withdrawals 100 UDT tokens from the Cheque cell (`inputs[0]`) using the sender's signature, and returns the tokens and CKBytes to the sender.

The signature in `witnesses[0]` is a valid match for the lock script indicated by `20-byte_sender_lock_hash` in the Cheque cell (`inputs[0]`). This authorizes a withdrawal by the sender if 6 epochs have passed and the receiver has not claimed the asset. Notice that a since value of `0xA000000000000006` is present on the Cheque cell. This prevents the transaction from being committed until 6 epochs after the Cheque cell was created.

The Cheque cell (`inputs[0]`) has 165 CKBytes of capacity, which is enough for the output cell plus some extra capacity that is used to pay the 0.01 CKByte transaction fee. For that reason, no extra capacity needs to be provided by the sender to complete this transaction. The 100 UDT tokens and remaining CKBytes are returned to the sender in `outputs[0]`.

#### 2. Withdraw via Sender Lock Script

```
inputs:
    cheque_cell:
        capacity: 165 CKBytes
        lock:
            code_hash: <cheque_lock_script>
            args: <20-byte_receiver_lock_hash> <20-byte_sender_lock_hash>
        type: <sudt_type_script>
        data:
            sudt_amount: 100 UDT
        since: 0xA000000000000006
    basic_cell:
        capacity: 200 CKBytes
        lock: <sender_secp256k1_blake2b_lock_script>

outputs:
    sudt_cell:
        capacity: 165 CKBytes
        lock: <sender_secp256k1_blake2b_lock_script>
        type: <sudt_type_script>
        data:
            sudt_amount: 100 UDT
    basic_cell:
        capacity: 199.99 CKBytes
        lock: <sender_secp256k1_blake2b_lock_script>

witnesses:
    <0x>
    <valid_signature_for_sender_secp256k1_blake2b_lock_script>
```

This transaction withdraws the 100 UDT tokens in the Cheque cell (`inputs[0]`) in a very similar way to the previous example, except that the sender's lock script is used to unlock the Cheque cell instead of a separate signature.

When using the sender's lock script to claim, no signature needs to be provided for the Cheque cell. Notice that `witnesses[0]` is empty because the Cheque cell (`inputs[0]`) does not require it. The input basic cell (`inputs[1]`) has a lock script `sender_secp256k1_blake2b_lock_script` that matches the Cheque cell sender lock hash `20-byte_sender_lock_hash`. The signature in `witnesses[1]` is valid and unlocks the basic cell (`inputs[1]`). The Cheque cell sender `20-byte_sender_lock_hash` matches the lock on the SUDT cell `sender_secp256k1_blake2b_lock_script`. The Cheque cell (`inputs[0]`) will unlock without a signature because the sender's lock script is present in another input cell (`inputs[1]`), and it is unlocked with a signature provided in `witnesses[1]`.

Notice that a since value of `0xA000000000000006` is present on the Cheque cell. This prevents the transaction from being committed until 6 epochs after the Cheque cell was created.

## Deployments

The Cheque Lock script executable has been deployed to the Nervos CKB L1 Mainnet and Testnet and can be accessed using the parameters provided below.

### Lina / Mirana (Mainnet)

| parameter   | value                                                                |
| ----------- | -------------------------------------------------------------------- |
| `code_hash` | `0xe4d4ecc6e5f9a059bf2f7a82cca292083aebc0c421566a52484fe2ec51a9fb0c` |
| `hash_type` | `type`                                                               |
| `tx_hash`   | `0x04632cc459459cf5c9d384b43dee3e36f542a464bdd4127be7d6618ac6f8d268` |
| `index`     | `0x0`                                                                |
| `dep_type`  | `dep_group`                                                          |

> Note: A `dep_type` of `dep_group` means that the contents of this dep cell contains references to multiple cell deps. These are `secp256k1_data` and `cheque_lock`, both of which have a `dep_type` of `code`.

The `out_point` of `secp256k1_data` is:

```
{
  tx_hash: 0xe2fb199810d49a4d8beec56718ba2593b665db9d52299a0f9e6e75416d73ff5c,
  index: 0x3
}
```

The `out_point` of `cheque_lock` is:

```
{
  tx_hash: 0x0a34aeea122d9795e06e185746a92e88bca0ad41b0e5842a960e5fd1d43760a6,
  index: 0x0
}
```

### Aggron / Pudge (Testnet)

| parameter   | value                                                                |
| ----------- | -------------------------------------------------------------------- |
| `code_hash` | `0x60d5f39efce409c587cb9ea359cefdead650ca128f0bd9cb3855348f98c70d5b` |
| `hash_type` | `type`                                                               |
| `tx_hash`   | `0x7f96858be0a9d584b4a9ea190e0420835156a6010a5fde15ffcdc9d9c721ccab` |
| `index`     | `0x0`                                                                |
| `dep_type`  | `dep_group`                                                          |

> Note: A `dep_type` of `dep_group` means that the contents of this dep cell contains references to multiple cell deps. These are `secp256k1_data` and `cheque_lock`, both of which have a `dep_type` of `code`.

The `out_point` of `secp256k1_data` is

```
{
  tx_hash: 0x8f8c79eb6671709633fe6a46de93c0fedc9c1b8a6527a18d3983879542635c9f,
  index: 0x3
}
```

The `out_point` of `cheque_lock` is:

```
{
  tx_hash: 0x1b16769dc508c8349803fe65558f49aa8cf04ca495fbead42513e69e46608b6c,
  index: 0x0
}
```

## Reproducible Build

A reproducible build can be used to verify that the deployed scripts are consistent with the source code. Please refer to the [CI configure file](https://github.com/nervosnetwork/ckb-Cheque-script/blob/main/.github/workflows/build_and_test.yml) for the exact software versions and steps needed to build the binaries used for verification purposes.

<!--

To build the deployed Cheque Lock script, one can use the following steps:

```bash
$ git clone https://github.com/nervosnetwork/ckb-Cheque-script
$ cd ckb-Cheque-script
$ git checkout 4ca3e62ae39c32cfcc061905515a2856cad03fd8
$ git submodule update --init
$ cd contracts/ckb-Cheque-script/ckb-lib-secp256k1/ckb-production-scripts
$ git submodule update --init
$ cd .. && make all-via-docker
$ cd ../../.. && capsule build --release
```

-->

## Discussion

A draft of this specification was previously released, reviewed, and discussed in the community on the [Nervos Talk forums](https://talk.nervos.org/t/sudt-Cheque-deposit-design-and-implementation/5209).

## References

[1] SUDT Cheque Deposit Design and Implementation, https://talk.nervos.org/t/sudt-Cheque-deposit-design-and-implementation/5209

[2] Cheque Script Source Code, https://github.com/nervosnetwork/ckb-Cheque-script/tree/4ca3e62ae39c32cfcc061905515a2856cad03fd8


================================================
File: rfcs/0042-omnilock/0042-omnilock.md
================================================
---
Number: "0042"
Category: Standards Track
Status: Proposal
Author: Xu Jiandong <lynndon@gmail.com>
Created: 2022-05-19
---

# Omnilock

Omnilock is a lock script designed for interoperability. It comes with built-in support for verification of transaction
signing methods used in Bitcoin, Ethereum, EOS, and Dogecoin. Omnilock is also extensible, so more verification
algorithms can be added in future.

Another feature of Omnilock for practitioners is the regulation compliance module which brings interoperability with
the traditional world. If enabled, the specified administrator can revoke tokens held by users under circumstances
which the administrator deems proper. This part has evolved from the [Regulation Compliance Extension
(RCE)](https://talk.nervos.org/t/rfc-regulation-compliance-extension/5338) proposal for
[xUDT](https://talk.nervos.org/t/rfc-extensible-udt/5337). This feature provides an option that sits at the other side
of the asset lock spectrum and lays the foundation of registered assets like Apple stock on CKB. When used together,
Omnilock and RCE provide an [ERC-1404](https://erc1404.org/) equivalence.


## Omnilock Script

### Lock Script

An Omnilock script has the following structure:
```text
Code hash: Omnilock script code hash
Hash type: Omnilock script hash type
Args: <21 byte auth> <Omnilock args>
```


There are 2 key fields in `args`: `Omnilock args` and `auth`. The `Omnilock args` is to control extra checking. It
allows different modes to be enabled in the same `Omnilock args`. The `auth` is used for authentication. It is generally
with pubkey hash in its content. 

The `Omnilock args` can be without mode (with `Omnilock flags` = 0) while the `auth` must be present. The functionality
of Omnilock script without mode is almost the same as traditional
[SECP256K1/blake160](https://github.com/nervosnetwork/rfcs/blob/master/rfcs/0024-ckb-genesis-script-list/0024-ckb-genesis-script-list.md#secp256k1blake160)
lock script. The Omnilock script can be considered as a traditional lock script with additional checking/modes.
Different modes can be enabled in different scenarios, depending on requirements.


### Omnilock args

The structure of `Omnilock args` is as follows:

```
<1 byte Omnilock flags> <32 byte AdminList cell Type ID, optional> <2 bytes minimum ckb/udt in ACP, optional> <8 bytes since for time lock, optional> <32 bytes type script hash for supply, optional>
```

| Name               | Flags      | Affected Args   |Affected Args Size (byte)|Affected Witness
| -------------------|------------|-----------------|---------|-------------------------------- 
| administrator mode | 0b00000001 |	AdminList cell Type ID | 32      | omni_identity/signature in OmniLockWitnessLock
| anyone-can-pay mode| 0b00000010 | minimum ckb/udt in ACP| 2 | N/A
| time-lock mode     | 0b00000100 | since for timelock| 8     | N/A
| supply mode        | 0b00001000 |type script hash for supply| 32 | N/A

All the modes will be described later.

### Authentication

An authentication (auth) is a 21-byte data structure containing the following components:

```
<1 byte flag> <20 bytes auth content>
```

Depending on the value of the flag, the auth content has the following interpretations:

* 0x0: The auth content represents the blake160 hash of a secp256k1 public key. The lock script will perform secp256k1
  signature verification, the same as the [SECP256K1/blake160
  lock](https://github.com/nervosnetwork/rfcs/blob/780b2f98068ed2337f3a97b02ec6b5336b6fb143/rfcs/0024-ckb-genesis-script-list/0024-ckb-genesis-script-list.md#secp256k1blake160).

* 0x01: It follows the unlocking method used by Ethereum. The signing message
  hash(sighash_all, see [reference implementation](https://github.com/nervosnetwork/ckb-system-scripts/blob/a7b7c75662ed950c9bd024e15f83ce702a54996e/c/secp256k1_blake160_sighash_all.c#L219)) 
  is converted as following:
  ```
  "0x" + hex(signing message hash)
  ```
  The hex operator is to convert binary into hex string. 

* 0x03: It follows the unlocking method used by Tron. The signing message hash
  is converted as following:
  ```
  "0x" + hex(signing message hash)
  ```

* 0x04: It follows the unlocking method used by Bitcoin. The signing message
  hash is required to be converted as following:
    ```
    "CKB (Bitcoin Layer) transaction: 0x" + hex(signing message hash)
    ```
    In this way, it can show message on wallets(e.g. UniSat, OKX) nicely.

* 0x05: It follows the unlocking method used by dogecoin. The signing message
  hash is converted as following:
  ```
  "0x" + hex(signing message hash)
  ```

* 0x12: It follows the unlocking method same to 0x02 with the signing message hash
  to be converted as following:
    ```
    "CKB transaction: 0x" + hex(signing message hash)
    ```
  In this way, it can show message on wallets(e.g. MetaMask) nicely.


* 0x06: It follows the same unlocking method used by [CKB
  MultiSig](https://github.com/nervosnetwork/ckb-system-scripts/blob/master/c/secp256k1_blake160_multisig_all.c) with a little modification.
  When a message is calculated for signing, there is a step to clear witness. In omnilock, it clears the whole field `lock` in `witness`. But in CKB MultiSig script, it only clears part of `lock` in `witness`. This part is used as `signatures` followed by `multisig_script`.

* 0xFC: The auth content that represents the blake160 hash of a lock script. The lock script will check if the current
  transaction contains an input cell with a matching lock script. Otherwise, it would return with an error. It's similar
  to [P2SH in BTC](https://en.bitcoin.it/wiki/Pay_to_script_hash).

* 0xFD: The auth content that represents the blake160 hash of a preimage. The preimage contains
  [exec](https://github.com/nervosnetwork/rfcs/blob/master/rfcs/0034-vm-syscalls-2/0034-vm-syscalls-2.md#exec) information that is used to delegate signature verification to
  another script via exec.

* 0xFE: The auth content that represents the blake160 hash of a preimage. The preimage contains [dynamic
  linking](https://docs.nervos.org/docs/labs/capsule-dynamic-loading-tutorial/) information that is used to delegate
  signature verification to the dynamic linking script. The interface described in [Swappable Signature Verification
  Protocol Spec](https://talk.nervos.org/t/rfc-swappable-signature-verification-protocol-spec/4802) is used here.


### Administrator Mode

When "administrator mode" is enabled, `<32 byte AdminList cell Type ID>` must be present. The AdminList cell contains the
type script hash used by a special cell with the same format as [RCE
Cell](https://talk.nervos.org/t/rfc-regulation-compliance-extension/5338). The RCE cell follows a set of rules and contains
whitelists and blacklists. These lists can be used in the [SMT proofs
scenarios](https://github.com/nervosnetwork/sparse-merkle-tree).

The RCE cells are organized in tree structure illustrated in the following diagram:

![RCE Cells](./rce_cells.png)


The diagram above shows the 4 lists in total. The AdminList cell Type ID is pointed to the root of tree and represents 4 lists in
order. If the current `RCRule` uses blacklist, the `auth` identity in `omni_identity` (see below) must not be present in
the blacklist SMT tree. If the current `RCRule` uses whitelist, the `auth` identity in `omni_identity` must be present
in the whitelist SMT tree.

The AdminList cell has the following distinctions compared to RCE Cell:

* The cell used here contains auth identities, not lock script hashes.

* If the cell contains an RCRule structure, this structure must be in whitelist mode.

* If the cell contains an RCCellVec structure, there must be at least one RCRule structure using whitelists in the RCCellVec.

To make this mode more flexible, when no type script hash is found in `cell_deps`, it continues searching in input cells
with the same type script hash. Once a cell is found, it will be used as `AdminList Cell`.

If the administrator mode flag is on, Anyone-can-pay mode, Time-lock mode and Supply mode flag will be ignored even set.
That means both the administrator and the user can unlock the cell, but the administrator is not constrained by
timelock. The administrator can only unlock existing cells with Administrator mode on. It's still impossible to bypass
supply limitation or mint new tokens at will.

### Anyone-can-pay Mode

When anyone-can-pay mode is enabled, `<2 bytes minimum ckb/udt in ACP>` must be present. It follows the rules of
[anyone-can-pay
lock](https://github.com/nervosnetwork/rfcs/blob/master/rfcs/0026-anyone-can-pay/0026-anyone-can-pay.md). The `<1 byte
CKByte minimum>` and `<1 byte UDT minimum>` are present at the same time.

### Time-lock Mode

When time-lock mode is enabled, `<8 bytes since for time lock>` must be present. The
[check_since](https://github.com/nervosnetwork/ckb-system-scripts/blob/63c63e9c96887395fc6990908bcba95476d8aad1/c/common.h#L91)
is used. The input parameter since is obtained from `<8 bytes since for time lock>`.

### Supply Mode

When supply mode is enabled, `<32 bytes type script hash>` must be present. The cell data of info cell which is specified
by type script hash has the following data structure:

```
version (1 byte)
current supply (16 bytes, little endian number)
max supply (16 bytes, little endian number)
sUDT script hash (32 bytes, sUDT type script hash)
... (variable length, other data)
```

Currently, the version is 0. Only the current supply field can be updated during transactions. The script iterates all
input and output cells, accumulating input amounts and output amounts identified by sUDT script hash. Then the script
verifies:

```
<issued amount> = <output amount> - <input amount>
<output current supply> = <issued amount> + <input current supply>
```
and
```
<output current supply> <= <max supply>
```

All the modes mentioned above can co-exist in Omnilock args in memory layout.

## Omnilock Witness

In witness, there is a signature field which is signed from a message. The message can be calculated via [blake2b hash function](https://github.com/nervosnetwork/rfcs/blob/master/rfcs/0022-transaction-structure/0022-transaction-structure.md#crypto-primitives) with following data:
- Transaction hash
- Witness length and content in same script group covered by inputs, excluding `lock` field which contains signature
- Other witness length and content that not covered by inputs


When unlocking an Omnilock, the corresponding witness must be a proper `WitnessArgs` data structure in molecule format. In
the lock field of the `WitnessArgs`, an `OmniLockWitnessLock` structure must be present as follows:
```
import xudt_rce;

array Auth[byte; 21];

table Identity {
    identity: Auth,
    proofs: SmtProofEntryVec,
}
option IdentityOpt (Identity);

// the data structure used in lock field of witness
table OmniLockWitnessLock {
    signature: BytesOpt,
    omni_identity: IdentityOpt,
    preimage: BytesOpt,
}
```

When `omni_identity` is present, it will be validated whether the provided auth in `omni_identity` is present in RC
AdminList Cell associated with the current lock script via SMT validation rules. In this case, the auth included in
`omni_identity` will be used in further validation.

If `omni_identity` is missing, the auth included in lock script args will then be used in further validation.

Once the processing above is successfully done and the auth to be used is confirmed, the flag in the designated auth
will be checked for the succeeding operations:

* When the auth flag is 0x0, a signature must be present in `OmniLockWitnessLock`. We will use the signature for secp256k1
  recoverable signature verification. The recovered public key hash using the blake160 algorithm must match the current
  auth content.

* When the auth flag is 0xFC, we will check against the current transaction, and there must be an input cell, whose lock
  script matches the auth content when hashed via blake160.


When `signature` is present, the signature can be used to unlock the cell in anyone-can-pay mode.

When `preimage` is present, the auth flag can be 0xFD or 0xFE.

* When auth flag is 0xFD, the `exec` method is used. The preimage's memory layout will be as follows:
```
exec code hash (32 bytes)
exec hash type (1 byte)
place (1 byte)
bounds (8 bytes)
pubkey blake160 hash (20 bytes)
```

The `place` and `bounds` are passed in directly to [exec](https://github.com/nervosnetwork/rfcs/blob/master/rfcs/0034-vm-syscalls-2/0034-vm-syscalls-2.md#exec):
```C
int ckb_exec(size_t index, size_t source, size_t place, size_t bounds, int argc, char* argv[]);
```
The `index` is located by `exec code hash` and `exec hash type` from `cell_deps`([See usage](https://github.com/nervosnetwork/ckb-c-stdlib/blob/20578dfb092b3b3761df755395e20ec142a83d6e/ckb_syscalls.h#L368)].
Finally, message, signature, pubkey blake160 hash are encoded into hex strings. Then these hex strings are passed in as `argv`.

* When auth flag is 0xFE, the `dynamic linking` method is used. The preimage's memory layout will be as follows:
```
dynamic library code hash (32 bytes)
dynamic library hash type (1 byte)
pubkey blake160 hash （20 bytes)
```

It loads the dynamic linking libraries via `code hash` and `hash type`, and gets the entry function named 
`validate_signature`. The entry function is expected to have following C API:
```C
int validate_signature(void *prefilled_data, const uint8_t *signature_buffer,
    size_t signature_size, const uint8_t *message_buffer, size_t message_size,
    uint8_t *pubkey_hash, size_t *pubkey_hash_len);
```
Then the entry function is called to validate the message and signature. The `pubkey_hash` returned from the entry
function is compared with the blake160 hash of the pubkey. If they are the same, then validation succeeds.

## Examples

### Unlock via owner's public key hash
```
CellDeps:
    <vec> Omnilock Script Cell
Inputs:
    <vec> Cell
        Data: <...>
        Type: <...>
        Lock:
            code_hash: Omnilock
            args: <flag: 0x0> <pubkey hash 1> <Omnilock flags: 0>
    <...>
Outputs:
    <vec> Any cell
Witnesses:
    WitnessArgs structure:
      Lock:
        signature: <valid secp256k1 signature for pubkey hash 1>
        omni_identity: <EMPTY>
        preimage: <EMPTY>
      <...>
```

### Unlock via owner's lock script hash

```
CellDeps:
    <vec> Omnilock Script Cell
Inputs:
    <vec> Cell
        Data: <...>
        Type: <...>
        Lock:
            code_hash: Omnilock
            args: <flag: 0xFC> <lock hash: 0x12...34> <Omnilock flags: 0>
    <vec> Cell
        Data: <...>
        Type: <...>
        Lock: blake160 for this lock script must be 0x12...34
    <...>
Outputs:
    <vec> Any cell
Witnesses:
    WitnessArgs structure:
      Lock:
        signature: <EMPTY>
        omni_identity: <EMPTY>
        preimage: <EMPTY>
      <...>
```

### Unlock via administrator's public key hash

```
CellDeps:
    <vec> Omnilock Script Cell
    <vec> AdminList Cell 1
Inputs:
    <vec> Cell
        Data: <...>
        Type: <...>
        Lock:
            code_hash: Omnilock
            args: <flag: 0x0> <pubkey hash 1> <Omnilock flags: 1> <AdminList Cell 1's type ID>
    <...>
Outputs:
    <vec> Any cell
Witnesses:
    WitnessArgs structure:
      Lock:
        signature: <valid secp256k1 signature for pubkey hash 2>
        omni_identity:
           identity: <flag: 0x0> <pubkey hash 2>
           proofs: <SMT proofs for the above identity in AdminList Cell 1>
        preimage: <EMPTY>
      <...>
```
### Unlock via administrator's lock script hash (1)
Note: the location of AdminList Cell 1 is in cell deps

```
CellDeps:
    <vec> Omnilock Script Cell
    <vec> AdminList Cell 1
Inputs:
    <vec> Cell
        Data: <...>
        Type: <...>
        Lock:
            code_hash: Omnilock
            args: <flag: 0> <pubkey hash 1> <Omnilock flags: 1> <AdminList Cell 1's type ID>
    <vec> Cell
        Data: <...>
        Type: <...>
        Lock: blake160 for this lock script must be 0x12...34
    <...>
Outputs:
    <vec> Any cell
Witnesses:
    WitnessArgs structure:
      Lock:
        signature: <EMPTY>
        omni_identity:
           identity: <flag: 0xFC> <lock hash: 0x12...34>
           proofs: <SMT proofs for the above identity in AdminList Cell 1>
        preimage: <EMPTY>
      <...>
```

### Unlock via administrator's lock script hash (2)
Note: the location of AdminList Cell 1 is in input cell

```
CellDeps:
    <vec> Omnilock Script Cell

Inputs:
    <vec> Cell
        Data: <...>
        Type: <...>
        Lock:
            code_hash: Omnilock
            args: <flag: 0> <pubkey hash 1> <Omnilock flags: 1> <AdminList Cell 1's type ID>
    <vec> Cell
        Data: <...>
        Type: <...>
        Lock: blake160 for this lock script must be 0x12...34
    <vec> AdminList Cell 1
        Data: <RCData, union of RCCellVec and RCRule>
        Type: <its hash is same to AdminList Cell 1's type ID>
        Lock: <...>
    <...>
Outputs:
    <vec> Any cell
Witnesses:
    WitnessArgs structure:
      Lock:
        signature: <EMPTY>
        omni_identity:
           identity: <flag: 0xFC> <lock hash: 0x12...34>
           proofs: <SMT proofs for the above identity in AdminList Cell 1>
        preimage: <EMPTY>
      <...>
```

### Unlock via anyone-can-pay

```
CellDeps:
    <vec> Omnilock Script Cell
Inputs:
    <vec> Cell
        Data: <...>
        Type: <...>
        Lock:
            code_hash: Omnilock
            args: <flag: 0x0> <pubkey hash 1> <Omnilock flags: 2> <2 bytes minimun ckb/udt in ACP>
    <...>
    follow anyone-can-pay rules
    <...>
Outputs:
    <vec> Any cell
Witnesses:
    WitnessArgs structure:
      Lock:
        signature: <EMPTY>
        omni_identity: <EMPTY>
        preimage: <EMPTY>
      <...>
```

### Unlock via dynamic linking

```
CellDeps:
    <vec> Omnilock Script Cell
Inputs:
    <vec> Cell
        Data: <...>
        Type: <...>
        Lock:
            code_hash: Omnilock
            args: <flag: 0xFE> <preimage hash> <Omnilock flags: 0>
    <...>
Outputs:
    <vec> Any cell
Witnesses:
    WitnessArgs structure:
      Lock:
        signature: <valid secp256k1 signature for pubkey hash 1>
        omni_identity: <EMPTY>
        preimage: <code hash> <hash type> <pubkey hash 1>
      <...>
```

### Unlock via exec, using ethereum signature
```
CellDeps:
    <vec> Omnilock Script Cell
Inputs:
    <vec> Cell
        Data: <...>
        Type: <...>
        Lock:
            code_hash: Omnilock
            args: <flag: 0xFD> <preimage hash> <Omnilock flags: 0>
    <...>
Outputs:
    <vec> Any cell
Witnesses:
    WitnessArgs structure:
      Lock:
        signature: <valid ethereum signature for pubkey hash 1>
        omni_identity: <EMPTY>
        preimage: <code hash> <hash type> <place> <bounds> <pubkey hash 1>
      <...>
```

### Unlock via owner's public key hash with time lock limit
```
CellDeps:
    <vec> Omnilock Script Cell
Inputs:
    <vec> Cell
        Data: <...>
        Type: <...>
        Lock:
            code_hash: Omnilock
            args: <flag: 0x0> <pubkey hash 1> <Omnilock flags: 4> <since 1>
    <...>
Outputs:
    <vec> Any cell
Witnesses:
    WitnessArgs structure:
      Lock:
        signature: <valid secp256k1 signature for pubkey hash 1>
        omni_identity: <EMPTY>
        preimage: <EMPTY>
      <...>
```


### Unlock with supply mode
```
CellDeps:
    <vec> Omnilock Script Cell
    <vec> sUDT Script Cell
Inputs:
    <vec> Cell
        Data: <version> <current supply, 2,000>  <max supply: 10,000> <sUDT script hash, H1>
        Type: <Type ID script with hash H2>
        Lock:
            code_hash: Omnilock
            args: <flag: 0x0> <pubkey hash 1> <Omnilock flags: 8> <type script hash, H2>
    <... one of the input cell must have owner lock script as lock, to mint>

Outputs:
    <vec> Cell
        Data: <version> <current supply, 3,000>  <max supply: 10,000> <sUDT script hash, H1>
        Type: <Type ID script with hash H2>
        Lock:
            code_hash: Omnilock
            args: <flag: 0x0> <pubkey hash 1> <Omnilock flags: 8> <type script hash, H2>
    <vec> Minted sUDT Cell
        Data: <amount, 1,000>
        Type: <type script hash, H1>
    <...>
Witnesses:
    WitnessArgs structure:
      Lock:
        signature: <valid secp256k1 signature for pubkey hash 1>
        omni_identity: <EMPTY>
        preimage: <EMPTY>
      <...>
```
Note, Here we combine Omnilock(lock script) and Type ID script(type script) into one cell.
```
Type: <Type ID script with hash H2>
Lock:
    code_hash: Omnilock
    args: <flag: 0x0> <pubkey hash 1> <Omnilock flags: 8> <type script hash, H2>
```
They can be in different cells.


## Notes

An [implementation](https://github.com/cryptape/omnilock) of the Omnilock spec above has been deployed to Mirana CKB mainnet and  Pudge testnet:


- Mirana

| parameter   | value                                                                |
| ----------- | -------------------------------------------------------------------- |
| `code_hash` | 0x9b819793a64463aed77c615d6cb226eea5487ccfc0783043a587254cda2b6f26   |
| `hash_type` | `type`                                                               |
| `tx_hash`   | 0xc76edf469816aa22f416503c38d0b533d2a018e253e379f134c3985b3472c842   |
| `index`     | `0x0`                                                                |
| `dep_type`  | `code`                                                               |

- Pudge

| parameter   | value                                                                |
| ----------- | -------------------------------------------------------------------- |
| `code_hash` | 0xf329effd1c475a2978453c8600e1eaf0bc2087ee093c3ee64cc96ec6847752cb   |
| `hash_type` | `type`                                                               |
| `tx_hash`   | 0x3d4296df1bd2cc2bd3f483f61ab7ebeac462a2f336f2b944168fe6ba5d81c014   |
| `index`     | `0x0`                                                                |
| `dep_type`  | `code`                                                               |


Reproducible build is supported to verify the deploy script. To build the deployed the script above, one can use the following steps:

```bash
$ git clone https://github.com/cryptape/omnilock.git
$ cd omnilock
$ git checkout cd764d
$ git submodule update --init --recursive
$ make all-via-docker
```

A draft of this specification has already been released, reviewed, and discussed in the community at [here](https://blog.cryptape.com/omnilock-a-universal-lock-that-powers-interoperability-1) for quite some time.


================================================
File: rfcs/0043-ckb-softfork-activation/0043-ckb-softfork-activation.md
================================================
---
Number: "0043"
Category: Standards Track
Status: Proposal
Author: Dingwei Zhang <zhangsoledad@gmail.com>
Created: 2022-06-16
---

# CKB softfork activation

## Abstract

This document specifies a proposed change to the semantics of the 'version' field in CKB blocks, allowing multiple backward-compatible changes (further called "softforks") to be deployed in parallel. It relies on interpreting the version field as a bit vector, where each bit can be used to track an independent change. These are tallied in each period. Once the consensus change succeeds or times out, there is a "fallow" pause, after which the bit can be reused for later changes.

## Specification

### Parameters

Each softfork deployment is specified by the following per-chain parameters (further elaborated below):

1. The `name` specifies a very brief description of the softfork, reasonable for use as an identifier.
2. The `bit` determines which bit in the `version` field of the block is to be used to signal the softfork lock-in and activation. It is chosen from the set {0,1,2,...,28}.
3. The `start_epoch` specifies the first epoch in which the bit gains meaning.
4. The `timeout_epoch` specifies an epoch at which the miner signaling ends. Once this epoch has been reached, if the softfork has not yet locked_in (excluding this epoch block's bit state), the deployment is considered failed on all descendants of the block.
5. The `period` specifies length of epochs of the signalling period.
6. The `threshold` specifies the minimum ratio of block per `period`, which indicate the locked_in of the softfork during the `period`.
7. The `minimum_activation_epoch` specifies the epoch at which the softfork is allowed to become active.

These parameters will be written to the software binary at the start of deployment and deployment will begin as the software is released.

### Selection guidelines
The following guidelines are suggested for selecting these parameters for a softfork:

1. `name` should be selected such that no two softforks, concurrent or otherwise, ever use the same name.
2. `bit` should be selected such that no two concurrent softforks use the same bit.
3. `start_epoch` can be set soon after software with parameters is expected to be released.
4. `timeout_epoch` should be set to an epoch when it is considered reasonable to expect the entire economy to have upgraded by, ensure sufficient time for the signaling `period`, should at least one and a half month, 270 epochs after `start_epoch`.
5. `period` should at least 42 epochs，approximately one week.
6. `threshold` should be 90% or 75% for testnet.
7. `minimum_activation_epoch` should be set to several epochs after `timeout_epoch` if the `start_epoch` is to be very soon after software with parameters is expected to be released.
Where the locked_in threshold is reached, softforks are guaranteed to activate eventually but not until `minimum_activation_epoch` after signal tracking starts, allowing users, developers, and organizations to prepare software, announcements, and celebrations for that event.

### States

With each block and softfork, we associate a deployment state. The possible states are:

1. DEFINED is the first state that each softfork starts. The blocks of 0 epoch is by definition in this state for each deployment.
2. STARTED for all blocks reach or past the `start_epoch`.
3. LOCKED_IN for one `period` after the first `period` with STARTED blocks of which at least `threshold` has the associated bit set in `version`.
4. ACTIVE for all blocks after the LOCKED_IN `period`.
5. FAILED for all blocks after the `timeout_epoch`, if LOCKED_IN was not reached.

### Bit flags

The `version` block header field is to be interpreted as a 32-bit little-endian integer, and bits are selected within this integer as values (1 << N) where N is the `bit` number.

```rust
    pub fn mask(&self) -> u32 {
        1u32 << bit as u32
    }
```

Blocks in the STARTED state get a `version` whose bit position bit is set to 1. The top 3 bits of such blocks must be 000, so the range of possible `version` values is [0x00000000...0x1FFFFFFF], inclusive.

By restricting the top 3 bits to 000, we get 29 out of those for this proposal and support future upgrades for different mechanisms.

Miners should continue setting the bit in the LOCKED_IN phase, so uptake is visible, though this does not affect consensus rules.

### New consensus rules
The new consensus rules for each softfork are enforced for each block with an ACTIVE state.

### State transitions

![State transitions](images/state-transitions.png)

The blocks of 0 epoch has a state DEFINED for each deployment, by definition.

```rust
if epoch.number().is_zero() {
    return ThresholdState::DEFINED;
}
```

We remain in the initial state until we reach the `start_epoch`.

```rust
match state {
    ThresholdState::DEFINED => {
        if epoch.number() >= start {
            next_state = ThresholdState::STARTED;
        }
    }
```

After a `period` in the STARTED state, we tally the bits set and transition to LOCKED_IN if a sufficient number of blocks in the past `period` set the deployment bit in their version numbers. If the threshold has not been met and we reach the `timeout_epoch`, we transition directly to FAILED.

Note that a block's state never depends on its version, only on that of its ancestors.

```rust
match state {
    ThresholdState::STARTED => {
        let mut count = 0;
        for block in (0..period_blocks) {
            if (block.version() & 0xE0000000 == 0x00000000 && (block.version() >> bit) & 1 == 1) {
                ++count;
            }
        }
        let threshold_number = threshold_number(period_blocks, threshold);
        if count >= threshold_number {
            next_state = ThresholdState::LOCKED_IN;
        } else if epoch_ext.number() >= timeout {
            next_state = ThresholdState::FAILED;
        }
    }
```
After a `period` of LOCKED_IN, we automatically transition to ACTIVE if the `minimum_activation_epoch` is reached. Otherwise, LOCKED_IN continues.

```rust
ThresholdState::LOCKED_IN => {
    if epoch.number() >= min_activation_epoch {
        next_state = ThresholdState::ACTIVE;
    }
}
```

Furthermore, ACTIVE and FAILED are terminal states in which a deployment stays once reached.

```rust
ThresholdState::FAILED | ThresholdState::ACTIVE => {
    // Nothing happens, these are terminal states.
}
```

## Deployments
A living list of deployment proposals can be found [here](./deployments.md).

## Reference

1. Wuille, P., Todd, P., Maxwell, G., & Russell, R. (2015). BIP9: Version bits with timeout and delay. Bitcoin BIPs. https://github.com/bitcoin/bips/blob/master/bip-0009.mediawiki
2. Fry, S., & Dashjr, L. (2017). BIP8: Version bits with lock-in by height. Bitcoin BIPs. https://github.com/bitcoin/bips/blob/master/bip-0008.mediawiki
3. Harding, D. A. (2021). Taproot activation proposal “Speedy Trial.” Bitcoin-Dev Mailing List. https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-March/018583.html


================================================
File: rfcs/0043-ckb-softfork-activation/deployments.md
================================================
# Deployments
---

List of proposed deployments.
| Name | Bit | Mainnet Start | Mainnet Timeout | Mainnet State | Testnet Start | Testnet Timeout | Testnet State | RFC |
| ----------- | ---------- | ---------------- | ---------- |  ----------- | ---------- | ---------------- | ---------- | ---------- |
| Light Client Protocol | 1 | TBD | TBD | TBD | epoch#5346 | epoch#5616 | active since epoch#5711 | [RFC0044](../0044-ckb-light-client/0044-ckb-light-client.md)


================================================
File: rfcs/0044-ckb-light-client/0044-ckb-light-client.md
================================================
---
Number: "0044"
Category: Standards Track
Status: Proposal
Author: Boyu Yang <yangby@cryptape.com>
Created: 2022-08-18
---

# CKB Light Client Protocol

## Abstract

This RFC describes a light client protocol on CKB which allows clients to
verify that a blockchain is valid with limited resources.

## Motivation

Downloading and verifying all blocks is taking hours and requiring gigabytes
of bandwidth and storage. Hence, clients with limited resources cannot
verify transactions independently without trusting full nodes.
Even some light clients only download all block headers, the storage and
bandwidth requirements of those clients still increase linearly with the
chain length.
We propose a more efficient [FlyClient]-based light client protocol. It uses
a sampling protocol tailored for [NC-Max] difficulty adjustment algorithm.
It requires downloading only a logarithmic number of block headers while
storing only a single block header between executions.

## Background

### Merkle Mountain Range (MMR)

A [Merkle Mountain Range (MMR)][MMR] is a binary hash tree that allows for
efficient appends of new leaves without changing the value of existing
nodes.

In MMR, we use the insertion order to reference leaves and nodes.
We insert a new leaf to MMR as following steps:
- Insert leaf or node to next position.
- If the new inserted leaf or node has a left sibling, we merge the left and
  right nodes to produce a new parent node, then go back to step 1 to insert
  the node.

For example, we insert a leaf to the example MMR which has 11 leaves as
following steps:
- Insert leaf to next position: `19`.
- Then check the left sibling `18` and calculate parent node:
  `merge(mmr[18], mmr[19])`.
- Insert parent node to position `20`.
- Since the node `20` also has a left sibling `17`, calculate parent node:
  `merge(mmr[17], mmr[20])`.
- Insert new node to next position `21`.
- Since the node `21` have no left sibling, complete the insertion.

```
# An MMR with 11 leaves:
          14
       /       \
     6          13
   /   \       /   \
  2     5     9     12     17
 / \   /  \  / \   /  \   /  \
0   1 3   4 7   8 10  11 15  16 18

# After insertion of a new leaf:

          14
       /       \
     6          13            21
   /   \       /   \         /   \
  2     5     9     12     17     20
 / \   /  \  / \   /  \   /  \   /  \
0   1 3   4 7   8 10  11 15  16 18  19
```

#### Merkle Root

An MMR is constructed by one or more sub merkle trees (or mountains). Each
sub merkle tree's root is a peak in MMR, we calculate the MMR root by
bagging these peaks from left to right.

For example:
- In the above 11 leaf MMR we have 3 peaks: `14, 17, 18`, we bag these peaks
  from left to right to get the root:
  `merge(merge(mmr[14], mmr[17]), mmr[18])`.
- In the above 12 leaf MMR we have 2 peaks: `14, 21`, we bag these peaks
  from left to right to get the root: `merge(mmr[14], mmr[21])`.

#### Merkle Proof

The merkle proof is an array of hashes constructed with the following parts:
- A merkle proof from the leaf's sibling to the peak that contains the leaf.
- A hash that bags all right-hand side peaks, skip this part if no
  right-hand peaks.
- Hashes of all left-hand peaks from right to left, skip this part if no
  left-hand peaks.

We can reconstruct a merkle root from leaves and the merkle proof for those
leaves.

### The FlyClient Sampling Protocol Under Variable Difficulty

The client uses $g(x) = \frac{1}{(1-x)\ln{\delta}}$ as the probability
density function (PDF) to sample blocks.

In this probability, $x$ denotes the relative aggregate difficulty weight
and $\delta$ denotes the relative difficulty weight of the blocks which are
sampled with probability $1$.

Concretely, let $\delta = c ^ {k}$, where $c$ denote the fraction of the
adversary's computing power relative to the honest computing power, $p$
denote the probability of catching the adversary with a single sample and
$k = \frac{1}{p}$.

The sampling domain is restricted from $0$ to $1 − \delta$ and the blocks in
the last $\delta$ fraction region should always be checked directly.

Let $n$ denote the number of the blocks in the region which requires
verifying and $L$ denote the number of blocks in the last $\delta$ fraction
region. Obviously, we have $L = \delta \times n = c ^ {k} \times n$.
Accordingly, $k = \log_{c}{(\frac{L}{n})}$.

Define $p_{m} = (1 - \frac{1}{k})^{m}$ as the probability of failure, i.e.,
not catching the optimal adversary after check $m$ blocks. If we want
$p_{m} \le 2^{-\lambda}$, then
$m \ge \frac{\lambda}{\log_{\frac{1}{2}} {(1 - \frac{1}{k})}}$.

## Specification

### MMR Node Specification

We treat all blocks as the leaf nodes of an MMR, and its block number is the
index of leaves in that MMR.

#### MMR Node

Each MMR node is defined as follows:

- `children_hash`
  - For a leaf node, it's its header hash.
  - For a non-leaf node, it's the hash of the serialized data that
    concatenate its two children nodes' hashes.
    A node's hash is the hash of its serialized data.

- `total_difficulty`
  - For a leaf node, it's the difficulty it took to mine the current block.
  - For a non-leaf node, it's the sum of `total_difficulty` in its child
    nodes.

- `start_*` and `end_*`
  - For a leaf node, both of them are the data of the current block.
  - For a non-leaf node:
    - `start_*` is the `start_*` of the leftmost node.
    - `end_*` is the `end_*` of the rightmost node.
  - There are 4 pairs of data:
    - `*_number` means a block number.
    - `*_epoch` means an epoch number.
    - `*_timestamp` means a block timestamp.
    - `*_compact_target` means a block compact target.

An MMR node will represent in [Molecule] schema as follows:
```
struct HeaderDigest {
    children_hash:          Byte32,

    total_difficulty:       Uint256,

    start_number:           Uint64,
    end_number:             Uint64,

    start_epoch:            Uint64,
    end_epoch:              Uint64,

    start_timestamp:        Uint64,
    end_timestamp:          Uint64,

    start_compact_target:   Uint32,
    end_compact_target:     Uint32,
}
```

#### Chain Root

A chain root for a block is the merkle root of all blocks on the chain
until that block (include itself).

After the epoch which MMR starts to be enabled in, the first 32 bytes of the
block extension should be the hash of its parent chain root.

#### Verifiable Header

A verifiable header is a header with the fields which are used to do
verification for its extra hash [\[1\]].

It contains a normal header, its uncles' hash, its block extension and the
chain root for its parent block.

### Protocol Messages

All protocol messages will represent in [Molecule] schema as follows:
```
union LightClientMessage {
    // A client asks the server for the last state of the chain.
    GetLastState,
    SendLastState,
    // A client asks the server for the proof of the last state which the
    // client known.
    GetLastStateProof,
    SendLastStateProof,
    // A client asks the server for the proof of some blocks.
    GetBlocksProof,
    SendBlocksProof,
    // A client asks the server for the proof of some transactions.
    GetTransactionsProof,
    SendTransactionsProof,
}

table GetLastState {
    // Whether the server is requested to push the state automatically.
    subscribe:                  Bool,
}

table SendLastState {
    // The verifiable header for the tip block in the server.
    last_header:                VerifiableHeader,
}

table GetLastStateProof {
    // The last block hash known by the client.
    // It could be different with the tip hash in the server.
    last_hash:                  Byte32,

    // The hash of the last proved block.
    start_hash:                 Byte32,
    // The block number of the last proved block.
    start_number:               Uint64,

    // How many continuous blocks before the tip block should be included at
    // least, if possible?
    last_n_blocks:              Uint64,
    // All blocks, whose total difficulty is not less than this difficulty
    // boundary, should be included in the proof.
    difficulty_boundary:        Uint256,
    // The sampled difficulties.
    difficulties:               Uint256Vec,
}

table SendLastStateProof {
    // If the block whose hash is sent from the client is on the chain, then
    // returns its verifiable header; otherwise, returns the verifiable
    // header for the tip block in the server.
    last_header:                VerifiableHeader,
    // The MMR proof for the chain root whose hash is in the last header.
    // Be empty if the block hash sent from the client isn't on the chain.
    proof:                      HeaderDigestVec,

    // Verifiable headers for all sampled blocks.
    headers:                    VerifiableHeaderVec,
}

table GetBlocksProof {
    // Refer to `GetLastStateProof.last_hash`.
    last_hash:                  Byte32,

    // Block hashes for the blocks which require verifying.
    block_hashes:               Byte32Vec,
}

table SendBlocksProof {
    // Refer to `SendLastStateProof.last_header`.
    last_header:                VerifiableHeader,
    // Refer to `SendLastStateProof.proof`.
    proof:                      HeaderDigestVec,

    // Block headers for the blocks which require verifying.
    headers:                    HeaderVec,

    // Block hashes for the blocks which were not found.
    missing_block_hashes:       Byte32Vec,
}

table GetTransactionsProof {
    // Refer to `GetLastStateProof.last_hash`.
    last_hash:                  Byte32,

    // Transaction hashes for the transactions which require verifying.
    tx_hashes:                  Byte32Vec,
}

table SendTransactionsProof {
    // Refer to `SendLastStateProof.last_header`.
    last_header:                VerifiableHeader,
    // Refer to `SendLastStateProof.proof`.
    proof:                      HeaderDigestVec,

    // A collection of filtered blocks, which include all requested
    // transactions, and be verified in the proof.
    filtered_blocks:            FilteredBlockVec,

    // Transaction hashes for the blocks which were not found.
    missing_tx_hashes:          Byte32Vec,
}
```

### Client-Server Interaction

We can divide the typical client-server interaction into 3 phases.

#### Phase 1: Client synchronizes to the latest tip block.

- At start, the client sends `GetLastState` messages to all discovered
  servers.

- All servers will reply `SendLastState` which includes their verifiable tip
  header and the total difficulty of the chain.

- The client will send `GetLastStateProof` to all servers which have replied
  their last state. The `GetLastStateProof` messages are different for
  different chain states, they are generated base on the sampling strategy
  which the light client used.

  _The sampling strategy is not a part of CKB. Each light client could
  choose their own sampling strategy._

  _In [CKB light client implementation], a sampling strategy based on the
  [FlyClient] sampling protocol under variable difficulty is used._

- The servers which received `GetLastStateProof` should reply
  `SendLastStateProof` to the client.

  If the last block that the client known is in the server's current chain,
  then the server should return the verifiable header of the last block that
  the client known, the proof for the sampled blocks and their verifiable
  headers.

  Otherwise, the server should only return its current verifiable tip
  header.

- Then the client will choose the best chain from all `SendLastStateProof`
  messages.

##### How a Server Choose Blocks from Sampled Difficulties?

Notice: Some variables in this section are defined in
[the "Background" section](#the-flyclient-sampling-protocol-under-variable-difficulty).

The server performs the following steps to decide which block should be
included in the proof:

- Construct a collection of blocks which is denoted as `reorg_n_blocks`.

  Check if the start block, which is sent from the client, is an ancestor
  block of the tip block.

  - If true, then left `reorg_n_blocks` to be empty.

  - Otherwise:

    - Find the block which is an ancestor block of the tip block and its
    number is the same as the start block.

    - Add the last $L$ blocks before the block, which is found in the
    previous step, into `reorg_n_blocks`.

- Construct a collection of blocks which is denoted as `last_n_blocks`.

  - Find the first block whose total difficulty is not less than
  $D_{\mathrm{boundary}}$.

  - If the number of blocks between the block, which is found in the
  previous step, and the tip block (excluded), is greater than $L$, then
  `last_n_blocks` includes all these blocks; otherwise, `last_n_blocks`
  includes the last $L$ blocks before the tip block.

- Construct a collection of blocks which is denoted as `sampled_blocks`.

  - Let $D_{\mathrm{boundary}}^{'}$ denote the total difficulty of the first
  block in `last_n_blocks`.

  - For each difficulty in the sampled difficulties, try to find the first
  block whose total difficulty is not less than it; if the total difficulty
  of this block is less than $D_{\mathrm{boundary}}^{'}$ and it's not a
  duplicate block, then add it into `sampled_blocks`.

  The sampled difficulties should be sorted.

- The proof should include all blocks in `reorg_n_blocks`, `last_n_blocks`,
  and `sampled_blocks`.

  And, the set of block headers should be sorted by theirs block numbers.

##### The Sampling Strategy used in the Official CKB Light Client

Notice: Some variables in this section are defined in
[the "Background" section](#the-flyclient-sampling-protocol-under-variable-difficulty).

In the official implementation of CKB light client, the sampling strategy is
based on the [FlyClient] sampling protocol under variable difficulty.

At first, the client should have a tip block header which requires
verification, and its parent chain root whose hash stored in that block, and
the total difficulty until that block (included that block).

Moreover, the client chooses the latest verified block as the start block.
If there are no verified blocks, the client should choose the genesis block
as the start block.

Then the client performs the following steps to samples blocks:
- Let region $R_{\mathrm{full}}$ denote the region between the start block
  and the tip block (excluded).
- Calculate $n$, which is the number of blocks in the region $R_{\mathrm{full}}$.
- Let $c = 0.5$ and $L = 100$, then calculate $k$ with the formula
  $k = \log_{c}{(\frac{L}{n})}$.
- Compare $n$ with $L$:
  - If $n \le L$, skip the sampling.
  - Otherwise:
    - Let $\lambda = 50$, then calculate $m$ with the formula
      $m = \lceil \frac{\lambda}{\log_{\frac{1}{2}} {(1 - \frac{1}{k})}} \rceil$.
    - Let $D_{\mathrm{start}}$ denote the total difficulty of the start
      block and $D_{\mathrm{end}}$ denote the total difficulty of the tip
      block.
    - Calculate the difficulty boundary $D_{\mathrm{boundary}}$, which is
      the start boundary of last $\delta$ fraction region, with formula
      $D_{\mathrm{boundary}} = D_{\mathrm{start}} + (1 - \delta)(D_{\mathrm{end}} - D_{\mathrm{start}})$.
    - Calculate $\delta = c ^ {k}$.
    - Use the PDF $g(x)$ to random $m - L$ difficulties, which are satisfied
      $d_{i} \in \left[ D_{\mathrm{start}}, D_{\mathrm{boundary}} \right), i \in \left[ 0, m-L-1 \right]$
      as the samples.

After sampling, the client sends a request to the server, namely, a CKB full
node, with following data:
- The blocks hash of the tip block that client knows.
- The blocks hash of the start block.
- The blocks number of the start block.
- The number $L$.
- The difficulty boundary $D_{\mathrm{boundary}}$.
- The difficulty samples. If there are no difficulties sampled in the
  previous step, leave them to be empty.

At last, the server replies the proof of the sampled blocks.

Accordingly, the client can treat that tip block as a valid tip block after
checking the returned proof.

#### Phase 2: Client asks a proof for some blocks.

After [phase 1], the client gets a trusted chain based on a proved tip block.

Then the client can use this phase to check blocks whether they are on that
trusted chain or not.

- At the start, the client sends the `GetBlocksProof` message, which
  contains the last block hash it knows, and hashes of the blocks which
  require verifying, to the server which has the best chain.

- Then, the server should reply a `SendBlocksProof` message.

  If the last block that the client known is in the server's current chain,
  then the server should return
  - the verifiable header of the last block that the client knows,
  - a proof for blocks which are from the provided block hashes and in the
    current chain,
  - the headers of those proved blocks,
  - hashes of blocks which are from the provided block hashes but not in the
    current chain.

  Otherwise, the server should only return its current verifiable tip
  header.

#### Phase 3: Client asks a proof for some transactions.

After [phase 1], the client gets a trusted chain based on a proved tip block.

Then the client can use this phase to check transactions whether they are on
that trusted chain or not.

- At the start, the client sends the `GetTransactionsProof` message, which
  contains the last block hash it knows, and hashes of the transactions
  which require verifying, to the server which has the best chain.

- Then, the server should reply a `SendTransactionsProof` message.

  If the last block that the client known is in the server's current chain,
  then the server should return
  - the verifiable header of the last block that the client knows,
  - a proof for blocks which contain valid transactions from the provided
    transaction hashes,
  - the proofs that proving those valid transactions are in those blocks,
  - hashes of transactions which are from the provided transaction hashes
    but not in the current chain.

  Otherwise, the server should only return its current verifiable tip
  header.

## Limitations

To avoid attacks by malicious clients, there are few limitations in server
side.
- In `GetLastStateProof` messages, the sum of the size of difficulties and 2
  times of the `last_n_blocks` should NOT greater than 1000.
- In `GetBlocksProof` messages, the size of block hashes should NOT greater
  than 1000.
- In `GetTransactionsProof` messages, the size of transactions hashes should
  NOT greater than 1000.

There are also few limitations in messages to improvement performance.
- In `GetLastStateProof` messages, the difficulties should be sorted.
- In `SendLastStateProof` messages, the headers should be sorted.

##### How a Server Choose Blocks from Sampled Difficulties?

## Deployment

Since CKB doesn't record the hashes of chain roots into headers at the start,
so it requires a soft fork to extend the relevant consensus rules.

Hence, this feature should be deployed concurrently with
[RFC-0043 CKB softfork activation].

In fact, it uses a modified version of [RFC-0043 CKB softfork activation],
which uses the cellbase witness field instead of the block header version
for signaling.

The parameters[\[2\]] to activate this feature are:
| Parameters | For CKB Testnet | For CKB Mainnet |
|-------|---------------|---------------|
| `name` | LightClient | LightClient |
| `bit` | 1 | 1 |
| `start_epoch` | 5346 (approx. 2022-10-31 15:00:00 UTC) | 8282 (approx. 2023-09-01 00:00:00 UTC) |
| `timeout_epoch` | 5616 (approx. 2022-12-14 15:00:00 UTC) | 8552 (approx. 2023-10-15 00:00:00 UTC) |
| `period` | 42 | 42 |
| `threshold` | 75% | 80% |
| `min_activation_epoch` | 5676 (approx. 2022-12-24 15:00:00 UTC) | 8648 (approx. 2023-11-01 00:00:00 UTC) |

For CKB Testnet, the deployment did activate at epoch 5711.

> Some parameters for CKB mainnet are to be determined.

And we also have to create all MMR nodes for blocks before that epoch to
make sure that the indexes of the MMR leaves are equal to the block numbers.

If users have data which generated by the previous version of CKB, they have
to do data migration before enable this feature.
And once the migration started, the data will no longer be compatible with
all older versions of CKB.

## References

- [FlyClient: Super-Light Clients for Cryptocurrencies][FlyClient]
- [Merkle Mountain Ranges][MMR]

[\[1\]]: ../0031-variable-length-header-field/0031-variable-length-header-field.md#specification
[\[2\]]: ../0043-ckb-softfork-activation/0043-ckb-softfork-activation.md#parameters
[phase 1]: #phase-1-client-synchronizes-to-the-latest-tip-block
[FlyClient]: https://eprint.iacr.org/2019/226.pdf
[NC-Max]: ../0020-ckb-consensus-protocol/0020-ckb-consensus-protocol.md
[MMR]: https://github.com/opentimestamps/opentimestamps-server/blob/master/doc/merkle-mountain-range.md
[Molecule]: ../0008-serialization/0008-serialization.md#molecule
[RFC-0043 CKB softfork activation]: ../0043-ckb-softfork-activation/0043-ckb-softfork-activation.md
[CKB light client implementation]: https://github.com/nervosnetwork/ckb-light-client


================================================
File: rfcs/0045-client-block-filter/0045-client-block-filter.md
================================================
---
Number: "0045"
Category: Standards Track
Status: Proposal
Author: Quake Wang <quake.wang@gmail.com>
Created: 2022-08-23
---

# CKB Client Side Block Filter Protocol

## Abstract

This RFC describes a block filter protocol that could be used together with [RFC 0044](../0044-ckb-light-client/0044-ckb-light-client.md). It allows clients to obtain compact probabilistic filters of CKB blocks from full nodes and download full blocks if the filter matches relevant data.

## Motivation

Light clients allow applications to read relevant transactions from the blockchain without incurring the full cost of downloading and validating all data. Such applications seek to simultaneously minimize the trust in peers and the amount of bandwidth, storage space, and computation required. They achieve this by sampling headers through the fly-client protocol, verifying the proofs of work, and following the longest proof-of-work chain. Light clients then download only the blockchain data relevant to them directly from peers and validate inclusion in the header chain. Though clients do not check the validity of all blocks in the longest proof-of-work chain, they rely on miner incentives for security.

Full nodes generate deterministic filters on block data that are served to the client. A light client can then download an entire block if the filter matches the data it is watching for. Since filters are deterministic, they only need to be constructed once and stored on disk, whenever a new block is appended to the chain. This keeps the computation required to serve filters minimal.

## Specification

### Protocol Messages

#### GetBlockFilters

`GetBlockFilters` is used to request the compact filters of a particular range of blocks:

```
struct GetBlockFilters {
    start_number:   Uint64, // The height of the first block in the requested range
}
```

#### BlockFilters
`BlockFilters` is sent in response to `GetBlockFilters`, one for each block in the requested range:

```
table BlockFilters {
    start_number:   Uint64,     // The height of the first block in the requested range
    block_hashes:   Byte32Vec,  // The hashes of the blocks in the range
    filters:        BytesVec,   // The filters of the blocks in the range
}
```

1. The `start_number` SHOULD match the field in the GetBlockFilters request.
2. The `block_hashes` field size should not be larger than 1000.
3. The `block_hashes` and `filters` fields size SHOULD match.

#### GetBlockFilterHashes
`GetBlockFilterHashes` is used to request verifiable filter hashes for a particular range of blocks:

```
struct GetBlockFilterHashes {
    start_number:   Uint64,  // The height of the first block in the requested range
}
```

#### BlockFilterHashes
`BlockFilterHashes` is sent in response to `GetBlockFilterHashes`:

```
table BlockFilterHashes {
    start_number:               Uint64,     // The height of the first block in the requested range
    parent_block_filter_hash:   Byte32,     // The hash of the parent block filter
    block_filter_hashes:        Byte32Vec,  // The hashes of the block filters in the range
}
```

1. The `start_number` SHOULD match the field in the GetBlockFilterHashes request.
2. The `block_filter_hashes` field size SHOULD not exceed 2000


#### GetBlockFilterCheckPoints
`GetBlockFilterCheckPoints` is used to request filter hashes at evenly spaced intervals over a range of blocks. Clients may use filter hashes from `GetBlockFilterHashes` to connect these checkpoints, as is described in the
[Client Operation](#client-operation) section below:

```
struct GetBlockFilterCheckPoints {
    start_number:   Uint64,     // The height of the first block in the requested range
}
```

#### BlockFilterCheckPoints
`BlockFilterCheckPoints` is sent in response to `GetBlockFilterCheckPoints`. The filter hashes included are the set of all filter hashes on the requested blocks range where the height is a multiple of the interval 2000:

```
table BlockFilterCheckPoints {
    start_number:           Uint64,
    block_filter_hashes:    Byte32Vec,
}
```

1. The `start_number` SHOULD match the field in the GetBlockFilterCheckPoints request.
2. The `block_filter_hashes` field size should not be larger than 2000.

### Filter Data Generation

We follow the BIP158 for filter data generation and use the same Golomb-Coded Sets parameters P and M values. The only difference is that we only use cell's lock/type script hash as the filter data:

```
filter.add_element(cell.lock.calc_script_hash().as_slice());
if let Some(type_script) = cell.type_().to_opt() {
    filter.add_element(type_script.calc_script_hash().as_slice());
}
```

### Node Operation

Full nodes MAY opt to support this RFC, such nodes SHOULD treat the filters as an additional index of the blockchain. For each new block that is connected to the main chain, nodes SHOULD generate filters and persist them. Nodes that are missing filters and are already synced with the blockchain SHOULD reindex the chain upon start-up, constructing filters for each block from genesis to the current tip.

Nodes SHOULD NOT generate filters dynamically on request, as malicious peers may be able to perform DoS attacks by requesting small filters derived from large blocks. This would require an asymmetrical amount of I/O on the node to compute and serve.

Nodes MAY prune block data after generating and storing all filters for a block.

### Client Operation

This section provides recommendations for light clients to download filters with maximal security.

Clients SHOULD first sync with the full nodes by verifying the best chain tip through the fly-client protocol before downloading any filters or filter hashes. Clients SHOULD disconnect any outbound peers whose best chain has significantly less work than the known longest chain.


Once a client's tip is in sync, it SHOULD download and verify filter hashes for all blocks. The client SHOULD send `GetBlockFilterHashes` messages to full nodes and store the filter hashes for each block. The client MAY first fetch hashes by sending `GetBlockFilterCheckPoints`. The checkpoints allow the client to download filter hashes for different intervals from multiple peers in parallel, verifying each range of 2000 headers against the checkpoints.

Unless securely connected to a trusted peer that is serving filter hashes, the client SHOULD connect to multiple outbound peers to mitigate the risk of downloading incorrect filters. If the client receives conflicting filter hashes from different peers for any block, it SHOULD interrogate them to determine which is faulty. The client SHOULD use `GetBlockFilterHashes` and/or `GetBlockFilterCheckPoints` to first identify the first filter hashes that the peers disagree on. The client then SHOULD download the full block from any peer and derive the correct filter and filter hash. The client SHOULD ban any peers that sent a filter hash that does not match the computed one.

Once the client has downloaded and verified all filter hashes needed, and no outbound peers have sent conflicting headers, the client can download the actual block filters it needs. Starting from the first block in the desired range, the client now MAY download the filters. The client SHOULD test that each filter links to its corresponding filter hash and ban peers that send incorrect filters. The client MAY download multiple filters at once to increase throughput.

Each time a new valid block header is received, the client SHOULD request the corresponding filter hashes from all eligible peers. If two peers send conflicting filter hashes, the client should interrogate them as described above and ban any peers that send an invalid header.

If a client is fetching full blocks from the P2P network, they SHOULD be downloaded from outbound peers at random to mitigate privacy loss due to transaction intersection analysis. Note that blocks may be downloaded from peers that do not support this RFC.
## Deployment

This RFC is deploy identically to CKB Light Client Protocol ([RFC0044](../0044-ckb-light-client/0044-ckb-light-client.md)).

## Reference

1. BIP157: https://github.com/bitcoin/bips/blob/master/bip-0157.mediawiki
2. BIP158: https://github.com/bitcoin/bips/blob/master/bip-0158.mediawiki


================================================
File: rfcs/0046-syscalls-summary/0046-syscalls-summary.md
================================================
---
Number: "0046"
Category: Informational
Status: Draft (for Informational)
Author: Shan <github.com/linnnsss>
Created: 2023-07-21
---

# CKB VM Syscalls Summary

This RFC aims to provide a comprehensive summary of all CKB VM syscalls as specified in RFC documents [RFC9 VM Syscalls](https://github.com/nervosnetwork/rfcs/blob/master/rfcs/0009-vm-syscalls/0009-vm-syscalls.md), [RFC34 VM Syscalls 2](https://github.com/nervosnetwork/rfcs/blob/master/rfcs/0034-vm-syscalls-2/0034-vm-syscalls-2.md), and RFC50 VM Syscalls 3 (currently under review). The goal is to gather relevant information from these documents and present it in an organized table for easy reference and comparison. This RFC also includes relevant constants, such as return codes, sources, cell fields, header fields, and input fields.

## CKB VM Syscalls

| VM Ver. | Syscall ID | C Function Name                                              | Description                                                  |
| ------- | ---------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 1       | 93         | [ckb_exit](https://github.com/nervosnetwork/rfcs/blob/master/rfcs/0009-vm-syscalls/0009-vm-syscalls.md#exit) | Immediately terminate the execution of the currently running script and exit with the specified return code. |
| 1       | 2061       | [ckb_load_tx_hash](https://github.com/nervosnetwork/rfcs/blob/master/rfcs/0009-vm-syscalls/0009-vm-syscalls.md#load-transaction-hash) | Calculate the hash of the current transaction and copy it using partial loading. |
| 1       | 2051       | [ckb_load_transaction](https://github.com/nervosnetwork/rfcs/blob/master/rfcs/0009-vm-syscalls/0009-vm-syscalls.md#load-transaction) | Serialize the full transaction of the running script using the Molecule Encoding 1 format and copy it using partial loading. |
| 1       | 2062       | [ckb_load_script_hash](https://github.com/nervosnetwork/rfcs/blob/master/rfcs/0009-vm-syscalls/0009-vm-syscalls.md#load-script-hash) | Calculate the hash of currently running script and copy it using partial loading. |
| 1       | 2052       | [ckb_load_script](https://github.com/nervosnetwork/rfcs/blob/master/rfcs/0009-vm-syscalls/0009-vm-syscalls.md#load-script) | Serialize the currently running script using the Molecule Encoding 1 format and copy it using partial loading. |
| 1       | 2071       | [ckb_load_cell](https://github.com/nervosnetwork/rfcs/blob/master/rfcs/0009-vm-syscalls/0009-vm-syscalls.md#load-cell) | Serialize the specified cell in the current transaction using the Molecule Encoding 1 format and copy it using partial loading. |
| 1       | 2081       | [ckb_load_cell_by_field](https://github.com/nervosnetwork/rfcs/blob/master/rfcs/0009-vm-syscalls/0009-vm-syscalls.md#load-cell-by-field) | Load a single field from the specified cell in the current transaction and copy it using partial loading. |
| 1       | 2092       | [ckb_load_cell_data](https://github.com/nervosnetwork/rfcs/blob/master/rfcs/0009-vm-syscalls/0009-vm-syscalls.md#load-cell-data) | Load the data from the cell data field in the specified cell from the current transaction and copy it using partial loading. |
| 1       | 2091       | [ckb_load_cell_data_as_code](https://github.com/nervosnetwork/rfcs/blob/master/rfcs/0009-vm-syscalls/0009-vm-syscalls.md#load-cell-data-as-code) | Load the data from the cell data field in the specified cell from the current transaction, mark the loaded memory page as executable, and copy it using partial loading. The loaded code can then be executed by CKB VM at a later time. |
| 1       | 2073       | [ckb_load_input](https://github.com/nervosnetwork/rfcs/blob/master/rfcs/0009-vm-syscalls/0009-vm-syscalls.md#load-input) | Serialize the specified input cell in the current transaction using the Molecule Encoding 1 format and copy it using partial loading. |
| 1       | 2083       | [ckb_load_input_by_field](https://github.com/nervosnetwork/rfcs/blob/master/rfcs/0009-vm-syscalls/0009-vm-syscalls.md#load-input-by-field) | Load a single field from the specified input cell in the current transaction and copy it using partial loading. |
| 1       | 2072       | [ckb_load_header](https://github.com/nervosnetwork/rfcs/blob/master/rfcs/0009-vm-syscalls/0009-vm-syscalls.md#load-header) | Serialize the specified header associated with an input cell, dep cell, or header dep using the Molecule Encoding 1 format and copy it using partial loading. |
| 1       | 2082       | [ckb_load_header_by_field](https://github.com/nervosnetwork/rfcs/blob/master/rfcs/0009-vm-syscalls/0009-vm-syscalls.md#load-header-by-field) | Load a single field from the specified header associated with an input cell, dep cell, or header dep and copy it using partial loading. |
| 1       | 2074       | [ckb_load_witness](https://github.com/nervosnetwork/rfcs/blob/master/rfcs/0009-vm-syscalls/0009-vm-syscalls.md#load-witness) | Load the specified witness in the current transaction and copy it using partial loading. |
| 1       | 2177       | [ckb_debug](https://github.com/nervosnetwork/rfcs/blob/master/rfcs/0009-vm-syscalls/0009-vm-syscalls.md#debug) | Print the specified message in CKB's terminal output for the purposes of debugging. |
| 2       | 2041       | [ckb_vm_version](https://github.com/nervosnetwork/rfcs/blob/master/rfcs/0034-vm-syscalls-2/0034-vm-syscalls-2.md#vm-version) | Return the version of CKB VM being used to execute the current script. |
| 2       | 2042       | [ckb_current_cycles](https://github.com/nervosnetwork/rfcs/blob/master/rfcs/0034-vm-syscalls-2/0034-vm-syscalls-2.md#current-cycles) | Return the number of cycles consumed by the currently running script *immediately before* executing this syscall. This syscall will consume an additional 500 cycles. |
| 2       | 2043       | [ckb_exec](https://github.com/nervosnetwork/rfcs/blob/master/rfcs/0034-vm-syscalls-2/0034-vm-syscalls-2.md#exec) | Run a script executable from the specified cell using the current VM context. This replaces the original calling running script executable with the new specified script executable. This is similar to the [exec call](https://en.wikipedia.org/wiki/Exec_(system_call)) found in several operating systems. |
| | | ckb_spawn | (To be added in CKB2023.) Run a script executable from the specified cell using the current VM context, but return to the original calling script executable upon termination. This is similar to the [spawn function](https://en.wikipedia.org/wiki/Spawn_(computing)) found in several operating systems and programming languages. |
| | | ckb_get_memory_limit | (To be added in CKB2023.) Return the maximum amount of memory available to the current script being executed. |
| | | ckb_set_content | (To be added in CKB2023.) Set the content of the designated memory region that can be read by the parent (calling) script which executed the current script via the spawn function. |
| | | ckb_load_extension | (To be added in CKB2023.) Load the extention field data and copy it using partial loading. |

## Constants

### Return Codes

These are the return codes used by the CKB VM syscalls. 

| Const No. | C Example              | Description                                       |
| --------- | ---------------------- | ------------------------------------------------- |
| 0         | CKB_SUCCESS            | No error.                                         |
| 1         | CKB_INDEX_OUT_OF_BOUND | Index out of bound. (e.g. No such input cell.)    |
| 2         | CKB_ITEM_MISSING       | The requested resource does not exist.            |
| 3         | CKB_LENGTH_NOT_ENOUGH  | The supplied memory buffer too small.             |
| 4         | CKB_INVALID_DATA       | The data provided is invalid.                     |

### Source

These are the sources for syscalls that query the transaction for input cells, output cells, dep cells, and header deps. 

| Const No.          | C Example               | Description                                                                                 |
| ------------------ | ----------------------- | ------------------------------------------------------------------------------------------- |
| 0x1                | CKB_SOURCE_INPUT        | All input cells in the transaction.                                                         |
| 0x0100000000000001 | CKB_SOURCE_GROUP_INPUT  | Only the input cells in the transaction using the same script as currently running script.  |
| 2                  | CKB_SOURCE_OUTPUT       | All output cells in the transaction.                                                        |
| 0x0100000000000002 | CKB_SOURCE_GROUP_OUTPUT | Only the output cells in the transaction using the same script as currently running script. |
| 3                  | CKB_SOURCE_CELL_DEP     | All dep cells in the transaction.                                                           |
| 4                  | CKB_SOURCE_HEADER_DEP   | All header deps in the transaction.                                                         |

### Cell Fields

These are the field specifiers for syscalls that request a specific field of a cell.

| Const No. | C Example                        | Description                                                            |
| --------- | -------------------------------- | ---------------------------------------------------------------------- |
| 0         | CKB_CELL_FIELD_CAPACITY          | The capacity (CKB) contained in the cell.                              |
| 1         | CKB_CELL_FIELD_DATA_HASH         | The hash of the data within the data field of the cell.                |
| 2         | CKB_CELL_FIELD_LOCK              | The lock script of the cell.                                           |
| 3         | CKB_CELL_FIELD_LOCK_HASH         | The hash of the lock script of the cell.                               |
| 4         | CKB_CELL_FIELD_TYPE              | The type script of the cell.                                           |
| 5         | CKB_CELL_FIELD_TYPE_HASH         | The hash of the type script of the cell.                               |
| 6         | CKB_CELL_FIELD_OCCUPIED_CAPACITY | The amount of capacity (CKB) that is currently being used by the cell. |

### Header Fields

These are the field specifiers for syscalls that request a specific field of a header dep.

| Const No. | C Example                                 | Description                                                      |
| --------- | ----------------------------------------- | ---------------------------------------------------------------- |
| 0         | CKB_HEADER_FIELD_EPOCH_NUMBER             | The epoch number for the header dep.                             |
| 1         | CKB_HEADER_FIELD_EPOCH_START_BLOCK_NUMBER | The block number of first block in the epoch for the header dep. |
| 2         | CKB_HEADER_FIELD_EPOCH_LENGTH             | The length of the epoch for the header dep.                      |

### Input Fields

These are the field specifiers for syscalls that request a specific field of an input cell.

| Const No. | C Example                 | Description                                  |
| --------- | ------------------------- | -------------------------------------------- |
| 0         | CKB_INPUT_FIELD_OUT_POINT | The out point of the specified input cell.   |
| 1         | CKB_INPUT_FIELD_SINCE     | The since value of the specified input cell. |


================================================
File: rfcs/0048-remove-block-header-version-reservation-rule/0048-remove-block-header-version-reservation-rule.md
================================================
---
Number: "0048"
Category: Standards Track
Status: Draft
Author: Dingwei Zhang <zhangsoledad@gmail.com>
Created: 2023-04-17
---

# Remove Block Header Version Reservation Rule


## Abstract

This rfc proposes to remove this reservation and allow for the use of CKB softfork activation [RFC43] in the block header. This change will be implemented in the 2023 edition of the CKB consensus rules.

## Motivation

The version field in the CKB block header currently has no real meaning, as the consensus rule forces it to be 0 in CKB2021 and earlier. This means that it cannot be used to signal CKB softfork activation [RFC43]. To address this issue, This rfc proposes to remove this reservation and allow for the use of version bits in the block header.

## Specification

This RFC must be activated via a hard fork. After activation, any unsigned 32-bit integer is legal for the version field and no verification rule will be required.


================================================
File: rfcs/0049-ckb-vm-version-2/0049-ckb-vm-version-2.md
================================================
---
Number: "0049"
Category: Standards Track
Status: Draft
Author: Wanbiao Ye <mohanson@outlook.com>
Created: 2023-04-17
---

# VM version2

## Abstract

This RFC delineates the specifications for CKB-VM version 2. CKB-VM version 2 pertains to the version implemented in the CKB Meepo hardfork.

## **Motivation**

The upgrade of CKB-VM in Meepo hardfork aims to enhance the security, portability, and efficiency of scripts. Throughout recent years, several questions have been a source of concern for us:

- We currently lack a secure and straightforward method to invoke one script from another.
    - The **[dynamic library call](https://github.com/nervosnetwork/rfcs/blob/master/rfcs/0009-vm-syscalls/0009-vm-syscalls.md#load-cell-data-as-code)** presents a security concern. The sub-script and parent script share the same memory space, leading to an uncontrolled security risk when calling an unknown sub-script.
    - Although the **[Exec](https://github.com/nervosnetwork/rfcs/blob/master/rfcs/0034-vm-syscalls-2/0034-vm-syscalls-2.md#exec)** system call doesn't pose any security issues, it is exceptionally challenging to utilize effectively.
- Running more intricate scripts (such as zero-knowledge proofs) on CKB-VM necessitates higher performance requirements for CKB-VM.

## **Specification**

To address the aforementioned issues, we have implemented the following optimizations for CKB-VM. These optimizations have been thoroughly tested and standardized through the use of RFC.

In comparison to version 1, version 2 of CKB-VM incorporates the following enhancements:

1. One notable addition is the inclusion of a new system call called "Spawn," which can be further explored in the RFC titled "VM Syscalls 3." In essence, Spawn serves as an alternative to dynamic library calls and Exec. With Spawn, a script can create a child script with an independent memory area, and data can be passed between the parent and child scripts without restriction.
2. [Macro-Operation Fusion](https://en.wikichip.org/wiki/macro-operation_fusion). There are 5 MOPs added in VM version 2, there are:

| Opcode |      Origin      |  Cycles   |                            Description                             |
| ------ | ---------------- | --------- | ------------------------------------------------------------------ |
| ADCS   | add + sltu       | 1 + 0     | Overflowing addition                                               |
| SBBS   | sub + sltu       | 1 + 0     | Borrowing subtraction                                              |
| ADD3A  | add + sltu + add | 1 + 0 + 0 | Overflowing addition and add the overflow flag to the third number |
| ADD3B  | add + sltu + add | 1 + 0 + 0 | Similar to ADD3A but the registers order is different              |
| ADD3C  | add + sltu + add | 1 + 0 + 0 | Similar to ADD3A but the registers order is different              |

Detailed matching patterns for the above MOPs(Please note that the registers here are only used for placeholders, and it does not mean that the MOP is only established when r0, r1, r2, r3):

**ADCS rd, rs1, rs2, rs3**

```
add r0, r1, r2
sltu r3, r0, r1
// or
add r0, r2, r1
sltu r3, r0, r1

Activated when:
r0 != r1
r0 != x0
```

**SBBS rd, rs1, rs2, rs3**

```
sub r0, r1, r2
sltu r3, r1, r2

Activated when:
r0 != r1
r0 != r2
```

**ADD3A rd, rs1, rs2, rs3, rs4**

```
add r0, r1, r0
sltu r2, r0, r1
add r3, r2, r4

Activated when:
r0 != r1
r0 != r4
r2 != r4
r0 != x0
r2 != x0
```

**ADD3B rd, rs1, rs2, rs3, rs4**

```
add r0, r1, r2
sltu r3, r0, r1
add r3, r3, r4

Activated when:
r0 != r1
r0 != r4
r3 != r4
r0 != x0
r3 != x0
```

**ADD3C rd, rs1, rs2, rs3, rs4**

```
add r0, r1, r2
sltu r3, r0, r1
add r3, r3, r4

Activated when:
r0 != r1
r0 != r4
r3 != r4
r0 != x0
r3 != x0
```


================================================
File: rfcs/0050-vm-syscalls-3/0050-vm-syscalls-3.md
================================================
---
Number: "0050"
Category: Standards Track
Status: Draft
Author: Xuejie Xiao <xxuejie@gmail.com>, Jiandong Xu<lynndon@gmail.com>, Wanbiao Ye <mohanson@outlook.com>, Dingwei Zhang <zhangsoledad@gmail.com>
Created: 2023-04-17
---

# VM Syscalls 3

## Abstract

This document describes the addition of the syscalls during the CKB Meepo hardfork. This update significantly enhances the flexibility of CKB Script.

## Introduction

The design of the syscall spawn function draws inspiration from Unix and Linux, hence they share the same terminologies: process, pipe, and file descriptor. The spawn mechanism is used in ckb-vm to create new processes, which can then execute a different program or command independently of the parent process.

In the context of ckb-vm, a process represents the active execution of a RISC-V binary. This binary can be located within a cell. Additionally, a RISC-V binary can also be found within the witness during a syscall spawn. A pipe is established by associating two file descriptors, each linked to one of its ends. These file descriptors can't be duplicated and are exclusively owned by the process. Furthermore, the file descriptors can only be either read from or written to; they can't be both read from and written to simultaneously.

It is worth noting that process scheduling in ckb-vm is deterministic, specifically:

- For each hardfork version, the process scheduling will be deterministic, any indeterminism will be treated as critical / security bugs that requires immediate intervention
- However, based on real usage on chain, it is expected that future hardfork versions would improve the process scheduling workflow, hence making the behavior different across versions

We added 8 spawn-related syscalls and one block-related syscall, respectively:

- [Spawn]
- [Pipe]
- [Inherited File Descriptors]
- [Read]
- [Write]
- [Close]
- [Wait]
- [Process ID]
- [Load Block Extension]

### Spawn
[Spawn]: #spawn

The syscall Spawn is the core part of this update. The parent process calls the Spawn system call, which creates a new process (a child process) that is an independent ckb-vm instance. It's important to note that the parent process will not be blocked by the child process as a result of this syscall.

```c
typedef struct spawn_args_t {
  size_t argc;
  const char** argv;
  /* Spawned VM process ID */
  uint64_t* process_id;
  /* A list of file descriptor, 0 indicates end of array */
  const uint64_t* inherited_fds;
} spawn_args_t;

int ckb_spawn(size_t index, size_t source, size_t place, size_t bounds,
              spawn_args_t* spawn_args);
```

The arguments used here are:

- index: an index value denoting the index of entries to read.
- source: a flag denoting the source of cells or witnesses to locate, possible values include:
    - 1: input cells.
    - `0x0100000000000001`: input cells with the same running script as current script
    - 2: output cells.
    - `0x0100000000000002`: output cells with the same running script as current script
    - 3: dep cells.
- place: A value of 0 or 1:
    - 0: read from cell data
    - 1: read from witness
- bounds: high 32 bits means offset, low 32 bits means length. if length equals to zero, it read to end instead of reading 0 bytes.
- spawn_args: pass data during process creation or save return data.
    - argc: argc contains the number of arguments passed to the program
    - argv: argv is a one-dimensional array of strings
    - process_id: a pointer used to save the process_id of the child process
    - inherited_fds: an array representing the file descriptors passed to the child process. It must end with zero, for example, when you want to pass `fd1` and `fd2`, you need to construct an array `[fd1, fd2, 0]`.

The arguments used here - index, source, bounds, place, argc, and argv - follow the usage described in [EXEC].

There are some hard limits to the system to avoid the overuse of resources.

- CKB-VM allows 16 processes to exist at the same time (excluding the root process). Processes that are created but exit normally will not be counted.
- A maximum of 4 instantiated VMs is allowed. Each process needs to occupy a VM instance to run. When the number of processes is greater than 4, some processes will enter a state called "uninstantiated". CKB-VM implements a scheduler to decide which processes should be "instantiated" and which processes should be "uninstantiated". However, switching the instantiation state of a VM is very expensive, developers should try to keep the number of processes below 4 so that all processes are instantiated.

### Pipe
[Pipe]: #pipe

This syscall create a pipe with read-write pair of file descriptions. The file descriptor with read permission is located at `fds[0]`, and the corresponding file descriptor with write permission is located at `fds[1]`. A maximum of 64 file descriptors can exist at the same time.

```c
int ckb_pipe(uint64_t fds[2]);
```

File descriptors can be passed to a child process via the `inherited_fds` parameter of the Spawn syscall.

### Inherited File Descriptors
[Inherited File Descriptors]: #inherited-file-descriptors

This syscall retrieves the file descriptors available to the current process, which are passed in from the parent process. These results are copied from the `inherited_fds` parameter of the Spawn syscall.

```c
int ckb_inherited_file_descriptors(uint64_t* fd, size_t* count);
```

When returning from the syscall, the syscall fills `fd` with the file descriptors in unit of `uint64_t` and fills in `count` with the count of corresponding file descriptors. The actual count of file descriptor written to `fd` is the minimum value between the count of `inherited_fds` in the Spawn syscall and the input value pointed to by `count` before syscall.

### Read
[Read]: #read

This syscall reads data from a pipe via a file descriptor. The syscall Read attempts to read up to value pointed by length bytes from file descriptor fd into the buffer, and the actual length of data read is written back to the length parameter. The syscall may pause the execution of current process.

```c
int ckb_read(uint64_t fd, void* buffer, size_t* length);
```

For the specific behavior description of Read, you can refer to the following [Write] syscall.

### Write
[Write]: #write

This syscall writes data to a pipe via a file descriptor. The syscall Write writes up to value pointed by length bytes from the buffer, and the actual length of data written is written back to the length parameter. The syscall may pause the execution of current process.

```c
int ckb_write(uint64_t fd, const void* buffer, size_t* length);
```

When using `ckb_read` and `ckb_write`, there may be the following scenarios:

**Success**

- If the writer writes W bytes and the reader reads R bytes where W > R, the writer will block, and the reader will return immediately with R bytes in `*length`. The reader can then call `ckb_read` again to read the remaining W - R bytes.
- If the writer writes W bytes and the reader reads R bytes where W <= R, both the writer and the reader will return immediately with W bytes in `*length`.

**Failure**

- If the writer writes data to pipe, but no other reader reads data from pipe, the writer will block permanently. If ckb-vm detects that all processes are blocked, ckb-vm will return a deadlock error.
- If the reader reads data from pipe, but no other writer writes data to pipe, the reader will block permanently. If ckb-vm detects that all processes are blocked, ckb-vm will return a deadlock error.

### Close
[Close]: #close

This syscall manually closes a file descriptor. After calling this, any attempt to read/write the file descriptor pointed to the other end would fail, so closing a single file descriptor, essentially closes the entire pair of pipes. After using close, there are four typical situations:

- close writer, and then try to write data to writer through `ckb_write`. In this case `ckb_write` will fail and return error(6).
- close writer, and then try to read data from reader through `ckb_read`. In this case if there is unread data in Pipe, ckb_read will execute normally; otherwise, it will return error(7).
- close reader, and then try to write data to writer through `ckb_write`. In this case `ckb_write` will fail and return error(7).
- close reader, and then try to read data from reader through `ckb_read`. In this case `ckb_read` will fail and return error(6).

```c
int ckb_close(uint64_t fd);
```

It's not always necessary to manually close file descriptors. When a process is terminated, all file descriptors owned by the process are automatically closed.

### Wait
[Wait]: #wait

The syscall pauses until the execution of a process specified by `pid` has ended. Retrieve the exit code of the process through the `exit_code` parameter. If a process is waited repeatedly, or you pass in the wrong Process ID, the method returns immediately with error(5), and the value saved in the `exit_code` will not be updated.

```c
int ckb_wait(uint64_t pid, int8_t* exit_code);
```

### Process ID
[Process ID]: #process-id

This syscall is used to get the current process id.

```c
uint64_t ckb_process_id();
```

Root process ID is 0.

### Load Block Extension
[Load Block Extension]: #load-block-extension

*Load Block Extension* syscall has a signature like the following:

```c
int ckb_load_block_extension(void* addr, uint64_t* len, size_t offset, size_t index, size_t source)
{
  return syscall(2104, addr, len, offset, index, source, 0);
}
```

The arguments used here are:

* `addr`, `len` and `offset` follow the usage described in [Partial Loading] section.
* `index`: an index value denoting the index of entries to read.
* `source`: a flag denoting the source of cells to locate, possible values include:
    + 1: input cells.
    + `0x0100000000000001`: input cells with the same running script as current script
    + 3: dep cells.
    + 4: header deps.

This syscall would locate the `extension` field associated either with an input cell, a dep cell, or a header dep based on `source` and `index` value, then use the same step as documented in [Partial Loading] section to feed the serialized value into VM.

Note when you are loading the `extension` associated with an input cell or a dep cell, the header hash of the corresponding block should still be included in `header deps` section of the current transaction.

This syscall might return the following errors:
* An invalid source value would immediately trigger an VM error and halt execution.
* The syscall would return with `1` as return value if the index value is out of bound.
* This syscall would return with `2` as return value if requesting a header for an input cell, but the `header deps` section is missing the header hash for the input cell.

In case of errors, `addr` and `index` will not contain meaningful data to use.

## Error Code

Five new error types added:

- Error code 5: The file descriptor is invalid during syscall [Wait].
- Error code 6: The file descriptor is not owned by this process.
- Error code 7: The other end of the pipe is closed.
- Error code 8: The maximum count of spawned processes has been reached.
- Error code 9: The maximum count of created pipes has been reached.

## Deadlock

Deadlock is a situation where two or more processes are unable to proceed because they are each waiting for resources or conditions that can only be provided by another waiting process. In the context of this scheduler, where processes communicate via pipes and can enter various states, such as `Runnable`, `Running`, `Terminated`, `WaitForExit`, `WaitForRead`, `WaitForWrite`. In our scheduler, deadlock will occur if all unterminated processes are waiting and no process is in a runnable state.

- The process enters the `Runnable` when a process is created, or it's blocking condition is resolved.
- The process enters the `Running` when a process starts running.
- The process enters the `Terminated` when a process is terminated.
- The process enters the `WaitForExit` state by calling the `wait()` on another process still running.
- The process enters the `WaitForRead` state by calling the `read()`. A process might not actually enter `WaitForRead` state by calling `read()`, if data are already available at the other end. It only enters this state when it wants data but data are not ready, in other words, it has a blocking condition.
- The process enters the `WaitForWrite` state by calling the `write()`. A process might not actually enter `WaitForRead` state by calling `write()`, if the other end is in `WaitForRead` state and is able to read all the data.

If multiple processes are in the `WaitForExit`, `WaitForWrite`, or `WaitForRead` states and are waiting on each other in a circular dependency, a deadlock can occur. Here are two examples:

1. A simple deadlock scenario, both processes are waiting for the other process to send data:
    - Process A is in `WaitForRead` for data from process B
    - Process B is in `WaitForRead` for data from process A. Both processes will wait indefinitely, as each is waiting for the other to proceed.

2. Deadlock caused by unbuffered pipes. Note that the pipe in ckb-vm is unbuffered. If one process blocks on a `WaitForWrite` state because the data is not fully read, and the reader process is also blocked in a `WaitForRead` state (but on a different file descriptor), this can create a deadlock if neither can proceed:
    - Process A wants to read 10 bytes from fd0, and then read 10 bytes from fd1, and finally read 10 bytes from fd0.
    - Process B writes 20 bytes into fd0, and then write 10 bytes into fd1.


## Cycles

Two new constants for cycles consumption are introduced:

```rust
pub const SPAWN_EXTRA_CYCLES_BASE: u64 = 100_000;
pub const SPAWN_YIELD_CYCLES_BASE: u64 = 800;
```

The Cycles consumption of each Syscall is as follows. Among them, the constant 500 and BYTES_TRANSFERRED_CYCLES can be referred to [RFC-0014](https://github.com/nervosnetwork/rfcs/blob/master/rfcs/0014-vm-cycle-limits/0014-vm-cycle-limits.md).

|     Syscall Name     |                                   Cycles Charge                                    |
| -------------------- | ---------------------------------------------------------------------------------- |
| spawn                | 500 + SPAWN_YIELD_CYCLES_BASE + BYTES_TRANSFERRED_CYCLES + SPAWN_EXTRA_CYCLES_BASE |
| pipe                 | 500 + SPAWN_YIELD_CYCLES_BASE                                                      |
| inherited_fd         | 500 + SPAWN_YIELD_CYCLES_BASE                                                      |
| read                 | 500 + SPAWN_YIELD_CYCLES_BASE + BYTES_TRANSFERRED_CYCLES                           |
| write                | 500 + SPAWN_YIELD_CYCLES_BASE + BYTES_TRANSFERRED_CYCLES                           |
| close                | 500 + SPAWN_YIELD_CYCLES_BASE                                                      |
| wait                 | 500 + SPAWN_YIELD_CYCLES_BASE                                                      |
| process_id           | 500                                                                                |
| load block extension | 500 + BYTES_TRANSFERRED_CYCLES                                                     |

In addition, when a VM switches between instantiated and uninstantiated states, it requires SPAWN_EXTRA_CYCLES_BASE cycles for each transition.

## Spawn Example

Consider the creation of a dependency library with a straightforward function that receives strings, concatenates them, and subsequently returns the resulting string to the caller(a.k.a echo).

**Caller**

```c
#include <stdint.h>
#include <string.h>

#include "ckb_syscalls.h"

#define CKB_STDIN (0)
#define CKB_STDOUT (1)

// Function read_all reads from fd until an error or EOF and returns the data it read.
int ckb_read_all(uint64_t fd, void* buffer, size_t* length) {
    int err = 0;
    size_t read_length = 0;
    size_t full_length = *length;
    uint8_t* b = buffer;
    while (true) {
        size_t n = full_length - read_length;
        err = ckb_read(fd, b, &n);
        if (err == CKB_OTHER_END_CLOSED) {
            err = 0;
            *length = read_length;
            break;
        } else {
            if (err != 0) {
                goto exit;
            }
        }
        if (full_length - read_length == 0) {
            err = CKB_LENGTH_NOT_ENOUGH;
            if (err != 0) {
                goto exit;
            }
        }
        b += n;
        read_length += n;
        *length = read_length;
    }

exit:
    return err;
}

// Mimic stdio fds on linux
int create_std_fds(uint64_t* fds, uint64_t* inherited_fds) {
    int err = 0;

    uint64_t to_child[2] = {0};
    uint64_t to_parent[2] = {0};
    err = ckb_pipe(to_child);
    if (err != 0) {
        goto exit;
    }
    err = ckb_pipe(to_parent);
    if (err != 0) {
        goto exit;
    }

    inherited_fds[0] = to_child[0];
    inherited_fds[1] = to_parent[1];
    inherited_fds[2] = 0;

    fds[CKB_STDIN] = to_parent[0];
    fds[CKB_STDOUT] = to_child[1];

exit:
    return err;
}

int main() {
    int err = 0;

    const char* argv[] = {};
    uint64_t pid = 0;
    uint64_t fds[2] = {0};
    // it must be end with zero
    uint64_t inherited_fds[3] = {0};
    err = create_std_fds(fds, inherited_fds);
    if (err != 0) {
        goto exit;
    }

    spawn_args_t spgs = {
        .argc = 0,
        .argv = argv,
        .process_id = &pid,
        .inherited_fds = inherited_fds,
    };
    err = ckb_spawn(0, 3, 0, 0, &spgs);
    if (err != 0) {
        goto exit;
    }

    size_t length = 0;
    length = 12;
    err = ckb_write(fds[CKB_STDOUT], "Hello World!", &length);
    if (err != 0) {
        goto exit;
    }
    err = ckb_close(fds[CKB_STDOUT]);
    if (err != 0) {
        goto exit;
    }

    uint8_t buffer[1024] = {0};
    length = 1024;
    err = ckb_read_all(fds[CKB_STDIN], buffer, &length);
    if (err != 0) {
        goto exit;
    }
    err = memcmp("Hello World!", buffer, length);
    if (err != 0) {
        goto exit;
    }

exit:
    return err;
}
```

**Callee**

```c
#include <stdint.h>
#include <string.h>

#include "ckb_syscalls.h"

#define CKB_STDIN (0)
#define CKB_STDOUT (1)

// Function read_all reads from fd until an error or EOF and returns the data it read.
int ckb_read_all(uint64_t fd, void* buffer, size_t* length) {
    int err = 0;
    size_t read_length = 0;
    size_t full_length = *length;
    uint8_t* b = buffer;
    while (true) {
        size_t n = full_length - read_length;
        err = ckb_read(fd, b, &n);
        if (err == CKB_OTHER_END_CLOSED) {
            err = 0;
            *length = read_length;
            break;
        } else {
            if (err != 0) {
                goto exit;
            }
        }
        if (full_length - read_length == 0) {
            err = CKB_LENGTH_NOT_ENOUGH;
            if (err != 0) {
                goto exit;
            }
        }
        b += n;
        read_length += n;
        *length = read_length;
    }

exit:
    return err;
}

int main() {
    int err = 0;

    uint64_t fds[2] = {0};
    uint64_t fds_len = 2;
    err = ckb_inherited_file_descriptors(fds, &fds_len);
    if (err != 0) {
        goto exit;
    }

    uint8_t buffer[1024] = {0};
    size_t length;
    length = 1024;
    err = ckb_read_all(fds[CKB_STDIN], buffer, &length);
    if (err != 0) {
        goto exit;
    }
    err = ckb_write(fds[CKB_STDOUT], buffer, &length);
    if (err != 0) {
        goto exit;
    }
    err = ckb_close(fds[CKB_STDOUT]);
    if (err != 0) {
        goto exit;
    }

exit:
    return err;
}
```

[EXEC]: ../0034-vm-syscalls-2/0034-vm-syscalls-2.md#exec
[Partial Loading]: ../0009-vm-syscalls/0009-vm-syscalls.md#partial-loading


================================================
File: rfcs/0051-ckb2023/0051-ckb2023.md
================================================
---
Number: "0051"
Category: Standards Track
Status: Draft
Author: Dingwei Zhang <zhangsoledad@gmail.com>
Created: 2023-04-17
---

# CKB Consensus Change (Edition CKB2023)

The current edition of CKB consensus rules is CKB2021. CKB2023 refers to the new edition of CKB consensus rules after its second hardfork, The purpose of a hard fork is to upgrade and update the rules encoded in the network. The changes are not backward compatible. This document outlines the changes in this upgrade.

## What's in CKB2023
CKB2023 will bring significant changes to the consensus rules, these changes include the removal of the reservation rule on version field in the block header, the introduction of a new version of the virtual machine (VM) with new syscalls and standard extensions, and the optimization of performance with new mops. This RFC provides a detailed overview of these changes.


### CKB VM v2

Since CKB2023, there will be multiple VM versions available. [RFC32] introduces a CKB VM version mechanism. It piggybacks on the `hash_type` field in the Script structure.

| `hash_type` | JSON representation | matches by | VM version |
| ----------- | ---------- | ---------------- | ---------- |
| 0           | "data"     | data hash        | 0          |
| 1           | "type"     | type script hash | 2          |
| 2           | "data1"    | data hash        | 1          |
| 4           | "data2"    | data hash        | 2          |


[RFC0049] introduces what's new in CKB VM v2 and [RFC0050] adds new syscalls for VM v2.

CKB VM v2 bring the following features:

* New syscalls Spawn, Get Memory Limit, Set Content will be added. The syscall Spawn is the core part of this update. The Spawn and the latter two syscalls: Get Memory Limit and Set Content together, implement a way to call another CKB Script in a CKB Script. Unlike the Exec syscall, Spawn saves the execution context of the current script, like posix_spawn, the parent script blocks until the child script ends.
* [“A” Standard Extension](https://five-embeddev.com/riscv-isa-manual/latest/a.html), strictly speaking “A” Standard Extension in ckb-vm does not bring functional changes, but many existing code will be compiled with Atomic Instructions and need to be patched, while ckb-vm can implement A instructions to eliminate such work. For example, in CKB VM v2, if you write a script with rust, you can now use [log](https://crates.io/crates/log) crate directly.
* Introduce more [macro-op fusion](https://en.wikichip.org/wiki/macro-operation_fusion) to reduce cycles consumption of scripts.


### Remove Block Header Version Reservation Rule

In CKB2021, the version field of the block header is reserved and only allowed to be 0. In the 2023 edition this reservation will be removed to allow for the use of [RFC0043]

## CKB2023 Timeline

The mainnet upgrade is divided into three phases.

* **Stage 1 - Code Preview**: An RC version of 0.200.0 is ready for preview on June 30 2023 via nervosnetwork/ckb [releases](https://github.com/nervosnetwork/ckb/releases). It will introduce the incompatible changes to help developers to adapt their tools and apps to CKB2023. But this version does not activate the consensus incompatible changes in CKB2023. Developers can test the new rules by running a dev chain locally.

* **Stage 2 - Testnet Activation**:

* **Stage 3 - Mainnet Activation**:

## Upgrade Strategies

First, the SDK, Tool, and dApps authors must adapt to any 0.200.0 rc version.

There are two strategies for ecosystem developers to upgrade to the CKB2023 consensus. Choose the former one if the developers can pause the app during the fork activation, otherwise, use the latter one.

- Release two different versions or use the feature switcher. Manually deploy the newer version or enable the feature CKB2023 after the fork activation.
- Use feature switcher and enable the feature CKB2023 automatically when the chain grows into the activation epoch. The activation epoch is different in the testnet and the mainnet, which is available via the updated `get_consensus` RPC.

## Appendix

### CKB2023 RFCs List

* [RFC0048]: Remove Block Header Version Reservation Rule.
* [RFC0050]: CKB VM Syscalls 3.
* [RFC0049]: CKB VM version2.
* RFC0051: This RFC, CKB2023 overview.

[RFC0043]: ../0043-ckb-softfork-activation/0043-ckb-softfork-activation.md
[RFC0048]: ../0048-remove-block-header-version-reservation-rule/0048-remove-block-header-version-reservation-rule.md
[RFC0049]: ../0049-ckb-vm-version-2/0049-ckb-vm-version-2.md
[RFC0050]: ../0050-vm-syscalls-3/0050-vm-syscalls-3.md


================================================
File: rfcs/0052-extensible-udt/0052-extensible-udt.md
================================================
---
Number: "0052"
Category: Standards Track
Status: Proposal
Author: Xuejie Xiao <xxuejie@gmail.com>, Xu Jiandong <lynndon@gmail.com>
Created: 2024-01-09
---


# Extensible UDT

Extensible UDT(xUDT) is an extension of [Simple
UDT](https://github.com/nervosnetwork/rfcs/blob/master/rfcs/0025-simple-udt/0025-simple-udt.md) for
defining more behaviors a UDT might need. While simple UDT provides a minimal
core for issuing UDTs on Nervos CKB, extensible UDT builds on top of simple UDT
for more potential needs, such as regulations.

## **Data Structure**

**xUDT Cell**

An xUDT cell is backward compatible with Simple UDT, all the existing rules
defined in the Simple UDT spec must still hold true for xUDT cells. On top of
sUDT, xUDT extends a cell like the following:

```yaml
data:
    <amount: uint128> <xUDT data>
type:
    code_hash: xUDT type script
    args: <owner lock script hash> <xUDT args>
lock:
    <user_defined>
```
The `amount` is a 128-bit unsigned integer in little endian format. The added `xUDT
args` and `xUDT data` parts provide all the new functions needed by xUDT, the
detailed structure is explained below.

### **xUDT Args**

xUDT args has the following structure:

```
<4-byte xUDT flags> <Variable length bytes, extension data>
```

Depending on the content of `flags`, which is represented as a 32-bit unsigned
integer in little-endian format, different extension data might be attached:

• If `flags & 0x1FFFFFFF` is 0, no extension data is required. Note a
backward-compatible way of viewing things, which is that a plain sUDT cell also
has a hidden `flags` field with all zeros.

• If `flags & 0x1FFFFFFF` is 0x1, extension data will contain a
[molecule](https://github.com/nervosnetwork/molecule) serialized `ScriptVec`
structure:

```
table Script {
    code_hash:      Byte32,
    hash_type:      byte,
    args:           Bytes,
}

vector ScriptVec <Script>
```

Each entry included in `ScriptVec` structure is interpreted as
an extension script with additional behaviors. When an xUDT script is
executed, it will run through each included extension script. Only when all
extension scripts pass validation, will xUDT also consider the validation to be
successful.

An extension script can be loaded in any of the following ways:

1. Some extension logics might have a predefined hash, for example, we can
   use `0x0000 ... 0001` to represent regulation extension. The actual code for
   such scripts can be embedded in xUDT script itself.
2. If an input cell in the current transaction uses a lock script with the same
   script hash as the current extension script, we can consider the extension
   script to be validated already.
3. If an extension script does not match any of the above criteria, xUDT will
   use the code_hash and hash_type included in the extension script to
   invoke [ckb_dlopen2](https://github.com/nervosnetwork/ckb-c-stdlib/blob/37eba3102100808ffc6fa2383bcf9e1e2651c8ea/ckb_dlfcn.h#L108-L113) function,
   hoping to load a dynamically linked script from cell deps in the current
   transaction. If a script can be located successfully, xUDT will then look for
   an exported function with the following signature:

```c
int validate(int is_owner_mode, size_t extension_index, const uint8_t* args, size_t args_length);
```

`is_owner_mode` indicates if the current xUDT is unlocked via owner mode(as
described by sUDT), `extension_index` refers to the index of the current
extension in the `ScriptVec` structure. `args` and `args_length` are set to the
script args included in `Script` structure of the current extension script.

If this function returns 0, the current extension script validation is
considered successful.

• If `flags & 0x1FFFFFFF` is 0x2, extension data will contain the blake160 hash
of the `ScriptVec` structure as explained in the previous section. The
actual `extension_scripts` (`ScriptVec`) structure data will be included in a
witness field `input_type` or `output_type` contained in the current
transaction. We will explain this part below. Choosing `input_type` or
`output_type` depends on whether the type script is running on input or output
cells. Under a lot of scenarios, it is `input_type`. But in the following example “Owner
Mode Without Consuming Cell”, we can see it’s possible on `output_type`.

### xUDT Witness

The `input_type` or `output_type` field in witness has the following data
structure in molecule format:

```js
table XudtWitness {
    owner_script: ScriptOpt,		
    owner_signature: BytesOpt,
    extension_scripts: ScriptVecOpt,
    extension_data: BytesVec,
}
```

The field `owner_script` and `owner_signature` will be used in owner mode. The
field `extension_scripts` is used when `flags & 0x1FFFFFFF` is 0x2 in args.

The length of `extension_data` structure inside must also be the same as
`ScriptVec` in `xUDT args` or `extension_scripts`. An extension script might also
require transaction-specific data for validation. The witness here provides a
place for these data needs.

### Owner Mode Update

As described in RFC sUDT, if an input cell in the current transaction uses an
input lock script with the same script hash as the owner lock script hash, the
`is_owner_mode` will be set to true. In xUDT, this rule is updated by the
following rule:

If an input or output cell in the current transaction uses one or more of the
following:

- input lock script (when `flags & 0x20000000` is **zero** or `flags` is not present)
- output type script (when `flags & 0x40000000` is **non-zero**)
- input type script (when `flags & 0x80000000` is **non-zero**)

With the same script hash as the owner lock script hash, the `is_owner_mode`
will be set to true. The output lock scripts are not included, because they
won’t be run in a transaction.

If the `owner_script` in witness isn’t none and its blake2b hash is the same as
the owner lock script hash in `args`, this script will be run as an extension
script. If the script returns success, `is_owner_mode` is set to true. Note, the
`owner_signature` field can be used by this owner script. When tokens are
minted, the `owner_script` and `owner_signature` can be set to some proper
values. When tokens are transferred, they can be set to none.

### **xUDT Data**

xUDT data is a molecule serialized `XudtData` structure:

```
vector Bytes <byte>
vector BytesVec <Bytes>

table XudtData {
  lock: Bytes,
  data: BytesVec,
}
```

The `data` field included in `XudtData`, must be of the same length
as `ScriptVec` structure included in xUDT args. Some extensions might require
user-specific data stored in each xUDT cell. xUDT data provides a place for such
data. The `XudtData` can be optional regardless of whether there is any extension
script or not. However, if an extension script requires such data, it must be
present.

The `lock` field included in `XudtData` will not be used by the xUDT script. It
is reserved for lock script specific data for current cells.

An extension script should first locate the index it resides in xUDT
args, then look for the data for the current extension script at the same index
in `data` field of `XudtData` structure.

## **Operations**

xUDT uses the same governance operations as Simple UDT: an owner lock controls
all governance operations, such as minting.

A normal transfer operation of xUDT, however, differs from Simple UDT. Depending
on the flags used, there might be 2 usage patterns:

### **Raw Extension Script**

When `flags & 0x1FFFFFFF` are set to 0x1, raw extension data is included in xUDT args directly.

```yaml
Inputs:
    <vec> xUDT_Cell
        Data:
            <amount: uint128> <xUDT data>
        Type:
            code_hash: xUDT type script
            args: <owner lock script hash> <xUDT args>
        Lock:
            <user defined>
    <...>
Outputs:
    <vec> xUDT_Cell
        Data:
            <amount: uint128> <xUDT data>
        Type:
            code_hash: xUDT type script
            args: <owner lock script hash> <xUDT args>
        Lock:
            <user defined>
    <...>
Witnesses:
    WitnessArgs structure:
        Lock: <user defined>
        Input Type: <XudtWitness>
            owner_script: <None>
            owner_signature: <None>				
            extension_scripts: <None>
            extension_data: 
                <vec> BytesVec
                    <data> 
                <...>
```

The witness of the same index as the first input xUDT cell is located by xUDT script. It is parsed first as WitnessArgs structure, the `input_type` or `output_type` field of `WitnessArgs`, is thus treated as `XudtWitness` structure.

Note that each extension script is only executed once in the transaction. When multiple instances of the same extension script are included, each instance will execute independently for each inclusion. The extension script is responsible for checking all xUDT cells of the current type, ensuring each cell data and witness for the current extension script, can be validated against the extension script’s rules.

### **P2SH Style Extension Script**

When `flags & 0x1FFFFFFF` are set to 0x2, only the blake160 hash of extension data is included in xUDT args. The user is required to provide the actual extension data in witness directly:

```yaml
Inputs:
    <vec> xUDT_Cell
        Data:
            <amount: uint128> <xUDT data>
        Type:
            code_hash: xUDT type script
            args: <owner lock script hash> <xUDT args, hash of raw extension data>
        Lock:
            <user defined>
    <...>
Outputs:
    <vec> xUDT_Cell
        Data:
            <amount: uint128> <xUDT data>
        Type:
            code_hash: xUDT type script
            args: <owner lock script hash> <xUDT args, hash of raw extension data>
        Lock:
            <user defined>
    <...>
Witnesses:
    WitnessArgs structure:
        Lock: <user defined>
        Input Type: XudtWitness
            owner_script: <None>
            owner_signature: <None>				
            extension_scripts: 
                <vec> ScriptVec
                    <script>
                <...>
            extension_data: 
                <vec> BytesVec
                    <data>
                <...>
```

The only difference here is that `XudtWitness` in `input_type` or
`output_type` field in the corresponding WitnessArgs structure, contains raw
extension data in `ScriptVec` data structure, xUDT script must first validate
that the hash of raw extension data provide here, is the same as blake160 hash
included in xUDT args. After this, it uses the same logic as the previous
workflow.

### Owner Mode without Consuming Cell

As described above, If an input cell uses an input lock script with same script
hash as the owner lock script hash, the `is_owner_mode` will be set to true. It
isn’t convenient: this requires extra cell to be consumed. With `owner_script`
and `owner_signature` set to proper values, we can use owner mode without extra
cell.

```yaml
Inputs:
    <vec>
			<Any input cells>
    <...>
Outputs:
    <vec> xUDT_Cell
        Data:
            <amount: uint128> <xUDT data>
        Type:
            code_hash: xUDT type script
            args: <owner lock script hash 1> <xUDT args>
        Lock:
            <user defined>
    <...>
Witnesses:
    WitnessArgs structure:
        Lock: <user defined>
        Input Type: <None>
        Output Type: XudtWitness
            owner_script: <owner script 1>
            owner_signature: <signature 1>				
            extension_scripts: 
                <vec> ScriptVec
                    <script>
                <...>
            extension_data: 
                <vec> BytesVec
                    <data>
                <...>
```

The example above shows a scenario of owner mode without consuming the owner's
cell.  We can implement an extension script as `<owner script 1>` with signature
validation. The `<signature 1>` can be used by `<owner script 1>` to place
signature information.

## Deployment

An [implementation](https://github.com/nervosnetwork/ckb-production-scripts/blob/master/c/xudt_rce.c) of
the spec above has been deployed to Mirana CKB mainnet and Pudge testnet:

- Mirana(mainnet)

| parameter | value |
| --- | --- |
| code_hash | 0x50bd8d6680b8b9cf98b73f3c08faf8b2a21914311954118ad6609be6e78a1b95 |
| hash_type | data1 |
| tx_hash | 0xc07844ce21b38e4b071dd0e1ee3b0e27afd8d7532491327f39b786343f558ab7 |
| index | 0x0 |
| dep_type | code |

This script is not upgradeable due to zero lock (lock args with all zeros).
We have previously deployed scripts with the ability to be upgraded. However, we
consider this upgrading mechanism to be harmful as it compromises the
decentralization of the blockchain. With the ability to upgrade, it means that
the owners (whether it's several people or even just one person) of this cell
can upgrade it at any time. This gives one or several individuals complete
control over the assets associated with the deploy scripts. That's why we have
chosen to use a zero lock when deploying this script.


- Pudge(testnet)

| parameter | value |
| --- | --- |
| code_hash | 0x50bd8d6680b8b9cf98b73f3c08faf8b2a21914311954118ad6609be6e78a1b95 |
| hash_type | data1 |
| tx_hash | 0xbf6fb538763efec2a70a6a3dcb7242787087e1030c4e7d86585bc63a9d337f5f |
| index | 0x0 |
| dep_type | code |

| parameter | value |
| --- | --- |
| code_hash | 0x25c29dc317811a6f6f3985a7a9ebc4838bd388d19d0feeecf0bcd60f6c0975bb |
| hash_type | type |
| tx_hash | 0xbf6fb538763efec2a70a6a3dcb7242787087e1030c4e7d86585bc63a9d337f5f |
| index | 0x0 |
| dep_type | code |

These 2 versions are pointing to the same xUDT deployment. 


A reproducible build is supported to verify the deploy script. To build the
deployed the script above, one can use the following steps:

```
$ git clone https://github.com/nervosnetwork/ckb-production-scripts
$ cd ckb-production-scripts
$ git checkout abdcb117b512e35910fa8e30241a7a354e5cacf0
$ git submodule update --init --recursive
$ make all-via-docker
```


================================================
File: .github/CODEOWNERS
================================================
* @nervosnetwork/rfc
/rfcs/0001-positioning/ @nervosnetwork/rfc @knwang
/rfcs/0002-ckb/ @nervosnetwork/rfc @janx
/rfcs/0003-ckb-vm/ @nervosnetwork/rfc @xxuejie
/rfcs/0004-ckb-block-sync/ @nervosnetwork/rfc @doitian
/rfcs/0005-priviledged-mode/ @nervosnetwork/rfc @xxuejie
/rfcs/0006-merkle-tree/ @nervosnetwork/rfc @kilb
/rfcs/0007-scoring-system-and-network-security/ @nervosnetwork/rfc @jjyr
/rfcs/0008-serialization/ @nervosnetwork/rfc @yangby-cryptape
/rfcs/0009-vm-syscalls/ @nervosnetwork/rfc @xxuejie
/rfcs/0010-eaglesong/ @nervosnetwork/rfc @aszepieniec
/rfcs/0011-transaction-filter-protocol/ @nervosnetwork/rfc @quake
/rfcs/0012-node-discovery/ @nervosnetwork/rfc @jjyr @TheWaWaR
/rfcs/0013-get-block-template/ @nervosnetwork/rfc @zhangsoledad
/rfcs/0014-vm-cycle-limits/ @nervosnetwork/rfc @xxuejie
/rfcs/0015-ckb-cryptoeconomics/ @nervosnetwork/rfc @knwang @janx
/rfcs/0017-tx-valid-since/ @nervosnetwork/rfc @jjyr
/rfcs/0019-data-structures/ @nervosnetwork/rfc @xxuejie
/rfcs/0020-ckb-consensus-protocol/ @nervosnetwork/rfc @nirenzang
/rfcs/0021-ckb-address-format/ @nervosnetwork/rfc @CipherWang
/rfcs/0022-transaction-structure/ @nervosnetwork/rfc @doitian
/rfcs/0023-dao-deposit-withdraw/ @nervosnetwork/rfc @xxuejie @doitian


================================================
File: .github/scripts/local-link-checker.sh
================================================
#!/usr/bin/env bash

set -e
set -u
[ -n "${DEBUG:-}" ] && set -x || true

function gherr() {
  local line="$1"
  local file="${line%%:*}"
  local lineno="${line#*:}"
  lineno="${lineno%%:*}"
  local matching="${line#*:*:}"
  echo "::error file=${file#./},line=$lineno::Broken link $matching"
}

function check() {
  local line dir link target failed=0

  while read line; do
    dir="$(dirname "${line%%:*}")"
    link="${line#*:*:}"

    case "$link" in
      //*)
        # ignore http links
        ;;
      /*)
        fail "Not a valid local link"
        ;;
      ../*)
        target="$(dirname "$dir")${link#..}"
        if ! [ -f "$target" ]; then
          failed=1
          gherr "$line"
        fi
        ;;
      *)
        # relative to current directory
        target="$dir/${link#./}"
        if ! [ -f "$target" ]; then
          failed=1
          gherr "$line"
        fi
        ;;
    esac
  done

  exit "$failed"
}

find . -name '*.md' -print0 | xargs -0 /usr/bin/grep -Hno '[^(): ][^():]*\.md' | check


================================================
File: .github/workflows/local-link-checker.yaml
================================================
name: Local Link Checker

on:
  push:
    branches:
      - "*"
  pull_request:

jobs:
  local-link-checker:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v3

    - name: run checker
      run: .github/scripts/local-link-checker.sh

    - name: info for fixing
      if: ${{ failure() }}
      run: echo "::error::Broken local links found, please use ./.github/scripts/local-link-checker.sh to check locally"


